Loading weights from: /workspace2/trained_weights/vehicle_final.pth
class GraphModule(torch.nn.Module):
    def forward(self, x):
        x: "f32[1, 3, 224, 224]"; 
    
        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
        # No stacktrace found for following nodes
        features_0_weight: "f32[64, 3, 3, 3]" = getattr(self.features, "0").weight
        features_0_bias: "f32[64]" = getattr(self.features, "0").bias
        features_3_squeeze_weight: "f32[16, 64, 1, 1]" = getattr(self.features, "3").squeeze.weight
        features_3_squeeze_bias: "f32[16]" = getattr(self.features, "3").squeeze.bias
        features_3_expand1x1_weight: "f32[64, 16, 1, 1]" = getattr(self.features, "3").expand1x1.weight
        features_3_expand1x1_bias: "f32[64]" = getattr(self.features, "3").expand1x1.bias
        features_3_expand3x3_weight: "f32[64, 16, 3, 3]" = getattr(self.features, "3").expand3x3.weight
        features_3_expand3x3_bias: "f32[64]" = getattr(self.features, "3").expand3x3.bias
        features_4_squeeze_weight: "f32[16, 128, 1, 1]" = getattr(self.features, "4").squeeze.weight
        features_4_squeeze_bias: "f32[16]" = getattr(self.features, "4").squeeze.bias
        features_4_expand1x1_weight: "f32[64, 16, 1, 1]" = getattr(self.features, "4").expand1x1.weight
        features_4_expand1x1_bias: "f32[64]" = getattr(self.features, "4").expand1x1.bias
        features_4_expand3x3_weight: "f32[64, 16, 3, 3]" = getattr(self.features, "4").expand3x3.weight
        features_4_expand3x3_bias: "f32[64]" = getattr(self.features, "4").expand3x3.bias
        features_6_squeeze_weight: "f32[32, 128, 1, 1]" = getattr(self.features, "6").squeeze.weight
        features_6_squeeze_bias: "f32[32]" = getattr(self.features, "6").squeeze.bias
        features_6_expand1x1_weight: "f32[128, 32, 1, 1]" = getattr(self.features, "6").expand1x1.weight
        features_6_expand1x1_bias: "f32[128]" = getattr(self.features, "6").expand1x1.bias
        features_6_expand3x3_weight: "f32[128, 32, 3, 3]" = getattr(self.features, "6").expand3x3.weight
        features_6_expand3x3_bias: "f32[128]" = getattr(self.features, "6").expand3x3.bias
        features_7_squeeze_weight: "f32[32, 256, 1, 1]" = getattr(self.features, "7").squeeze.weight
        features_7_squeeze_bias: "f32[32]" = getattr(self.features, "7").squeeze.bias
        features_7_expand1x1_weight: "f32[128, 32, 1, 1]" = getattr(self.features, "7").expand1x1.weight
        features_7_expand1x1_bias: "f32[128]" = getattr(self.features, "7").expand1x1.bias
        features_7_expand3x3_weight: "f32[128, 32, 3, 3]" = getattr(self.features, "7").expand3x3.weight
        features_7_expand3x3_bias: "f32[128]" = getattr(self.features, "7").expand3x3.bias
        features_9_squeeze_weight: "f32[48, 256, 1, 1]" = getattr(self.features, "9").squeeze.weight
        features_9_squeeze_bias: "f32[48]" = getattr(self.features, "9").squeeze.bias
        features_9_expand1x1_weight: "f32[192, 48, 1, 1]" = getattr(self.features, "9").expand1x1.weight
        features_9_expand1x1_bias: "f32[192]" = getattr(self.features, "9").expand1x1.bias
        features_9_expand3x3_weight: "f32[192, 48, 3, 3]" = getattr(self.features, "9").expand3x3.weight
        features_9_expand3x3_bias: "f32[192]" = getattr(self.features, "9").expand3x3.bias
        features_10_squeeze_weight: "f32[48, 384, 1, 1]" = getattr(self.features, "10").squeeze.weight
        features_10_squeeze_bias: "f32[48]" = getattr(self.features, "10").squeeze.bias
        features_10_expand1x1_weight: "f32[192, 48, 1, 1]" = getattr(self.features, "10").expand1x1.weight
        features_10_expand1x1_bias: "f32[192]" = getattr(self.features, "10").expand1x1.bias
        features_10_expand3x3_weight: "f32[192, 48, 3, 3]" = getattr(self.features, "10").expand3x3.weight
        features_10_expand3x3_bias: "f32[192]" = getattr(self.features, "10").expand3x3.bias
        features_11_squeeze_weight: "f32[64, 384, 1, 1]" = getattr(self.features, "11").squeeze.weight
        features_11_squeeze_bias: "f32[64]" = getattr(self.features, "11").squeeze.bias
        features_11_expand1x1_weight: "f32[256, 64, 1, 1]" = getattr(self.features, "11").expand1x1.weight
        features_11_expand1x1_bias: "f32[256]" = getattr(self.features, "11").expand1x1.bias
        features_11_expand3x3_weight: "f32[256, 64, 3, 3]" = getattr(self.features, "11").expand3x3.weight
        features_11_expand3x3_bias: "f32[256]" = getattr(self.features, "11").expand3x3.bias
        features_12_squeeze_weight: "f32[64, 512, 1, 1]" = getattr(self.features, "12").squeeze.weight
        features_12_squeeze_bias: "f32[64]" = getattr(self.features, "12").squeeze.bias
        features_12_expand1x1_weight: "f32[256, 64, 1, 1]" = getattr(self.features, "12").expand1x1.weight
        features_12_expand1x1_bias: "f32[256]" = getattr(self.features, "12").expand1x1.bias
        features_12_expand3x3_weight: "f32[256, 64, 3, 3]" = getattr(self.features, "12").expand3x3.weight
        features_12_expand3x3_bias: "f32[256]" = getattr(self.features, "12").expand3x3.bias
        classifier_1_weight: "f32[4, 512, 1, 1]" = getattr(self.classifier, "1").weight
        classifier_1_bias: "f32[4]" = getattr(self.classifier, "1").bias
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d: "f32[1, 64, 111, 111]" = torch.ops.aten.conv2d.default(x, features_0_weight, features_0_bias, [2, 2]);  x = features_0_weight = features_0_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu_: "f32[1, 64, 111, 111]" = torch.ops.aten.relu_.default(conv2d);  conv2d = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        max_pool2d: "f32[1, 64, 55, 55]" = torch.ops.aten.max_pool2d.default(relu_, [3, 3], [2, 2], [0, 0], [1, 1], True);  relu_ = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_1: "f32[1, 16, 55, 55]" = torch.ops.aten.conv2d.default(max_pool2d, features_3_squeeze_weight, features_3_squeeze_bias);  max_pool2d = features_3_squeeze_weight = features_3_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__1: "f32[1, 16, 55, 55]" = torch.ops.aten.relu_.default(conv2d_1);  conv2d_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_2: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(relu__1, features_3_expand1x1_weight, features_3_expand1x1_bias);  features_3_expand1x1_weight = features_3_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__2: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(conv2d_2);  conv2d_2 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_3: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(relu__1, features_3_expand3x3_weight, features_3_expand3x3_bias, [1, 1], [1, 1]);  relu__1 = features_3_expand3x3_weight = features_3_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__3: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(conv2d_3);  conv2d_3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat: "f32[1, 128, 55, 55]" = torch.ops.aten.cat.default([relu__2, relu__3], 1);  relu__2 = relu__3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_4: "f32[1, 16, 55, 55]" = torch.ops.aten.conv2d.default(cat, features_4_squeeze_weight, features_4_squeeze_bias);  cat = features_4_squeeze_weight = features_4_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__4: "f32[1, 16, 55, 55]" = torch.ops.aten.relu_.default(conv2d_4);  conv2d_4 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_5: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(relu__4, features_4_expand1x1_weight, features_4_expand1x1_bias);  features_4_expand1x1_weight = features_4_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__5: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(conv2d_5);  conv2d_5 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_6: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(relu__4, features_4_expand3x3_weight, features_4_expand3x3_bias, [1, 1], [1, 1]);  relu__4 = features_4_expand3x3_weight = features_4_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__6: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(conv2d_6);  conv2d_6 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_1: "f32[1, 128, 55, 55]" = torch.ops.aten.cat.default([relu__5, relu__6], 1);  relu__5 = relu__6 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        max_pool2d_1: "f32[1, 128, 27, 27]" = torch.ops.aten.max_pool2d.default(cat_1, [3, 3], [2, 2], [0, 0], [1, 1], True);  cat_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_7: "f32[1, 32, 27, 27]" = torch.ops.aten.conv2d.default(max_pool2d_1, features_6_squeeze_weight, features_6_squeeze_bias);  max_pool2d_1 = features_6_squeeze_weight = features_6_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__7: "f32[1, 32, 27, 27]" = torch.ops.aten.relu_.default(conv2d_7);  conv2d_7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_8: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(relu__7, features_6_expand1x1_weight, features_6_expand1x1_bias);  features_6_expand1x1_weight = features_6_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__8: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(conv2d_8);  conv2d_8 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_9: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(relu__7, features_6_expand3x3_weight, features_6_expand3x3_bias, [1, 1], [1, 1]);  relu__7 = features_6_expand3x3_weight = features_6_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__9: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(conv2d_9);  conv2d_9 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_2: "f32[1, 256, 27, 27]" = torch.ops.aten.cat.default([relu__8, relu__9], 1);  relu__8 = relu__9 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_10: "f32[1, 32, 27, 27]" = torch.ops.aten.conv2d.default(cat_2, features_7_squeeze_weight, features_7_squeeze_bias);  cat_2 = features_7_squeeze_weight = features_7_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__10: "f32[1, 32, 27, 27]" = torch.ops.aten.relu_.default(conv2d_10);  conv2d_10 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_11: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(relu__10, features_7_expand1x1_weight, features_7_expand1x1_bias);  features_7_expand1x1_weight = features_7_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__11: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(conv2d_11);  conv2d_11 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_12: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(relu__10, features_7_expand3x3_weight, features_7_expand3x3_bias, [1, 1], [1, 1]);  relu__10 = features_7_expand3x3_weight = features_7_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__12: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(conv2d_12);  conv2d_12 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_3: "f32[1, 256, 27, 27]" = torch.ops.aten.cat.default([relu__11, relu__12], 1);  relu__11 = relu__12 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        max_pool2d_2: "f32[1, 256, 13, 13]" = torch.ops.aten.max_pool2d.default(cat_3, [3, 3], [2, 2], [0, 0], [1, 1], True);  cat_3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_13: "f32[1, 48, 13, 13]" = torch.ops.aten.conv2d.default(max_pool2d_2, features_9_squeeze_weight, features_9_squeeze_bias);  max_pool2d_2 = features_9_squeeze_weight = features_9_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__13: "f32[1, 48, 13, 13]" = torch.ops.aten.relu_.default(conv2d_13);  conv2d_13 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_14: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(relu__13, features_9_expand1x1_weight, features_9_expand1x1_bias);  features_9_expand1x1_weight = features_9_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__14: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(conv2d_14);  conv2d_14 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_15: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(relu__13, features_9_expand3x3_weight, features_9_expand3x3_bias, [1, 1], [1, 1]);  relu__13 = features_9_expand3x3_weight = features_9_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__15: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(conv2d_15);  conv2d_15 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_4: "f32[1, 384, 13, 13]" = torch.ops.aten.cat.default([relu__14, relu__15], 1);  relu__14 = relu__15 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_16: "f32[1, 48, 13, 13]" = torch.ops.aten.conv2d.default(cat_4, features_10_squeeze_weight, features_10_squeeze_bias);  cat_4 = features_10_squeeze_weight = features_10_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__16: "f32[1, 48, 13, 13]" = torch.ops.aten.relu_.default(conv2d_16);  conv2d_16 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_17: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(relu__16, features_10_expand1x1_weight, features_10_expand1x1_bias);  features_10_expand1x1_weight = features_10_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__17: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(conv2d_17);  conv2d_17 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_18: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(relu__16, features_10_expand3x3_weight, features_10_expand3x3_bias, [1, 1], [1, 1]);  relu__16 = features_10_expand3x3_weight = features_10_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__18: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(conv2d_18);  conv2d_18 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_5: "f32[1, 384, 13, 13]" = torch.ops.aten.cat.default([relu__17, relu__18], 1);  relu__17 = relu__18 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_19: "f32[1, 64, 13, 13]" = torch.ops.aten.conv2d.default(cat_5, features_11_squeeze_weight, features_11_squeeze_bias);  cat_5 = features_11_squeeze_weight = features_11_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__19: "f32[1, 64, 13, 13]" = torch.ops.aten.relu_.default(conv2d_19);  conv2d_19 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_20: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(relu__19, features_11_expand1x1_weight, features_11_expand1x1_bias);  features_11_expand1x1_weight = features_11_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__20: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(conv2d_20);  conv2d_20 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_21: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(relu__19, features_11_expand3x3_weight, features_11_expand3x3_bias, [1, 1], [1, 1]);  relu__19 = features_11_expand3x3_weight = features_11_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__21: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(conv2d_21);  conv2d_21 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_6: "f32[1, 512, 13, 13]" = torch.ops.aten.cat.default([relu__20, relu__21], 1);  relu__20 = relu__21 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_22: "f32[1, 64, 13, 13]" = torch.ops.aten.conv2d.default(cat_6, features_12_squeeze_weight, features_12_squeeze_bias);  cat_6 = features_12_squeeze_weight = features_12_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__22: "f32[1, 64, 13, 13]" = torch.ops.aten.relu_.default(conv2d_22);  conv2d_22 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_23: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(relu__22, features_12_expand1x1_weight, features_12_expand1x1_bias);  features_12_expand1x1_weight = features_12_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__23: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(conv2d_23);  conv2d_23 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_24: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(relu__22, features_12_expand3x3_weight, features_12_expand3x3_bias, [1, 1], [1, 1]);  relu__22 = features_12_expand3x3_weight = features_12_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__24: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(conv2d_24);  conv2d_24 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_7: "f32[1, 512, 13, 13]" = torch.ops.aten.cat.default([relu__23, relu__24], 1);  relu__23 = relu__24 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
        dropout: "f32[1, 512, 13, 13]" = torch.ops.aten.dropout.default(cat_7, 0.5, False);  cat_7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_25: "f32[1, 4, 13, 13]" = torch.ops.aten.conv2d.default(dropout, classifier_1_weight, classifier_1_bias);  dropout = classifier_1_weight = classifier_1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__25: "f32[1, 4, 13, 13]" = torch.ops.aten.relu_.default(conv2d_25);  conv2d_25 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:1500 in forward, code: return F.adaptive_avg_pool2d(input, self.output_size)
        adaptive_avg_pool2d: "f32[1, 4, 1, 1]" = torch.ops.aten.adaptive_avg_pool2d.default(relu__25, [1, 1]);  relu__25 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:97 in forward, code: return torch.flatten(x, 1)
        flatten: "f32[1, 4]" = torch.ops.aten.flatten.using_ints(adaptive_avg_pool2d, 1);  adaptive_avg_pool2d = None
        return pytree.tree_unflatten((flatten,), self._out_spec)
        Skipping import of cpp extensions due to incompatible torch version 2.9.0+cpu for torchao version 0.14.0+cpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
/workspace/executorch-venv/lib/python3.12/site-packages/executorch/backends/arm/quantizer/quantization_config.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(act_scale * weight_scale).to(

class GraphModule(torch.nn.Module):
    def forward(self, x):
        x: "f32[1, 3, 224, 224]"; 
    
        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
        # No stacktrace found for following nodes
        quantize_per_tensor_default = torch.ops.quantized_decomposed.quantize_per_tensor.default(x, 0.03172215074300766, 8, -128, 127, torch.int8);  x = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default, 0.03172215074300766, 8, -128, 127, torch.int8);  quantize_per_tensor_default = None
        
        # No stacktrace found for following nodes
        _scale_0 = self._scale_0
        _zero_point_0 = self._zero_point_0
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default = self._frozen_param0
        dequantize_per_channel_default = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default, _scale_0, _zero_point_0, 0, -127, 127, torch.int8);  quantize_per_channel_default = _scale_0 = _zero_point_0 = None
        
        # No stacktrace found for following nodes
        _scale_1 = self._scale_1
        _zero_point_1 = self._zero_point_1
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_1 = self._frozen_param1
        dequantize_per_channel_default_1 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_1, _scale_1, _zero_point_1, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_1 = _scale_1 = _zero_point_1 = None
        conv2d: "f32[1, 64, 111, 111]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default, dequantize_per_channel_default, dequantize_per_channel_default_1, [2, 2]);  dequantize_per_tensor_default = dequantize_per_channel_default = dequantize_per_channel_default_1 = None
        quantize_per_tensor_default_1 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d, 0.05981677025556564, -11, -128, 127, torch.int8);  conv2d = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_1 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_1, 0.05981677025556564, -11, -128, 127, torch.int8);  quantize_per_tensor_default_1 = None
        relu_: "f32[1, 64, 111, 111]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_1);  dequantize_per_tensor_default_1 = None
        quantize_per_tensor_default_2 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu_, 0.05981677025556564, -11, -128, 127, torch.int8);  relu_ = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        dequantize_per_tensor_default_2 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_2, 0.05981677025556564, -11, -128, 127, torch.int8);  quantize_per_tensor_default_2 = None
        max_pool2d: "f32[1, 64, 55, 55]" = torch.ops.aten.max_pool2d.default(dequantize_per_tensor_default_2, [3, 3], [2, 2], [0, 0], [1, 1], True);  dequantize_per_tensor_default_2 = None
        quantize_per_tensor_default_3 = torch.ops.quantized_decomposed.quantize_per_tensor.default(max_pool2d, 0.05981677025556564, -11, -128, 127, torch.int8);  max_pool2d = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_3 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_3, 0.05981677025556564, -11, -128, 127, torch.int8);  quantize_per_tensor_default_3 = None
        
        # No stacktrace found for following nodes
        _scale_2 = self._scale_2
        _zero_point_2 = self._zero_point_2
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_2 = self._frozen_param2
        dequantize_per_channel_default_2 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_2, _scale_2, _zero_point_2, 0, -127, 127, torch.int8);  quantize_per_channel_default_2 = _scale_2 = _zero_point_2 = None
        
        # No stacktrace found for following nodes
        _scale_3 = self._scale_3
        _zero_point_3 = self._zero_point_3
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_3 = self._frozen_param3
        dequantize_per_channel_default_3 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_3, _scale_3, _zero_point_3, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_3 = _scale_3 = _zero_point_3 = None
        conv2d_1: "f32[1, 16, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_3, dequantize_per_channel_default_2, dequantize_per_channel_default_3);  dequantize_per_tensor_default_3 = dequantize_per_channel_default_2 = dequantize_per_channel_default_3 = None
        quantize_per_tensor_default_4 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_1, 0.14951655268669128, -22, -128, 127, torch.int8);  conv2d_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_4 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_4, 0.14951655268669128, -22, -128, 127, torch.int8);  quantize_per_tensor_default_4 = None
        relu__1: "f32[1, 16, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_4);  dequantize_per_tensor_default_4 = None
        quantize_per_tensor_default_5 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__1, 0.14951655268669128, -22, -128, 127, torch.int8);  relu__1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_68 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_5, 0.14951655268669128, -22, -128, 127, torch.int8)
        dequantize_per_tensor_default_67 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_5, 0.14951655268669128, -22, -128, 127, torch.int8);  quantize_per_tensor_default_5 = None
        
        # No stacktrace found for following nodes
        _scale_4 = self._scale_4
        _zero_point_4 = self._zero_point_4
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_4 = self._frozen_param4
        dequantize_per_channel_default_4 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_4, _scale_4, _zero_point_4, 0, -127, 127, torch.int8);  quantize_per_channel_default_4 = _scale_4 = _zero_point_4 = None
        
        # No stacktrace found for following nodes
        _scale_5 = self._scale_5
        _zero_point_5 = self._zero_point_5
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_5 = self._frozen_param5
        dequantize_per_channel_default_5 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_5, _scale_5, _zero_point_5, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_5 = _scale_5 = _zero_point_5 = None
        conv2d_2: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_67, dequantize_per_channel_default_4, dequantize_per_channel_default_5);  dequantize_per_tensor_default_67 = dequantize_per_channel_default_4 = dequantize_per_channel_default_5 = None
        quantize_per_tensor_default_6 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_2, 0.22975404560565948, 43, -128, 127, torch.int8);  conv2d_2 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_6 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_6, 0.22975404560565948, 43, -128, 127, torch.int8);  quantize_per_tensor_default_6 = None
        relu__2: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_6);  dequantize_per_tensor_default_6 = None
        quantize_per_tensor_default_7 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__2, 0.22975404560565948, 43, -128, 127, torch.int8);  relu__2 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_7 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_7, 0.22975404560565948, 43, -128, 127, torch.int8);  quantize_per_tensor_default_7 = None
        _scale_6 = self._scale_6
        _zero_point_6 = self._zero_point_6
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_6 = self._frozen_param6
        dequantize_per_channel_default_6 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_6, _scale_6, _zero_point_6, 0, -127, 127, torch.int8);  quantize_per_channel_default_6 = _scale_6 = _zero_point_6 = None
        
        # No stacktrace found for following nodes
        _scale_7 = self._scale_7
        _zero_point_7 = self._zero_point_7
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_7 = self._frozen_param7
        dequantize_per_channel_default_7 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_7, _scale_7, _zero_point_7, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_7 = _scale_7 = _zero_point_7 = None
        conv2d_3: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_68, dequantize_per_channel_default_6, dequantize_per_channel_default_7, [1, 1], [1, 1]);  dequantize_per_tensor_default_68 = dequantize_per_channel_default_6 = dequantize_per_channel_default_7 = None
        quantize_per_tensor_default_8 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_3, 0.22975404560565948, 43, -128, 127, torch.int8);  conv2d_3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_8 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_8, 0.22975404560565948, 43, -128, 127, torch.int8);  quantize_per_tensor_default_8 = None
        relu__3: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_8);  dequantize_per_tensor_default_8 = None
        quantize_per_tensor_default_9 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__3, 0.22975404560565948, 43, -128, 127, torch.int8);  relu__3 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_9 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_9, 0.22975404560565948, 43, -128, 127, torch.int8);  quantize_per_tensor_default_9 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat: "f32[1, 128, 55, 55]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_7, dequantize_per_tensor_default_9], 1);  dequantize_per_tensor_default_7 = dequantize_per_tensor_default_9 = None
        quantize_per_tensor_default_10 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat, 0.22975404560565948, 43, -128, 127, torch.int8);  cat = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_10 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_10, 0.22975404560565948, 43, -128, 127, torch.int8);  quantize_per_tensor_default_10 = None
        
        # No stacktrace found for following nodes
        _scale_8 = self._scale_8
        _zero_point_8 = self._zero_point_8
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_8 = self._frozen_param8
        dequantize_per_channel_default_8 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_8, _scale_8, _zero_point_8, 0, -127, 127, torch.int8);  quantize_per_channel_default_8 = _scale_8 = _zero_point_8 = None
        
        # No stacktrace found for following nodes
        _scale_9 = self._scale_9
        _zero_point_9 = self._zero_point_9
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_9 = self._frozen_param9
        dequantize_per_channel_default_9 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_9, _scale_9, _zero_point_9, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_9 = _scale_9 = _zero_point_9 = None
        conv2d_4: "f32[1, 16, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_10, dequantize_per_channel_default_8, dequantize_per_channel_default_9);  dequantize_per_tensor_default_10 = dequantize_per_channel_default_8 = dequantize_per_channel_default_9 = None
        quantize_per_tensor_default_11 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_4, 0.19444149732589722, -34, -128, 127, torch.int8);  conv2d_4 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_11 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_11, 0.19444149732589722, -34, -128, 127, torch.int8);  quantize_per_tensor_default_11 = None
        relu__4: "f32[1, 16, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_11);  dequantize_per_tensor_default_11 = None
        quantize_per_tensor_default_12 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__4, 0.19444149732589722, -34, -128, 127, torch.int8);  relu__4 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_70 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_12, 0.19444149732589722, -34, -128, 127, torch.int8)
        dequantize_per_tensor_default_69 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_12, 0.19444149732589722, -34, -128, 127, torch.int8);  quantize_per_tensor_default_12 = None
        
        # No stacktrace found for following nodes
        _scale_10 = self._scale_10
        _zero_point_10 = self._zero_point_10
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_10 = self._frozen_param10
        dequantize_per_channel_default_10 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_10, _scale_10, _zero_point_10, 0, -127, 127, torch.int8);  quantize_per_channel_default_10 = _scale_10 = _zero_point_10 = None
        
        # No stacktrace found for following nodes
        _scale_11 = self._scale_11
        _zero_point_11 = self._zero_point_11
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_11 = self._frozen_param11
        dequantize_per_channel_default_11 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_11, _scale_11, _zero_point_11, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_11 = _scale_11 = _zero_point_11 = None
        conv2d_5: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_69, dequantize_per_channel_default_10, dequantize_per_channel_default_11);  dequantize_per_tensor_default_69 = dequantize_per_channel_default_10 = dequantize_per_channel_default_11 = None
        quantize_per_tensor_default_13 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_5, 0.24228017032146454, 14, -128, 127, torch.int8);  conv2d_5 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_13 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_13, 0.24228017032146454, 14, -128, 127, torch.int8);  quantize_per_tensor_default_13 = None
        relu__5: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_13);  dequantize_per_tensor_default_13 = None
        quantize_per_tensor_default_14 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__5, 0.24228017032146454, 14, -128, 127, torch.int8);  relu__5 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_14 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_14, 0.24228017032146454, 14, -128, 127, torch.int8);  quantize_per_tensor_default_14 = None
        _scale_12 = self._scale_12
        _zero_point_12 = self._zero_point_12
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_12 = self._frozen_param12
        dequantize_per_channel_default_12 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_12, _scale_12, _zero_point_12, 0, -127, 127, torch.int8);  quantize_per_channel_default_12 = _scale_12 = _zero_point_12 = None
        
        # No stacktrace found for following nodes
        _scale_13 = self._scale_13
        _zero_point_13 = self._zero_point_13
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_13 = self._frozen_param13
        dequantize_per_channel_default_13 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_13, _scale_13, _zero_point_13, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_13 = _scale_13 = _zero_point_13 = None
        conv2d_6: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_70, dequantize_per_channel_default_12, dequantize_per_channel_default_13, [1, 1], [1, 1]);  dequantize_per_tensor_default_70 = dequantize_per_channel_default_12 = dequantize_per_channel_default_13 = None
        quantize_per_tensor_default_15 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_6, 0.24228017032146454, 14, -128, 127, torch.int8);  conv2d_6 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_15 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_15, 0.24228017032146454, 14, -128, 127, torch.int8);  quantize_per_tensor_default_15 = None
        relu__6: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_15);  dequantize_per_tensor_default_15 = None
        quantize_per_tensor_default_16 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__6, 0.24228017032146454, 14, -128, 127, torch.int8);  relu__6 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_16 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_16, 0.24228017032146454, 14, -128, 127, torch.int8);  quantize_per_tensor_default_16 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_1: "f32[1, 128, 55, 55]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_14, dequantize_per_tensor_default_16], 1);  dequantize_per_tensor_default_14 = dequantize_per_tensor_default_16 = None
        quantize_per_tensor_default_17 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_1, 0.24228017032146454, 14, -128, 127, torch.int8);  cat_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        dequantize_per_tensor_default_17 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_17, 0.24228017032146454, 14, -128, 127, torch.int8);  quantize_per_tensor_default_17 = None
        max_pool2d_1: "f32[1, 128, 27, 27]" = torch.ops.aten.max_pool2d.default(dequantize_per_tensor_default_17, [3, 3], [2, 2], [0, 0], [1, 1], True);  dequantize_per_tensor_default_17 = None
        quantize_per_tensor_default_18 = torch.ops.quantized_decomposed.quantize_per_tensor.default(max_pool2d_1, 0.24228017032146454, 14, -128, 127, torch.int8);  max_pool2d_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_18 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_18, 0.24228017032146454, 14, -128, 127, torch.int8);  quantize_per_tensor_default_18 = None
        
        # No stacktrace found for following nodes
        _scale_14 = self._scale_14
        _zero_point_14 = self._zero_point_14
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_14 = self._frozen_param14
        dequantize_per_channel_default_14 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_14, _scale_14, _zero_point_14, 0, -127, 127, torch.int8);  quantize_per_channel_default_14 = _scale_14 = _zero_point_14 = None
        
        # No stacktrace found for following nodes
        _scale_15 = self._scale_15
        _zero_point_15 = self._zero_point_15
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_15 = self._frozen_param15
        dequantize_per_channel_default_15 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_15, _scale_15, _zero_point_15, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_15 = _scale_15 = _zero_point_15 = None
        conv2d_7: "f32[1, 32, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_18, dequantize_per_channel_default_14, dequantize_per_channel_default_15);  dequantize_per_tensor_default_18 = dequantize_per_channel_default_14 = dequantize_per_channel_default_15 = None
        quantize_per_tensor_default_19 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_7, 0.546843409538269, -37, -128, 127, torch.int8);  conv2d_7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_19 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_19, 0.546843409538269, -37, -128, 127, torch.int8);  quantize_per_tensor_default_19 = None
        relu__7: "f32[1, 32, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_19);  dequantize_per_tensor_default_19 = None
        quantize_per_tensor_default_20 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__7, 0.546843409538269, -37, -128, 127, torch.int8);  relu__7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_72 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_20, 0.546843409538269, -37, -128, 127, torch.int8)
        dequantize_per_tensor_default_71 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_20, 0.546843409538269, -37, -128, 127, torch.int8);  quantize_per_tensor_default_20 = None
        
        # No stacktrace found for following nodes
        _scale_16 = self._scale_16
        _zero_point_16 = self._zero_point_16
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_16 = self._frozen_param16
        dequantize_per_channel_default_16 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_16, _scale_16, _zero_point_16, 0, -127, 127, torch.int8);  quantize_per_channel_default_16 = _scale_16 = _zero_point_16 = None
        
        # No stacktrace found for following nodes
        _scale_17 = self._scale_17
        _zero_point_17 = self._zero_point_17
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_17 = self._frozen_param17
        dequantize_per_channel_default_17 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_17, _scale_17, _zero_point_17, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_17 = _scale_17 = _zero_point_17 = None
        conv2d_8: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_71, dequantize_per_channel_default_16, dequantize_per_channel_default_17);  dequantize_per_tensor_default_71 = dequantize_per_channel_default_16 = dequantize_per_channel_default_17 = None
        quantize_per_tensor_default_21 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_8, 0.7956364154815674, 3, -128, 127, torch.int8);  conv2d_8 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_21 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_21, 0.7956364154815674, 3, -128, 127, torch.int8);  quantize_per_tensor_default_21 = None
        relu__8: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_21);  dequantize_per_tensor_default_21 = None
        quantize_per_tensor_default_22 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__8, 0.7956364154815674, 3, -128, 127, torch.int8);  relu__8 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_22 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_22, 0.7956364154815674, 3, -128, 127, torch.int8);  quantize_per_tensor_default_22 = None
        _scale_18 = self._scale_18
        _zero_point_18 = self._zero_point_18
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_18 = self._frozen_param18
        dequantize_per_channel_default_18 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_18, _scale_18, _zero_point_18, 0, -127, 127, torch.int8);  quantize_per_channel_default_18 = _scale_18 = _zero_point_18 = None
        
        # No stacktrace found for following nodes
        _scale_19 = self._scale_19
        _zero_point_19 = self._zero_point_19
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_19 = self._frozen_param19
        dequantize_per_channel_default_19 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_19, _scale_19, _zero_point_19, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_19 = _scale_19 = _zero_point_19 = None
        conv2d_9: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_72, dequantize_per_channel_default_18, dequantize_per_channel_default_19, [1, 1], [1, 1]);  dequantize_per_tensor_default_72 = dequantize_per_channel_default_18 = dequantize_per_channel_default_19 = None
        quantize_per_tensor_default_23 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_9, 0.7956364154815674, 3, -128, 127, torch.int8);  conv2d_9 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_23 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_23, 0.7956364154815674, 3, -128, 127, torch.int8);  quantize_per_tensor_default_23 = None
        relu__9: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_23);  dequantize_per_tensor_default_23 = None
        quantize_per_tensor_default_24 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__9, 0.7956364154815674, 3, -128, 127, torch.int8);  relu__9 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_24 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_24, 0.7956364154815674, 3, -128, 127, torch.int8);  quantize_per_tensor_default_24 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_2: "f32[1, 256, 27, 27]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_22, dequantize_per_tensor_default_24], 1);  dequantize_per_tensor_default_22 = dequantize_per_tensor_default_24 = None
        quantize_per_tensor_default_25 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_2, 0.7956364154815674, 3, -128, 127, torch.int8);  cat_2 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_25 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_25, 0.7956364154815674, 3, -128, 127, torch.int8);  quantize_per_tensor_default_25 = None
        
        # No stacktrace found for following nodes
        _scale_20 = self._scale_20
        _zero_point_20 = self._zero_point_20
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_20 = self._frozen_param20
        dequantize_per_channel_default_20 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_20, _scale_20, _zero_point_20, 0, -127, 127, torch.int8);  quantize_per_channel_default_20 = _scale_20 = _zero_point_20 = None
        
        # No stacktrace found for following nodes
        _scale_21 = self._scale_21
        _zero_point_21 = self._zero_point_21
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_21 = self._frozen_param21
        dequantize_per_channel_default_21 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_21, _scale_21, _zero_point_21, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_21 = _scale_21 = _zero_point_21 = None
        conv2d_10: "f32[1, 32, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_25, dequantize_per_channel_default_20, dequantize_per_channel_default_21);  dequantize_per_tensor_default_25 = dequantize_per_channel_default_20 = dequantize_per_channel_default_21 = None
        quantize_per_tensor_default_26 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_10, 0.7869913578033447, -22, -128, 127, torch.int8);  conv2d_10 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_26 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_26, 0.7869913578033447, -22, -128, 127, torch.int8);  quantize_per_tensor_default_26 = None
        relu__10: "f32[1, 32, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_26);  dequantize_per_tensor_default_26 = None
        quantize_per_tensor_default_27 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__10, 0.7869913578033447, -22, -128, 127, torch.int8);  relu__10 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_74 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_27, 0.7869913578033447, -22, -128, 127, torch.int8)
        dequantize_per_tensor_default_73 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_27, 0.7869913578033447, -22, -128, 127, torch.int8);  quantize_per_tensor_default_27 = None
        
        # No stacktrace found for following nodes
        _scale_22 = self._scale_22
        _zero_point_22 = self._zero_point_22
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_22 = self._frozen_param22
        dequantize_per_channel_default_22 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_22, _scale_22, _zero_point_22, 0, -127, 127, torch.int8);  quantize_per_channel_default_22 = _scale_22 = _zero_point_22 = None
        
        # No stacktrace found for following nodes
        _scale_23 = self._scale_23
        _zero_point_23 = self._zero_point_23
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_23 = self._frozen_param23
        dequantize_per_channel_default_23 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_23, _scale_23, _zero_point_23, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_23 = _scale_23 = _zero_point_23 = None
        conv2d_11: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_73, dequantize_per_channel_default_22, dequantize_per_channel_default_23);  dequantize_per_tensor_default_73 = dequantize_per_channel_default_22 = dequantize_per_channel_default_23 = None
        quantize_per_tensor_default_28 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_11, 1.5204071998596191, 37, -128, 127, torch.int8);  conv2d_11 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_28 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_28, 1.5204071998596191, 37, -128, 127, torch.int8);  quantize_per_tensor_default_28 = None
        relu__11: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_28);  dequantize_per_tensor_default_28 = None
        quantize_per_tensor_default_29 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__11, 1.5204071998596191, 37, -128, 127, torch.int8);  relu__11 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_29 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_29, 1.5204071998596191, 37, -128, 127, torch.int8);  quantize_per_tensor_default_29 = None
        _scale_24 = self._scale_24
        _zero_point_24 = self._zero_point_24
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_24 = self._frozen_param24
        dequantize_per_channel_default_24 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_24, _scale_24, _zero_point_24, 0, -127, 127, torch.int8);  quantize_per_channel_default_24 = _scale_24 = _zero_point_24 = None
        
        # No stacktrace found for following nodes
        _scale_25 = self._scale_25
        _zero_point_25 = self._zero_point_25
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_25 = self._frozen_param25
        dequantize_per_channel_default_25 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_25, _scale_25, _zero_point_25, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_25 = _scale_25 = _zero_point_25 = None
        conv2d_12: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_74, dequantize_per_channel_default_24, dequantize_per_channel_default_25, [1, 1], [1, 1]);  dequantize_per_tensor_default_74 = dequantize_per_channel_default_24 = dequantize_per_channel_default_25 = None
        quantize_per_tensor_default_30 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_12, 1.5204071998596191, 37, -128, 127, torch.int8);  conv2d_12 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_30 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_30, 1.5204071998596191, 37, -128, 127, torch.int8);  quantize_per_tensor_default_30 = None
        relu__12: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_30);  dequantize_per_tensor_default_30 = None
        quantize_per_tensor_default_31 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__12, 1.5204071998596191, 37, -128, 127, torch.int8);  relu__12 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_31 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_31, 1.5204071998596191, 37, -128, 127, torch.int8);  quantize_per_tensor_default_31 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_3: "f32[1, 256, 27, 27]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_29, dequantize_per_tensor_default_31], 1);  dequantize_per_tensor_default_29 = dequantize_per_tensor_default_31 = None
        quantize_per_tensor_default_32 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_3, 1.5204071998596191, 37, -128, 127, torch.int8);  cat_3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        dequantize_per_tensor_default_32 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_32, 1.5204071998596191, 37, -128, 127, torch.int8);  quantize_per_tensor_default_32 = None
        max_pool2d_2: "f32[1, 256, 13, 13]" = torch.ops.aten.max_pool2d.default(dequantize_per_tensor_default_32, [3, 3], [2, 2], [0, 0], [1, 1], True);  dequantize_per_tensor_default_32 = None
        quantize_per_tensor_default_33 = torch.ops.quantized_decomposed.quantize_per_tensor.default(max_pool2d_2, 1.5204071998596191, 37, -128, 127, torch.int8);  max_pool2d_2 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_33 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_33, 1.5204071998596191, 37, -128, 127, torch.int8);  quantize_per_tensor_default_33 = None
        
        # No stacktrace found for following nodes
        _scale_26 = self._scale_26
        _zero_point_26 = self._zero_point_26
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_26 = self._frozen_param26
        dequantize_per_channel_default_26 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_26, _scale_26, _zero_point_26, 0, -127, 127, torch.int8);  quantize_per_channel_default_26 = _scale_26 = _zero_point_26 = None
        
        # No stacktrace found for following nodes
        _scale_27 = self._scale_27
        _zero_point_27 = self._zero_point_27
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_27 = self._frozen_param27
        dequantize_per_channel_default_27 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_27, _scale_27, _zero_point_27, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_27 = _scale_27 = _zero_point_27 = None
        conv2d_13: "f32[1, 48, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_33, dequantize_per_channel_default_26, dequantize_per_channel_default_27);  dequantize_per_tensor_default_33 = dequantize_per_channel_default_26 = dequantize_per_channel_default_27 = None
        quantize_per_tensor_default_34 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_13, 1.2129478454589844, -15, -128, 127, torch.int8);  conv2d_13 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_34 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_34, 1.2129478454589844, -15, -128, 127, torch.int8);  quantize_per_tensor_default_34 = None
        relu__13: "f32[1, 48, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_34);  dequantize_per_tensor_default_34 = None
        quantize_per_tensor_default_35 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__13, 1.2129478454589844, -15, -128, 127, torch.int8);  relu__13 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_76 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_35, 1.2129478454589844, -15, -128, 127, torch.int8)
        dequantize_per_tensor_default_75 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_35, 1.2129478454589844, -15, -128, 127, torch.int8);  quantize_per_tensor_default_35 = None
        
        # No stacktrace found for following nodes
        _scale_28 = self._scale_28
        _zero_point_28 = self._zero_point_28
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_28 = self._frozen_param28
        dequantize_per_channel_default_28 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_28, _scale_28, _zero_point_28, 0, -127, 127, torch.int8);  quantize_per_channel_default_28 = _scale_28 = _zero_point_28 = None
        
        # No stacktrace found for following nodes
        _scale_29 = self._scale_29
        _zero_point_29 = self._zero_point_29
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_29 = self._frozen_param29
        dequantize_per_channel_default_29 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_29, _scale_29, _zero_point_29, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_29 = _scale_29 = _zero_point_29 = None
        conv2d_14: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_75, dequantize_per_channel_default_28, dequantize_per_channel_default_29);  dequantize_per_tensor_default_75 = dequantize_per_channel_default_28 = dequantize_per_channel_default_29 = None
        quantize_per_tensor_default_36 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_14, 2.3511459827423096, 24, -128, 127, torch.int8);  conv2d_14 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_36 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_36, 2.3511459827423096, 24, -128, 127, torch.int8);  quantize_per_tensor_default_36 = None
        relu__14: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_36);  dequantize_per_tensor_default_36 = None
        quantize_per_tensor_default_37 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__14, 2.3511459827423096, 24, -128, 127, torch.int8);  relu__14 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_37 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_37, 2.3511459827423096, 24, -128, 127, torch.int8);  quantize_per_tensor_default_37 = None
        _scale_30 = self._scale_30
        _zero_point_30 = self._zero_point_30
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_30 = self._frozen_param30
        dequantize_per_channel_default_30 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_30, _scale_30, _zero_point_30, 0, -127, 127, torch.int8);  quantize_per_channel_default_30 = _scale_30 = _zero_point_30 = None
        
        # No stacktrace found for following nodes
        _scale_31 = self._scale_31
        _zero_point_31 = self._zero_point_31
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_31 = self._frozen_param31
        dequantize_per_channel_default_31 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_31, _scale_31, _zero_point_31, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_31 = _scale_31 = _zero_point_31 = None
        conv2d_15: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_76, dequantize_per_channel_default_30, dequantize_per_channel_default_31, [1, 1], [1, 1]);  dequantize_per_tensor_default_76 = dequantize_per_channel_default_30 = dequantize_per_channel_default_31 = None
        quantize_per_tensor_default_38 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_15, 2.3511459827423096, 24, -128, 127, torch.int8);  conv2d_15 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_38 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_38, 2.3511459827423096, 24, -128, 127, torch.int8);  quantize_per_tensor_default_38 = None
        relu__15: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_38);  dequantize_per_tensor_default_38 = None
        quantize_per_tensor_default_39 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__15, 2.3511459827423096, 24, -128, 127, torch.int8);  relu__15 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_39 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_39, 2.3511459827423096, 24, -128, 127, torch.int8);  quantize_per_tensor_default_39 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_4: "f32[1, 384, 13, 13]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_37, dequantize_per_tensor_default_39], 1);  dequantize_per_tensor_default_37 = dequantize_per_tensor_default_39 = None
        quantize_per_tensor_default_40 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_4, 2.3511459827423096, 24, -128, 127, torch.int8);  cat_4 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_40 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_40, 2.3511459827423096, 24, -128, 127, torch.int8);  quantize_per_tensor_default_40 = None
        
        # No stacktrace found for following nodes
        _scale_32 = self._scale_32
        _zero_point_32 = self._zero_point_32
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_32 = self._frozen_param32
        dequantize_per_channel_default_32 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_32, _scale_32, _zero_point_32, 0, -127, 127, torch.int8);  quantize_per_channel_default_32 = _scale_32 = _zero_point_32 = None
        
        # No stacktrace found for following nodes
        _scale_33 = self._scale_33
        _zero_point_33 = self._zero_point_33
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_33 = self._frozen_param33
        dequantize_per_channel_default_33 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_33, _scale_33, _zero_point_33, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_33 = _scale_33 = _zero_point_33 = None
        conv2d_16: "f32[1, 48, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_40, dequantize_per_channel_default_32, dequantize_per_channel_default_33);  dequantize_per_tensor_default_40 = dequantize_per_channel_default_32 = dequantize_per_channel_default_33 = None
        quantize_per_tensor_default_41 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_16, 2.410737991333008, -42, -128, 127, torch.int8);  conv2d_16 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_41 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_41, 2.410737991333008, -42, -128, 127, torch.int8);  quantize_per_tensor_default_41 = None
        relu__16: "f32[1, 48, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_41);  dequantize_per_tensor_default_41 = None
        quantize_per_tensor_default_42 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__16, 2.410737991333008, -42, -128, 127, torch.int8);  relu__16 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_78 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_42, 2.410737991333008, -42, -128, 127, torch.int8)
        dequantize_per_tensor_default_77 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_42, 2.410737991333008, -42, -128, 127, torch.int8);  quantize_per_tensor_default_42 = None
        
        # No stacktrace found for following nodes
        _scale_34 = self._scale_34
        _zero_point_34 = self._zero_point_34
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_34 = self._frozen_param34
        dequantize_per_channel_default_34 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_34, _scale_34, _zero_point_34, 0, -127, 127, torch.int8);  quantize_per_channel_default_34 = _scale_34 = _zero_point_34 = None
        
        # No stacktrace found for following nodes
        _scale_35 = self._scale_35
        _zero_point_35 = self._zero_point_35
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_35 = self._frozen_param35
        dequantize_per_channel_default_35 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_35, _scale_35, _zero_point_35, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_35 = _scale_35 = _zero_point_35 = None
        conv2d_17: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_77, dequantize_per_channel_default_34, dequantize_per_channel_default_35);  dequantize_per_tensor_default_77 = dequantize_per_channel_default_34 = dequantize_per_channel_default_35 = None
        quantize_per_tensor_default_43 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_17, 2.964160203933716, -2, -128, 127, torch.int8);  conv2d_17 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_43 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_43, 2.964160203933716, -2, -128, 127, torch.int8);  quantize_per_tensor_default_43 = None
        relu__17: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_43);  dequantize_per_tensor_default_43 = None
        quantize_per_tensor_default_44 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__17, 2.964160203933716, -2, -128, 127, torch.int8);  relu__17 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_44 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_44, 2.964160203933716, -2, -128, 127, torch.int8);  quantize_per_tensor_default_44 = None
        _scale_36 = self._scale_36
        _zero_point_36 = self._zero_point_36
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_36 = self._frozen_param36
        dequantize_per_channel_default_36 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_36, _scale_36, _zero_point_36, 0, -127, 127, torch.int8);  quantize_per_channel_default_36 = _scale_36 = _zero_point_36 = None
        
        # No stacktrace found for following nodes
        _scale_37 = self._scale_37
        _zero_point_37 = self._zero_point_37
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_37 = self._frozen_param37
        dequantize_per_channel_default_37 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_37, _scale_37, _zero_point_37, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_37 = _scale_37 = _zero_point_37 = None
        conv2d_18: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_78, dequantize_per_channel_default_36, dequantize_per_channel_default_37, [1, 1], [1, 1]);  dequantize_per_tensor_default_78 = dequantize_per_channel_default_36 = dequantize_per_channel_default_37 = None
        quantize_per_tensor_default_45 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_18, 2.964160203933716, -2, -128, 127, torch.int8);  conv2d_18 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_45 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_45, 2.964160203933716, -2, -128, 127, torch.int8);  quantize_per_tensor_default_45 = None
        relu__18: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_45);  dequantize_per_tensor_default_45 = None
        quantize_per_tensor_default_46 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__18, 2.964160203933716, -2, -128, 127, torch.int8);  relu__18 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_46 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_46, 2.964160203933716, -2, -128, 127, torch.int8);  quantize_per_tensor_default_46 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_5: "f32[1, 384, 13, 13]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_44, dequantize_per_tensor_default_46], 1);  dequantize_per_tensor_default_44 = dequantize_per_tensor_default_46 = None
        quantize_per_tensor_default_47 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_5, 2.964160203933716, -2, -128, 127, torch.int8);  cat_5 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_47 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_47, 2.964160203933716, -2, -128, 127, torch.int8);  quantize_per_tensor_default_47 = None
        
        # No stacktrace found for following nodes
        _scale_38 = self._scale_38
        _zero_point_38 = self._zero_point_38
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_38 = self._frozen_param38
        dequantize_per_channel_default_38 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_38, _scale_38, _zero_point_38, 0, -127, 127, torch.int8);  quantize_per_channel_default_38 = _scale_38 = _zero_point_38 = None
        
        # No stacktrace found for following nodes
        _scale_39 = self._scale_39
        _zero_point_39 = self._zero_point_39
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_39 = self._frozen_param39
        dequantize_per_channel_default_39 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_39, _scale_39, _zero_point_39, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_39 = _scale_39 = _zero_point_39 = None
        conv2d_19: "f32[1, 64, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_47, dequantize_per_channel_default_38, dequantize_per_channel_default_39);  dequantize_per_tensor_default_47 = dequantize_per_channel_default_38 = dequantize_per_channel_default_39 = None
        quantize_per_tensor_default_48 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_19, 1.872473955154419, -8, -128, 127, torch.int8);  conv2d_19 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_48 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_48, 1.872473955154419, -8, -128, 127, torch.int8);  quantize_per_tensor_default_48 = None
        relu__19: "f32[1, 64, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_48);  dequantize_per_tensor_default_48 = None
        quantize_per_tensor_default_49 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__19, 1.872473955154419, -8, -128, 127, torch.int8);  relu__19 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_80 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_49, 1.872473955154419, -8, -128, 127, torch.int8)
        dequantize_per_tensor_default_79 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_49, 1.872473955154419, -8, -128, 127, torch.int8);  quantize_per_tensor_default_49 = None
        
        # No stacktrace found for following nodes
        _scale_40 = self._scale_40
        _zero_point_40 = self._zero_point_40
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_40 = self._frozen_param40
        dequantize_per_channel_default_40 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_40, _scale_40, _zero_point_40, 0, -127, 127, torch.int8);  quantize_per_channel_default_40 = _scale_40 = _zero_point_40 = None
        
        # No stacktrace found for following nodes
        _scale_41 = self._scale_41
        _zero_point_41 = self._zero_point_41
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_41 = self._frozen_param41
        dequantize_per_channel_default_41 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_41, _scale_41, _zero_point_41, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_41 = _scale_41 = _zero_point_41 = None
        conv2d_20: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_79, dequantize_per_channel_default_40, dequantize_per_channel_default_41);  dequantize_per_tensor_default_79 = dequantize_per_channel_default_40 = dequantize_per_channel_default_41 = None
        quantize_per_tensor_default_50 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_20, 2.6768367290496826, -7, -128, 127, torch.int8);  conv2d_20 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_50 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_50, 2.6768367290496826, -7, -128, 127, torch.int8);  quantize_per_tensor_default_50 = None
        relu__20: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_50);  dequantize_per_tensor_default_50 = None
        quantize_per_tensor_default_51 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__20, 2.6768367290496826, -7, -128, 127, torch.int8);  relu__20 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_51 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_51, 2.6768367290496826, -7, -128, 127, torch.int8);  quantize_per_tensor_default_51 = None
        _scale_42 = self._scale_42
        _zero_point_42 = self._zero_point_42
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_42 = self._frozen_param42
        dequantize_per_channel_default_42 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_42, _scale_42, _zero_point_42, 0, -127, 127, torch.int8);  quantize_per_channel_default_42 = _scale_42 = _zero_point_42 = None
        
        # No stacktrace found for following nodes
        _scale_43 = self._scale_43
        _zero_point_43 = self._zero_point_43
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_43 = self._frozen_param43
        dequantize_per_channel_default_43 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_43, _scale_43, _zero_point_43, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_43 = _scale_43 = _zero_point_43 = None
        conv2d_21: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_80, dequantize_per_channel_default_42, dequantize_per_channel_default_43, [1, 1], [1, 1]);  dequantize_per_tensor_default_80 = dequantize_per_channel_default_42 = dequantize_per_channel_default_43 = None
        quantize_per_tensor_default_52 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_21, 2.6768367290496826, -7, -128, 127, torch.int8);  conv2d_21 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_52 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_52, 2.6768367290496826, -7, -128, 127, torch.int8);  quantize_per_tensor_default_52 = None
        relu__21: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_52);  dequantize_per_tensor_default_52 = None
        quantize_per_tensor_default_53 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__21, 2.6768367290496826, -7, -128, 127, torch.int8);  relu__21 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_53 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_53, 2.6768367290496826, -7, -128, 127, torch.int8);  quantize_per_tensor_default_53 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_6: "f32[1, 512, 13, 13]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_51, dequantize_per_tensor_default_53], 1);  dequantize_per_tensor_default_51 = dequantize_per_tensor_default_53 = None
        quantize_per_tensor_default_54 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_6, 2.6768367290496826, -7, -128, 127, torch.int8);  cat_6 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_54 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_54, 2.6768367290496826, -7, -128, 127, torch.int8);  quantize_per_tensor_default_54 = None
        
        # No stacktrace found for following nodes
        _scale_44 = self._scale_44
        _zero_point_44 = self._zero_point_44
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_44 = self._frozen_param44
        dequantize_per_channel_default_44 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_44, _scale_44, _zero_point_44, 0, -127, 127, torch.int8);  quantize_per_channel_default_44 = _scale_44 = _zero_point_44 = None
        
        # No stacktrace found for following nodes
        _scale_45 = self._scale_45
        _zero_point_45 = self._zero_point_45
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_45 = self._frozen_param45
        dequantize_per_channel_default_45 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_45, _scale_45, _zero_point_45, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_45 = _scale_45 = _zero_point_45 = None
        conv2d_22: "f32[1, 64, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_54, dequantize_per_channel_default_44, dequantize_per_channel_default_45);  dequantize_per_tensor_default_54 = dequantize_per_channel_default_44 = dequantize_per_channel_default_45 = None
        quantize_per_tensor_default_55 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_22, 4.506610870361328, -36, -128, 127, torch.int8);  conv2d_22 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_55 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_55, 4.506610870361328, -36, -128, 127, torch.int8);  quantize_per_tensor_default_55 = None
        relu__22: "f32[1, 64, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_55);  dequantize_per_tensor_default_55 = None
        quantize_per_tensor_default_56 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__22, 4.506610870361328, -36, -128, 127, torch.int8);  relu__22 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_82 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_56, 4.506610870361328, -36, -128, 127, torch.int8)
        dequantize_per_tensor_default_81 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_56, 4.506610870361328, -36, -128, 127, torch.int8);  quantize_per_tensor_default_56 = None
        
        # No stacktrace found for following nodes
        _scale_46 = self._scale_46
        _zero_point_46 = self._zero_point_46
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_46 = self._frozen_param46
        dequantize_per_channel_default_46 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_46, _scale_46, _zero_point_46, 0, -127, 127, torch.int8);  quantize_per_channel_default_46 = _scale_46 = _zero_point_46 = None
        
        # No stacktrace found for following nodes
        _scale_47 = self._scale_47
        _zero_point_47 = self._zero_point_47
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_47 = self._frozen_param47
        dequantize_per_channel_default_47 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_47, _scale_47, _zero_point_47, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_47 = _scale_47 = _zero_point_47 = None
        conv2d_23: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_81, dequantize_per_channel_default_46, dequantize_per_channel_default_47);  dequantize_per_tensor_default_81 = dequantize_per_channel_default_46 = dequantize_per_channel_default_47 = None
        quantize_per_tensor_default_57 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_23, 6.354524612426758, 86, -128, 127, torch.int8);  conv2d_23 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_57 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_57, 6.354524612426758, 86, -128, 127, torch.int8);  quantize_per_tensor_default_57 = None
        relu__23: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_57);  dequantize_per_tensor_default_57 = None
        quantize_per_tensor_default_58 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__23, 6.354524612426758, 86, -128, 127, torch.int8);  relu__23 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_58 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_58, 6.354524612426758, 86, -128, 127, torch.int8);  quantize_per_tensor_default_58 = None
        _scale_48 = self._scale_48
        _zero_point_48 = self._zero_point_48
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_48 = self._frozen_param48
        dequantize_per_channel_default_48 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_48, _scale_48, _zero_point_48, 0, -127, 127, torch.int8);  quantize_per_channel_default_48 = _scale_48 = _zero_point_48 = None
        
        # No stacktrace found for following nodes
        _scale_49 = self._scale_49
        _zero_point_49 = self._zero_point_49
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_49 = self._frozen_param49
        dequantize_per_channel_default_49 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_49, _scale_49, _zero_point_49, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_49 = _scale_49 = _zero_point_49 = None
        conv2d_24: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_82, dequantize_per_channel_default_48, dequantize_per_channel_default_49, [1, 1], [1, 1]);  dequantize_per_tensor_default_82 = dequantize_per_channel_default_48 = dequantize_per_channel_default_49 = None
        quantize_per_tensor_default_59 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_24, 6.354524612426758, 86, -128, 127, torch.int8);  conv2d_24 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_59 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_59, 6.354524612426758, 86, -128, 127, torch.int8);  quantize_per_tensor_default_59 = None
        relu__24: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_59);  dequantize_per_tensor_default_59 = None
        quantize_per_tensor_default_60 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__24, 6.354524612426758, 86, -128, 127, torch.int8);  relu__24 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_60 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_60, 6.354524612426758, 86, -128, 127, torch.int8);  quantize_per_tensor_default_60 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_7: "f32[1, 512, 13, 13]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_58, dequantize_per_tensor_default_60], 1);  dequantize_per_tensor_default_58 = dequantize_per_tensor_default_60 = None
        quantize_per_tensor_default_61 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_7, 6.354524612426758, 86, -128, 127, torch.int8);  cat_7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
        dequantize_per_tensor_default_61 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_61, 6.354524612426758, 86, -128, 127, torch.int8);  quantize_per_tensor_default_61 = None
        dropout: "f32[1, 512, 13, 13]" = torch.ops.aten.dropout.default(dequantize_per_tensor_default_61, 0.5, False);  dequantize_per_tensor_default_61 = None
        quantize_per_tensor_default_62 = torch.ops.quantized_decomposed.quantize_per_tensor.default(dropout, 6.354524612426758, 86, -128, 127, torch.int8);  dropout = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_62 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_62, 6.354524612426758, 86, -128, 127, torch.int8);  quantize_per_tensor_default_62 = None
        
        # No stacktrace found for following nodes
        _scale_50 = self._scale_50
        _zero_point_50 = self._zero_point_50
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_50 = self._frozen_param50
        dequantize_per_channel_default_50 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_50, _scale_50, _zero_point_50, 0, -127, 127, torch.int8);  quantize_per_channel_default_50 = _scale_50 = _zero_point_50 = None
        
        # No stacktrace found for following nodes
        _scale_51 = self._scale_51
        _zero_point_51 = self._zero_point_51
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_51 = self._frozen_param51
        dequantize_per_channel_default_51 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_51, _scale_51, _zero_point_51, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_51 = _scale_51 = _zero_point_51 = None
        conv2d_25: "f32[1, 4, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_62, dequantize_per_channel_default_50, dequantize_per_channel_default_51);  dequantize_per_tensor_default_62 = dequantize_per_channel_default_50 = dequantize_per_channel_default_51 = None
        quantize_per_tensor_default_63 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_25, 0.18244418501853943, -55, -128, 127, torch.int8);  conv2d_25 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_63 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_63, 0.18244418501853943, -55, -128, 127, torch.int8);  quantize_per_tensor_default_63 = None
        relu__25: "f32[1, 4, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_63);  dequantize_per_tensor_default_63 = None
        quantize_per_tensor_default_64 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__25, 0.18244418501853943, -55, -128, 127, torch.int8);  relu__25 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:1500 in forward, code: return F.adaptive_avg_pool2d(input, self.output_size)
        dequantize_per_tensor_default_64 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_64, 0.18244418501853943, -55, -128, 127, torch.int8);  quantize_per_tensor_default_64 = None
        adaptive_avg_pool2d: "f32[1, 4, 1, 1]" = torch.ops.aten.adaptive_avg_pool2d.default(dequantize_per_tensor_default_64, [1, 1]);  dequantize_per_tensor_default_64 = None
        quantize_per_tensor_default_65 = torch.ops.quantized_decomposed.quantize_per_tensor.default(adaptive_avg_pool2d, 0.18244418501853943, -55, -128, 127, torch.int8);  adaptive_avg_pool2d = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:97 in forward, code: return torch.flatten(x, 1)
        dequantize_per_tensor_default_65 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_65, 0.18244418501853943, -55, -128, 127, torch.int8);  quantize_per_tensor_default_65 = None
        flatten: "f32[1, 4]" = torch.ops.aten.flatten.using_ints(dequantize_per_tensor_default_65, 1);  dequantize_per_tensor_default_65 = None
        quantize_per_tensor_default_66 = torch.ops.quantized_decomposed.quantize_per_tensor.default(flatten, 0.18244418501853943, -55, -128, 127, torch.int8);  flatten = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_66 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_66, 0.18244418501853943, -55, -128, 127, torch.int8);  quantize_per_tensor_default_66 = None
        return pytree.tree_unflatten((dequantize_per_tensor_default_66,), self._out_spec)
        WARNING:executorch.backends.arm._passes.to_tosa_memory_format_pass:Ignoring dim_order kwarg '[0, 1, 2, 3]' for 'dim_order_ops__clone_dim_order_default'.

[ Before Graph Optimisation ]
0     Const                const-1                       
1     Const                const-0                       
2     Const                aten_convolution_default_25_output_zp
3     Const                aten_convolution_default_25_input_zp
4     Const                aten_convolution_default_25_shifts
5     Const                aten_convolution_default_25_multipliers
6     Const                layer-26_weight_zp            
7     Const                layer-26_input_zp             
8     Const                b__frozen_param51             
9     Const                b__frozen_param50             
10    Const                aten_convolution_default_23_output_zp
11    Const                aten_convolution_default_23_input_zp
12    Const                aten_convolution_default_23_shifts
13    Const                aten_convolution_default_23_multipliers
14    Const                layer-24_weight_zp            
15    Const                layer-24_input_zp             
16    Const                b__frozen_param49             
17    Const                b__frozen_param48             
18    Const                aten_convolution_default_22_output_zp
19    Const                aten_convolution_default_22_input_zp
20    Const                aten_convolution_default_22_shifts
21    Const                aten_convolution_default_22_multipliers
22    Const                layer-23_weight_zp            
23    Const                layer-23_input_zp             
24    Const                b__frozen_param45             
25    Const                b__frozen_param44             
26    Const                aten_convolution_default_20_output_zp
27    Const                aten_convolution_default_20_input_zp
28    Const                aten_convolution_default_20_shifts
29    Const                aten_convolution_default_20_multipliers
30    Const                layer-21_weight_zp            
31    Const                layer-21_input_zp             
32    Const                b__frozen_param43             
33    Const                b__frozen_param42             
34    Const                aten_convolution_default_19_output_zp
35    Const                aten_convolution_default_19_input_zp
36    Const                aten_convolution_default_19_shifts
37    Const                aten_convolution_default_19_multipliers
38    Const                layer-20_weight_zp            
39    Const                layer-20_input_zp             
40    Const                b__frozen_param39             
41    Const                b__frozen_param38             
42    Const                aten_convolution_default_17_output_zp
43    Const                aten_convolution_default_17_input_zp
44    Const                aten_convolution_default_17_shifts
45    Const                aten_convolution_default_17_multipliers
46    Const                layer-18_weight_zp            
47    Const                layer-18_input_zp             
48    Const                b__frozen_param37             
49    Const                b__frozen_param36             
50    Const                aten_convolution_default_16_output_zp
51    Const                aten_convolution_default_16_input_zp
52    Const                aten_convolution_default_16_shifts
53    Const                aten_convolution_default_16_multipliers
54    Const                layer-17_weight_zp            
55    Const                layer-17_input_zp             
56    Const                b__frozen_param33             
57    Const                b__frozen_param32             
58    Const                aten_convolution_default_14_output_zp
59    Const                aten_convolution_default_14_input_zp
60    Const                aten_convolution_default_14_shifts
61    Const                aten_convolution_default_14_multipliers
62    Const                layer-15_weight_zp            
63    Const                layer-15_input_zp             
64    Const                b__frozen_param31             
65    Const                b__frozen_param30             
66    Const                aten_convolution_default_13_output_zp
67    Const                aten_convolution_default_13_input_zp
68    Const                aten_convolution_default_13_shifts
69    Const                aten_convolution_default_13_multipliers
70    Const                layer-14_weight_zp            
71    Const                layer-14_input_zp             
72    Const                b__frozen_param27             
73    Const                b__frozen_param26             
74    Const                aten_convolution_default_11_output_zp
75    Const                aten_convolution_default_11_input_zp
76    Const                aten_convolution_default_11_shifts
77    Const                aten_convolution_default_11_multipliers
78    Const                layer-12_weight_zp            
79    Const                layer-12_input_zp             
80    Const                b__frozen_param25             
81    Const                b__frozen_param24             
82    Const                aten_convolution_default_10_output_zp
83    Const                aten_convolution_default_10_input_zp
84    Const                aten_convolution_default_10_shifts
85    Const                aten_convolution_default_10_multipliers
86    Const                layer-11_weight_zp            
87    Const                layer-11_input_zp             
88    Const                b__frozen_param21             
89    Const                b__frozen_param20             
90    Const                aten_convolution_default_8_output_zp
91    Const                aten_convolution_default_8_input_zp
92    Const                aten_convolution_default_8_shifts
93    Const                aten_convolution_default_8_multipliers
94    Const                layer-9_weight_zp             
95    Const                layer-9_input_zp              
96    Const                b__frozen_param19             
97    Const                b__frozen_param18             
98    Const                aten_convolution_default_7_output_zp
99    Const                aten_convolution_default_7_input_zp
100   Const                aten_convolution_default_7_shifts
101   Const                aten_convolution_default_7_multipliers
102   Const                layer-8_weight_zp             
103   Const                layer-8_input_zp              
104   Const                b__frozen_param15             
105   Const                b__frozen_param14             
106   Const                aten_convolution_default_5_output_zp
107   Const                aten_convolution_default_5_input_zp
108   Const                aten_convolution_default_5_shifts
109   Const                aten_convolution_default_5_multipliers
110   Const                layer-6_weight_zp             
111   Const                layer-6_input_zp              
112   Const                b__frozen_param13             
113   Const                b__frozen_param12             
114   Const                aten_convolution_default_4_output_zp
115   Const                aten_convolution_default_4_input_zp
116   Const                aten_convolution_default_4_shifts
117   Const                aten_convolution_default_4_multipliers
118   Const                layer-5_weight_zp             
119   Const                layer-5_input_zp              
120   Const                b__frozen_param9              
121   Const                b__frozen_param8              
122   Const                aten_convolution_default_2_output_zp
123   Const                aten_convolution_default_2_input_zp
124   Const                aten_convolution_default_2_shifts
125   Const                aten_convolution_default_2_multipliers
126   Const                layer-3_weight_zp             
127   Const                layer-3_input_zp              
128   Const                b__frozen_param7              
129   Const                b__frozen_param6              
130   Const                aten_convolution_default_1_output_zp
131   Const                aten_convolution_default_1_input_zp
132   Const                aten_convolution_default_1_shifts
133   Const                aten_convolution_default_1_multipliers
134   Const                layer-2_weight_zp             
135   Const                layer-2_input_zp              
136   Const                b__frozen_param3              
137   Const                b__frozen_param2              
138   Const                aten_convolution_default_output_zp
139   Const                aten_convolution_default_input_zp
140   Const                aten_convolution_default_shifts
141   Const                aten_convolution_default_multipliers
142   Const                layer-1_weight_zp             
143   Const                layer-1_input_zp              
144   Const                b__frozen_param1              
145   Const                b__frozen_param0              
146   Transpose            tosa_transpose_default        
147   Slice                aten_slice_copy_tensor        
148   Slice                aten_slice_copy_tensor_1      
149   Conv2D               layer-1                       
150   Rescale              aten_convolution_default      
151   Clamp                aten_clamp_default            
152   MaxPool              aten_max_pool2d_default       
153   Conv2D               layer-2                       
154   Rescale              aten_convolution_default_1    
155   Clamp                aten_clamp_default_1          
156   Conv2D               layer-3                       
157   Rescale              aten_convolution_default_2    
158   Clamp                aten_clamp_default_2          
159   Const                aten_convolution_default_3_output_zp
160   Const                aten_convolution_default_3_input_zp
161   Const                aten_convolution_default_3_shifts
162   Const                aten_convolution_default_3_multipliers
163   Const                layer-4_weight_zp             
164   Const                layer-4_input_zp              
165   Const                b__frozen_param5              
166   Const                b__frozen_param4              
167   Conv2D               layer-4                       
168   Rescale              aten_convolution_default_3    
169   Clamp                aten_clamp_default_3          
170   Concat               aten_cat_default              
171   Conv2D               layer-5                       
172   Rescale              aten_convolution_default_4    
173   Clamp                aten_clamp_default_4          
174   Conv2D               layer-6                       
175   Rescale              aten_convolution_default_5    
176   Clamp                aten_clamp_default_5          
177   Const                aten_convolution_default_6_output_zp
178   Const                aten_convolution_default_6_input_zp
179   Const                aten_convolution_default_6_shifts
180   Const                aten_convolution_default_6_multipliers
181   Const                layer-7_weight_zp             
182   Const                layer-7_input_zp              
183   Const                b__frozen_param11             
184   Const                b__frozen_param10             
185   Conv2D               layer-7                       
186   Rescale              aten_convolution_default_6    
187   Clamp                aten_clamp_default_6          
188   Concat               aten_cat_default_1            
189   MaxPool              aten_max_pool2d_default_1     
190   Conv2D               layer-8                       
191   Rescale              aten_convolution_default_7    
192   Clamp                aten_clamp_default_7          
193   Conv2D               layer-9                       
194   Rescale              aten_convolution_default_8    
195   Clamp                aten_clamp_default_8          
196   Const                aten_convolution_default_9_output_zp
197   Const                aten_convolution_default_9_input_zp
198   Const                aten_convolution_default_9_shifts
199   Const                aten_convolution_default_9_multipliers
200   Const                layer-10_weight_zp            
201   Const                layer-10_input_zp             
202   Const                b__frozen_param17             
203   Const                b__frozen_param16             
204   Conv2D               layer-10                      
205   Rescale              aten_convolution_default_9    
206   Clamp                aten_clamp_default_9          
207   Concat               aten_cat_default_2            
208   Conv2D               layer-11                      
209   Rescale              aten_convolution_default_10   
210   Clamp                aten_clamp_default_10         
211   Conv2D               layer-12                      
212   Rescale              aten_convolution_default_11   
213   Clamp                aten_clamp_default_11         
214   Const                aten_convolution_default_12_output_zp
215   Const                aten_convolution_default_12_input_zp
216   Const                aten_convolution_default_12_shifts
217   Const                aten_convolution_default_12_multipliers
218   Const                layer-13_weight_zp            
219   Const                layer-13_input_zp             
220   Const                b__frozen_param23             
221   Const                b__frozen_param22             
222   Conv2D               layer-13                      
223   Rescale              aten_convolution_default_12   
224   Clamp                aten_clamp_default_12         
225   Concat               aten_cat_default_3            
226   MaxPool              aten_max_pool2d_default_2     
227   Conv2D               layer-14                      
228   Rescale              aten_convolution_default_13   
229   Clamp                aten_clamp_default_13         
230   Conv2D               layer-15                      
231   Rescale              aten_convolution_default_14   
232   Clamp                aten_clamp_default_14         
233   Const                aten_convolution_default_15_output_zp
234   Const                aten_convolution_default_15_input_zp
235   Const                aten_convolution_default_15_shifts
236   Const                aten_convolution_default_15_multipliers
237   Const                layer-16_weight_zp            
238   Const                layer-16_input_zp             
239   Const                b__frozen_param29             
240   Const                b__frozen_param28             
241   Conv2D               layer-16                      
242   Rescale              aten_convolution_default_15   
243   Clamp                aten_clamp_default_15         
244   Concat               aten_cat_default_4            
245   Conv2D               layer-17                      
246   Rescale              aten_convolution_default_16   
247   Clamp                aten_clamp_default_16         
248   Conv2D               layer-18                      
249   Rescale              aten_convolution_default_17   
250   Clamp                aten_clamp_default_17         
251   Const                aten_convolution_default_18_output_zp
252   Const                aten_convolution_default_18_input_zp
253   Const                aten_convolution_default_18_shifts
254   Const                aten_convolution_default_18_multipliers
255   Const                layer-19_weight_zp            
256   Const                layer-19_input_zp             
257   Const                b__frozen_param35             
258   Const                b__frozen_param34             
259   Conv2D               layer-19                      
260   Rescale              aten_convolution_default_18   
261   Clamp                aten_clamp_default_18         
262   Concat               aten_cat_default_5            
263   Conv2D               layer-20                      
264   Rescale              aten_convolution_default_19   
265   Clamp                aten_clamp_default_19         
266   Conv2D               layer-21                      
267   Rescale              aten_convolution_default_20   
268   Clamp                aten_clamp_default_20         
269   Const                aten_convolution_default_21_output_zp
270   Const                aten_convolution_default_21_input_zp
271   Const                aten_convolution_default_21_shifts
272   Const                aten_convolution_default_21_multipliers
273   Const                layer-22_weight_zp            
274   Const                layer-22_input_zp             
275   Const                b__frozen_param41             
276   Const                b__frozen_param40             
277   Conv2D               layer-22                      
278   Rescale              aten_convolution_default_21   
279   Clamp                aten_clamp_default_21         
280   Concat               aten_cat_default_6            
281   Conv2D               layer-23                      
282   Rescale              aten_convolution_default_22   
283   Clamp                aten_clamp_default_22         
284   Conv2D               layer-24                      
285   Rescale              aten_convolution_default_23   
286   Clamp                aten_clamp_default_23         
287   Const                aten_convolution_default_24_output_zp
288   Const                aten_convolution_default_24_input_zp
289   Const                aten_convolution_default_24_shifts
290   Const                aten_convolution_default_24_multipliers
291   Const                layer-25_weight_zp            
292   Const                layer-25_input_zp             
293   Const                b__frozen_param47             
294   Const                b__frozen_param46             
295   Conv2D               layer-25                      
296   Rescale              aten_convolution_default_24   
297   Clamp                aten_clamp_default_24         
298   Concat               aten_cat_default_7            
299   Conv2D               layer-26                      
300   Rescale              aten_convolution_default_25   
301   Clamp                aten_clamp_default_25         
302   AvgPool              aten_avg_pool2d_default       
303   Reshape              aten_view_copy_default        


[ After Graph Optimization ]
0     Const                const-1                       
1     Const                const-0                       
2     Const                aten_convolution_default_25_output_zp
3     Const                aten_convolution_default_25_input_zp
4     Const                aten_convolution_default_25_shifts
5     Const                aten_convolution_default_25_multipliers
6     Const                layer-26_weight_zp            
7     Const                layer-26_input_zp             
8     Const                b__frozen_param51             
9     Const                b__frozen_param50             
10    Const                aten_convolution_default_23_output_zp
11    Const                aten_convolution_default_23_input_zp
12    Const                aten_convolution_default_23_shifts
13    Const                aten_convolution_default_23_multipliers
14    Const                layer-24_weight_zp            
15    Const                layer-24_input_zp             
16    Const                b__frozen_param49             
17    Const                b__frozen_param48             
18    Const                aten_convolution_default_22_output_zp
19    Const                aten_convolution_default_22_input_zp
20    Const                aten_convolution_default_22_shifts
21    Const                aten_convolution_default_22_multipliers
22    Const                layer-23_weight_zp            
23    Const                layer-23_input_zp             
24    Const                b__frozen_param45             
25    Const                b__frozen_param44             
26    Const                aten_convolution_default_20_output_zp
27    Const                aten_convolution_default_20_input_zp
28    Const                aten_convolution_default_20_shifts
29    Const                aten_convolution_default_20_multipliers
30    Const                layer-21_weight_zp            
31    Const                layer-21_input_zp             
32    Const                b__frozen_param43             
33    Const                b__frozen_param42             
34    Const                aten_convolution_default_19_output_zp
35    Const                aten_convolution_default_19_input_zp
36    Const                aten_convolution_default_19_shifts
37    Const                aten_convolution_default_19_multipliers
38    Const                layer-20_weight_zp            
39    Const                layer-20_input_zp             
40    Const                b__frozen_param39             
41    Const                b__frozen_param38             
42    Const                aten_convolution_default_17_output_zp
43    Const                aten_convolution_default_17_input_zp
44    Const                aten_convolution_default_17_shifts
45    Const                aten_convolution_default_17_multipliers
46    Const                layer-18_weight_zp            
47    Const                layer-18_input_zp             
48    Const                b__frozen_param37             
49    Const                b__frozen_param36             
50    Const                aten_convolution_default_16_output_zp
51    Const                aten_convolution_default_16_input_zp
52    Const                aten_convolution_default_16_shifts
53    Const                aten_convolution_default_16_multipliers
54    Const                layer-17_weight_zp            
55    Const                layer-17_input_zp             
56    Const                b__frozen_param33             
57    Const                b__frozen_param32             
58    Const                aten_convolution_default_14_output_zp
59    Const                aten_convolution_default_14_input_zp
60    Const                aten_convolution_default_14_shifts
61    Const                aten_convolution_default_14_multipliers
62    Const                layer-15_weight_zp            
63    Const                layer-15_input_zp             
64    Const                b__frozen_param31             
65    Const                b__frozen_param30             
66    Const                aten_convolution_default_13_output_zp
67    Const                aten_convolution_default_13_input_zp
68    Const                aten_convolution_default_13_shifts
69    Const                aten_convolution_default_13_multipliers
70    Const                layer-14_weight_zp            
71    Const                layer-14_input_zp             
72    Const                b__frozen_param27             
73    Const                b__frozen_param26             
74    Const                aten_convolution_default_11_output_zp
75    Const                aten_convolution_default_11_input_zp
76    Const                aten_convolution_default_11_shifts
77    Const                aten_convolution_default_11_multipliers
78    Const                layer-12_weight_zp            
79    Const                layer-12_input_zp             
80    Const                b__frozen_param25             
81    Const                b__frozen_param24             
82    Const                aten_convolution_default_10_output_zp
83    Const                aten_convolution_default_10_input_zp
84    Const                aten_convolution_default_10_shifts
85    Const                aten_convolution_default_10_multipliers
86    Const                layer-11_weight_zp            
87    Const                layer-11_input_zp             
88    Const                b__frozen_param21             
89    Const                b__frozen_param20             
90    Const                aten_convolution_default_8_output_zp
91    Const                aten_convolution_default_8_input_zp
92    Const                aten_convolution_default_8_shifts
93    Const                aten_convolution_default_8_multipliers
94    Const                layer-9_weight_zp             
95    Const                layer-9_input_zp              
96    Const                b__frozen_param19             
97    Const                b__frozen_param18             
98    Const                aten_convolution_default_7_output_zp
99    Const                aten_convolution_default_7_input_zp
100   Const                aten_convolution_default_7_shifts
101   Const                aten_convolution_default_7_multipliers
102   Const                layer-8_weight_zp             
103   Const                layer-8_input_zp              
104   Const                b__frozen_param15             
105   Const                b__frozen_param14             
106   Const                aten_convolution_default_5_output_zp
107   Const                aten_convolution_default_5_input_zp
108   Const                aten_convolution_default_5_shifts
109   Const                aten_convolution_default_5_multipliers
110   Const                layer-6_weight_zp             
111   Const                layer-6_input_zp              
112   Const                b__frozen_param13             
113   Const                b__frozen_param12             
114   Const                aten_convolution_default_4_output_zp
115   Const                aten_convolution_default_4_input_zp
116   Const                aten_convolution_default_4_shifts
117   Const                aten_convolution_default_4_multipliers
118   Const                layer-5_weight_zp             
119   Const                layer-5_input_zp              
120   Const                b__frozen_param9              
121   Const                b__frozen_param8              
122   Const                aten_convolution_default_2_output_zp
123   Const                aten_convolution_default_2_input_zp
124   Const                aten_convolution_default_2_shifts
125   Const                aten_convolution_default_2_multipliers
126   Const                layer-3_weight_zp             
127   Const                layer-3_input_zp              
128   Const                b__frozen_param7              
129   Const                b__frozen_param6              
130   Const                aten_convolution_default_1_output_zp
131   Const                aten_convolution_default_1_input_zp
132   Const                aten_convolution_default_1_shifts
133   Const                aten_convolution_default_1_multipliers
134   Const                layer-2_weight_zp             
135   Const                layer-2_input_zp              
136   Const                b__frozen_param3              
137   Const                b__frozen_param2              
138   Const                aten_convolution_default_output_zp
139   Const                aten_convolution_default_input_zp
140   Const                aten_convolution_default_shifts
141   Const                aten_convolution_default_multipliers
142   Const                layer-1_weight_zp             
143   Const                layer-1_input_zp              
144   Const                b__frozen_param1              
145   Const                b__frozen_param0              
146   Transpose            tosa_transpose_default        
147   Slice                aten_slice_copy_tensor        
148   Slice                aten_slice_copy_tensor_1      
149   Conv2D               layer-1                       
150   Rescale              aten_convolution_default      
151   Clamp                aten_clamp_default            
152   MaxPool              aten_max_pool2d_default       
153   Conv2D               layer-2                       
154   Rescale              aten_convolution_default_1    
155   Clamp                aten_clamp_default_1          
156   Conv2D               layer-3                       
157   Rescale              aten_convolution_default_2    
158   Clamp                aten_clamp_default_2          
159   Const                aten_convolution_default_3_output_zp
160   Const                aten_convolution_default_3_input_zp
161   Const                aten_convolution_default_3_shifts
162   Const                aten_convolution_default_3_multipliers
163   Const                layer-4_weight_zp             
164   Const                layer-4_input_zp              
165   Const                b__frozen_param5              
166   Const                b__frozen_param4              
167   Conv2D               layer-4                       
168   Rescale              aten_convolution_default_3    
169   Clamp                aten_clamp_default_3          
170   Concat               aten_cat_default              
171   Conv2D               layer-5                       
172   Rescale              aten_convolution_default_4    
173   Clamp                aten_clamp_default_4          
174   Conv2D               layer-6                       
175   Rescale              aten_convolution_default_5    
176   Clamp                aten_clamp_default_5          
177   Const                aten_convolution_default_6_output_zp
178   Const                aten_convolution_default_6_input_zp
179   Const                aten_convolution_default_6_shifts
180   Const                aten_convolution_default_6_multipliers
181   Const                layer-7_weight_zp             
182   Const                layer-7_input_zp              
183   Const                b__frozen_param11             
184   Const                b__frozen_param10             
185   Conv2D               layer-7                       
186   Rescale              aten_convolution_default_6    
187   Clamp                aten_clamp_default_6          
188   Concat               aten_cat_default_1            
189   MaxPool              aten_max_pool2d_default_1     
190   Conv2D               layer-8                       
191   Rescale              aten_convolution_default_7    
192   Clamp                aten_clamp_default_7          
193   Conv2D               layer-9                       
194   Rescale              aten_convolution_default_8    
195   Clamp                aten_clamp_default_8          
196   Const                aten_convolution_default_9_output_zp
197   Const                aten_convolution_default_9_input_zp
198   Const                aten_convolution_default_9_shifts
199   Const                aten_convolution_default_9_multipliers
200   Const                layer-10_weight_zp            
201   Const                layer-10_input_zp             
202   Const                b__frozen_param17             
203   Const                b__frozen_param16             
204   Conv2D               layer-10                      
205   Rescale              aten_convolution_default_9    
206   Clamp                aten_clamp_default_9          
207   Concat               aten_cat_default_2            
208   Conv2D               layer-11                      
209   Rescale              aten_convolution_default_10   
210   Clamp                aten_clamp_default_10         
211   Conv2D               layer-12                      
212   Rescale              aten_convolution_default_11   
213   Clamp                aten_clamp_default_11         
214   Const                aten_convolution_default_12_output_zp
215   Const                aten_convolution_default_12_input_zp
216   Const                aten_convolution_default_12_shifts
217   Const                aten_convolution_default_12_multipliers
218   Const                layer-13_weight_zp            
219   Const                layer-13_input_zp             
220   Const                b__frozen_param23             
221   Const                b__frozen_param22             
222   Conv2D               layer-13                      
223   Rescale              aten_convolution_default_12   
224   Clamp                aten_clamp_default_12         
225   Concat               aten_cat_default_3            
226   MaxPool              aten_max_pool2d_default_2     
227   Conv2D               layer-14                      
228   Rescale              aten_convolution_default_13   
229   Clamp                aten_clamp_default_13         
230   Conv2D               layer-15                      
231   Rescale              aten_convolution_default_14   
232   Clamp                aten_clamp_default_14         
233   Const                aten_convolution_default_15_output_zp
234   Const                aten_convolution_default_15_input_zp
235   Const                aten_convolution_default_15_shifts
236   Const                aten_convolution_default_15_multipliers
237   Const                layer-16_weight_zp            
238   Const                layer-16_input_zp             
239   Const                b__frozen_param29             
240   Const                b__frozen_param28             
241   Conv2D               layer-16                      
242   Rescale              aten_convolution_default_15   
243   Clamp                aten_clamp_default_15         
244   Concat               aten_cat_default_4            
245   Conv2D               layer-17                      
246   Rescale              aten_convolution_default_16   
247   Clamp                aten_clamp_default_16         
248   Conv2D               layer-18                      
249   Rescale              aten_convolution_default_17   
250   Clamp                aten_clamp_default_17         
251   Const                aten_convolution_default_18_output_zp
252   Const                aten_convolution_default_18_input_zp
253   Const                aten_convolution_default_18_shifts
254   Const                aten_convolution_default_18_multipliers
255   Const                layer-19_weight_zp            
256   Const                layer-19_input_zp             
257   Const                b__frozen_param35             
258   Const                b__frozen_param34             
259   Conv2D               layer-19                      
260   Rescale              aten_convolution_default_18   
261   Clamp                aten_clamp_default_18         
262   Concat               aten_cat_default_5            
263   Conv2D               layer-20                      
264   Rescale              aten_convolution_default_19   
265   Clamp                aten_clamp_default_19         
266   Conv2D               layer-21                      
267   Rescale              aten_convolution_default_20   
268   Clamp                aten_clamp_default_20         
269   Const                aten_convolution_default_21_output_zp
270   Const                aten_convolution_default_21_input_zp
271   Const                aten_convolution_default_21_shifts
272   Const                aten_convolution_default_21_multipliers
273   Const                layer-22_weight_zp            
274   Const                layer-22_input_zp             
275   Const                b__frozen_param41             
276   Const                b__frozen_param40             
277   Conv2D               layer-22                      
278   Rescale              aten_convolution_default_21   
279   Clamp                aten_clamp_default_21         
280   Concat               aten_cat_default_6            
281   Conv2D               layer-23                      
282   Rescale              aten_convolution_default_22   
283   Clamp                aten_clamp_default_22         
284   Conv2D               layer-24                      
285   Rescale              aten_convolution_default_23   
286   Clamp                aten_clamp_default_23         
287   Const                aten_convolution_default_24_output_zp
288   Const                aten_convolution_default_24_input_zp
289   Const                aten_convolution_default_24_shifts
290   Const                aten_convolution_default_24_multipliers
291   Const                layer-25_weight_zp            
292   Const                layer-25_input_zp             
293   Const                b__frozen_param47             
294   Const                b__frozen_param46             
295   Conv2D               layer-25                      
296   Rescale              aten_convolution_default_24   
297   Clamp                aten_clamp_default_24         
298   Concat               aten_cat_default_7            
299   Conv2D               layer-26                      
300   Rescale              aten_convolution_default_25   
301   Clamp                aten_clamp_default_25         
302   AvgPool              aten_avg_pool2d_default       
303   Reshape              aten_view_copy_default        


[ Graph With Tensor Quantization ]
0 Const const-1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const-1
1 Const const-0
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const-0
2 Const aten_convolution_default_25_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_output_zp
3 Const aten_convolution_default_25_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_input_zp
4 Const aten_convolution_default_25_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_shifts
5 Const aten_convolution_default_25_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_multipliers
6 Const layer-26_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26_weight_zp
7 Const layer-26_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26_input_zp
8 Const b__frozen_param51
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param51
9 Const b__frozen_param50
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param50
10 Const aten_convolution_default_23_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_output_zp
11 Const aten_convolution_default_23_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_input_zp
12 Const aten_convolution_default_23_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_shifts
13 Const aten_convolution_default_23_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_multipliers
14 Const layer-24_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24_weight_zp
15 Const layer-24_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24_input_zp
16 Const b__frozen_param49
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param49
17 Const b__frozen_param48
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param48
18 Const aten_convolution_default_22_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_output_zp
19 Const aten_convolution_default_22_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_input_zp
20 Const aten_convolution_default_22_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_shifts
21 Const aten_convolution_default_22_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_multipliers
22 Const layer-23_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23_weight_zp
23 Const layer-23_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23_input_zp
24 Const b__frozen_param45
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param45
25 Const b__frozen_param44
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param44
26 Const aten_convolution_default_20_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_output_zp
27 Const aten_convolution_default_20_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_input_zp
28 Const aten_convolution_default_20_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_shifts
29 Const aten_convolution_default_20_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_multipliers
30 Const layer-21_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21_weight_zp
31 Const layer-21_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21_input_zp
32 Const b__frozen_param43
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param43
33 Const b__frozen_param42
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param42
34 Const aten_convolution_default_19_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_output_zp
35 Const aten_convolution_default_19_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_input_zp
36 Const aten_convolution_default_19_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_shifts
37 Const aten_convolution_default_19_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_multipliers
38 Const layer-20_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20_weight_zp
39 Const layer-20_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20_input_zp
40 Const b__frozen_param39
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param39
41 Const b__frozen_param38
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param38
42 Const aten_convolution_default_17_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_output_zp
43 Const aten_convolution_default_17_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_input_zp
44 Const aten_convolution_default_17_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_shifts
45 Const aten_convolution_default_17_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_multipliers
46 Const layer-18_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18_weight_zp
47 Const layer-18_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18_input_zp
48 Const b__frozen_param37
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param37
49 Const b__frozen_param36
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param36
50 Const aten_convolution_default_16_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_output_zp
51 Const aten_convolution_default_16_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_input_zp
52 Const aten_convolution_default_16_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_shifts
53 Const aten_convolution_default_16_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_multipliers
54 Const layer-17_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17_weight_zp
55 Const layer-17_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17_input_zp
56 Const b__frozen_param33
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param33
57 Const b__frozen_param32
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param32
58 Const aten_convolution_default_14_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_output_zp
59 Const aten_convolution_default_14_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_input_zp
60 Const aten_convolution_default_14_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_shifts
61 Const aten_convolution_default_14_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_multipliers
62 Const layer-15_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15_weight_zp
63 Const layer-15_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15_input_zp
64 Const b__frozen_param31
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param31
65 Const b__frozen_param30
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param30
66 Const aten_convolution_default_13_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_output_zp
67 Const aten_convolution_default_13_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_input_zp
68 Const aten_convolution_default_13_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_shifts
69 Const aten_convolution_default_13_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_multipliers
70 Const layer-14_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14_weight_zp
71 Const layer-14_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14_input_zp
72 Const b__frozen_param27
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param27
73 Const b__frozen_param26
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param26
74 Const aten_convolution_default_11_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_output_zp
75 Const aten_convolution_default_11_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_input_zp
76 Const aten_convolution_default_11_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_shifts
77 Const aten_convolution_default_11_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_multipliers
78 Const layer-12_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12_weight_zp
79 Const layer-12_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12_input_zp
80 Const b__frozen_param25
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param25
81 Const b__frozen_param24
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param24
82 Const aten_convolution_default_10_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_output_zp
83 Const aten_convolution_default_10_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_input_zp
84 Const aten_convolution_default_10_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_shifts
85 Const aten_convolution_default_10_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_multipliers
86 Const layer-11_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11_weight_zp
87 Const layer-11_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11_input_zp
88 Const b__frozen_param21
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param21
89 Const b__frozen_param20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param20
90 Const aten_convolution_default_8_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_output_zp
91 Const aten_convolution_default_8_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_input_zp
92 Const aten_convolution_default_8_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_shifts
93 Const aten_convolution_default_8_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_multipliers
94 Const layer-9_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9_weight_zp
95 Const layer-9_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9_input_zp
96 Const b__frozen_param19
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param19
97 Const b__frozen_param18
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param18
98 Const aten_convolution_default_7_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_output_zp
99 Const aten_convolution_default_7_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_input_zp
100 Const aten_convolution_default_7_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_shifts
101 Const aten_convolution_default_7_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_multipliers
102 Const layer-8_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8_weight_zp
103 Const layer-8_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8_input_zp
104 Const b__frozen_param15
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param15
105 Const b__frozen_param14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param14
106 Const aten_convolution_default_5_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_output_zp
107 Const aten_convolution_default_5_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_input_zp
108 Const aten_convolution_default_5_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_shifts
109 Const aten_convolution_default_5_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_multipliers
110 Const layer-6_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6_weight_zp
111 Const layer-6_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6_input_zp
112 Const b__frozen_param13
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param13
113 Const b__frozen_param12
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param12
114 Const aten_convolution_default_4_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_output_zp
115 Const aten_convolution_default_4_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_input_zp
116 Const aten_convolution_default_4_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_shifts
117 Const aten_convolution_default_4_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_multipliers
118 Const layer-5_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_weight_zp
119 Const layer-5_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_input_zp
120 Const b__frozen_param9
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param9
121 Const b__frozen_param8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param8
122 Const aten_convolution_default_2_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_output_zp
123 Const aten_convolution_default_2_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_input_zp
124 Const aten_convolution_default_2_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_shifts
125 Const aten_convolution_default_2_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_multipliers
126 Const layer-3_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3_weight_zp
127 Const layer-3_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3_input_zp
128 Const b__frozen_param7
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param7
129 Const b__frozen_param6
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param6
130 Const aten_convolution_default_1_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_output_zp
131 Const aten_convolution_default_1_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_input_zp
132 Const aten_convolution_default_1_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_shifts
133 Const aten_convolution_default_1_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_multipliers
134 Const layer-2_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_weight_zp
135 Const layer-2_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_input_zp
136 Const b__frozen_param3
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param3
137 Const b__frozen_param2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param2
138 Const aten_convolution_default_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_output_zp
139 Const aten_convolution_default_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_input_zp
140 Const aten_convolution_default_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_shifts
141 Const aten_convolution_default_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_multipliers
142 Const layer-1_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_weight_zp
143 Const layer-1_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_input_zp
144 Const b__frozen_param1
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param1
145 Const b__frozen_param0
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param0
146 Transpose tosa_transpose_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 quantized_decomposed_quantize_per_tensor_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
147 Slice aten_slice_copy_tensor
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
    Input 01 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_start_shape
    Input 02 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_sizes_shape
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor
148 Slice aten_slice_copy_tensor_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor
    Input 01 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_1_start_shape
    Input 02 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_1_sizes_shape
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_1
149 Conv2D layer-1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [8], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param0
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param1
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1
150 Rescale aten_convolution_default
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-11], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default
151 Clamp aten_clamp_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default
152 MaxPool aten_max_pool2d_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default
153 Conv2D layer-2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-11], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param2
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param3
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2
154 Rescale aten_convolution_default_1
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1
155 Clamp aten_clamp_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
156 Conv2D layer-3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param6
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param7
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3
157 Rescale aten_convolution_default_2
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [43], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2
158 Clamp aten_clamp_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_2
159 Const aten_convolution_default_3_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_output_zp
160 Const aten_convolution_default_3_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_input_zp
161 Const aten_convolution_default_3_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_shifts
162 Const aten_convolution_default_3_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_multipliers
163 Const layer-4_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_weight_zp
164 Const layer-4_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_input_zp
165 Const b__frozen_param5
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param5
166 Const b__frozen_param4
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param4
167 Conv2D layer-4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param4
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param5
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4
168 Rescale aten_convolution_default_3
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [43], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3
169 Clamp aten_clamp_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_3
170 Concat aten_cat_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_3
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
171 Conv2D layer-5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [43], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param8
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param9
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5
172 Rescale aten_convolution_default_4
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-34], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4
173 Clamp aten_clamp_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
174 Conv2D layer-6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-34], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param12
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param13
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6
175 Rescale aten_convolution_default_5
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [14], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5
176 Clamp aten_clamp_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_5
177 Const aten_convolution_default_6_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_output_zp
178 Const aten_convolution_default_6_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_input_zp
179 Const aten_convolution_default_6_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_shifts
180 Const aten_convolution_default_6_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_multipliers
181 Const layer-7_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7_weight_zp
182 Const layer-7_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7_input_zp
183 Const b__frozen_param11
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param11
184 Const b__frozen_param10
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param10
185 Conv2D layer-7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-34], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param10
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param11
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7
186 Rescale aten_convolution_default_6
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [14], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6
187 Clamp aten_clamp_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_6
188 Concat aten_cat_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_6
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_5
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
189 MaxPool aten_max_pool2d_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_1
190 Conv2D layer-8
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [14], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param14
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param15
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8
191 Rescale aten_convolution_default_7
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7
192 Clamp aten_clamp_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
193 Conv2D layer-9
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param18
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param19
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9
194 Rescale aten_convolution_default_8
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [3], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8
195 Clamp aten_clamp_default_8
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_8
196 Const aten_convolution_default_9_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_output_zp
197 Const aten_convolution_default_9_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_input_zp
198 Const aten_convolution_default_9_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_shifts
199 Const aten_convolution_default_9_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_multipliers
200 Const layer-10_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10_weight_zp
201 Const layer-10_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10_input_zp
202 Const b__frozen_param17
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param17
203 Const b__frozen_param16
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param16
204 Conv2D layer-10
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param16
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param17
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10
205 Rescale aten_convolution_default_9
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [3], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9
206 Clamp aten_clamp_default_9
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_9
207 Concat aten_cat_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_9
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
208 Conv2D layer-11
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [3], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param20
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param21
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11
209 Rescale aten_convolution_default_10
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10
210 Clamp aten_clamp_default_10
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
211 Conv2D layer-12
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param24
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param25
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12
212 Rescale aten_convolution_default_11
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [37], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11
213 Clamp aten_clamp_default_11
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_11
214 Const aten_convolution_default_12_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_output_zp
215 Const aten_convolution_default_12_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_input_zp
216 Const aten_convolution_default_12_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_shifts
217 Const aten_convolution_default_12_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_multipliers
218 Const layer-13_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13_weight_zp
219 Const layer-13_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13_input_zp
220 Const b__frozen_param23
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param23
221 Const b__frozen_param22
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param22
222 Conv2D layer-13
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param22
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param23
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13
223 Rescale aten_convolution_default_12
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [37], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12
224 Clamp aten_clamp_default_12
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_12
225 Concat aten_cat_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_12
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_11
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
226 MaxPool aten_max_pool2d_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_2
227 Conv2D layer-14
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [37], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_2
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param26
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param27
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14
228 Rescale aten_convolution_default_13
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-15], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13
229 Clamp aten_clamp_default_13
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
230 Conv2D layer-15
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-15], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param30
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param31
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15
231 Rescale aten_convolution_default_14
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [24], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14
232 Clamp aten_clamp_default_14
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_14
233 Const aten_convolution_default_15_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_output_zp
234 Const aten_convolution_default_15_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_input_zp
235 Const aten_convolution_default_15_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_shifts
236 Const aten_convolution_default_15_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_multipliers
237 Const layer-16_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16_weight_zp
238 Const layer-16_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16_input_zp
239 Const b__frozen_param29
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param29
240 Const b__frozen_param28
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param28
241 Conv2D layer-16
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-15], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param28
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param29
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16
242 Rescale aten_convolution_default_15
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [24], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15
243 Clamp aten_clamp_default_15
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_15
244 Concat aten_cat_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_15
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
245 Conv2D layer-17
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [24], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param32
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param33
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17
246 Rescale aten_convolution_default_16
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-42], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16
247 Clamp aten_clamp_default_16
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
248 Conv2D layer-18
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-42], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param36
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param37
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18
249 Rescale aten_convolution_default_17
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-2], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17
250 Clamp aten_clamp_default_17
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_17
251 Const aten_convolution_default_18_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_output_zp
252 Const aten_convolution_default_18_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_input_zp
253 Const aten_convolution_default_18_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_shifts
254 Const aten_convolution_default_18_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_multipliers
255 Const layer-19_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19_weight_zp
256 Const layer-19_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19_input_zp
257 Const b__frozen_param35
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param35
258 Const b__frozen_param34
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param34
259 Conv2D layer-19
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-42], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param34
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param35
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19
260 Rescale aten_convolution_default_18
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-2], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18
261 Clamp aten_clamp_default_18
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_18
262 Concat aten_cat_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_18
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_17
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
263 Conv2D layer-20
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-2], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param38
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param39
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20
264 Rescale aten_convolution_default_19
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19
265 Clamp aten_clamp_default_19
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
266 Conv2D layer-21
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param42
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param43
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21
267 Rescale aten_convolution_default_20
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-7], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20
268 Clamp aten_clamp_default_20
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_20
269 Const aten_convolution_default_21_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_output_zp
270 Const aten_convolution_default_21_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_input_zp
271 Const aten_convolution_default_21_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_shifts
272 Const aten_convolution_default_21_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_multipliers
273 Const layer-22_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22_weight_zp
274 Const layer-22_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22_input_zp
275 Const b__frozen_param41
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param41
276 Const b__frozen_param40
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param40
277 Conv2D layer-22
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param40
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param41
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22
278 Rescale aten_convolution_default_21
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-7], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21
279 Clamp aten_clamp_default_21
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_21
280 Concat aten_cat_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_21
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
281 Conv2D layer-23
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-7], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param44
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param45
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23
282 Rescale aten_convolution_default_22
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-36], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22
283 Clamp aten_clamp_default_22
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
284 Conv2D layer-24
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-36], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param48
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param49
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24
285 Rescale aten_convolution_default_23
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [86], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23
286 Clamp aten_clamp_default_23
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_23
287 Const aten_convolution_default_24_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_output_zp
288 Const aten_convolution_default_24_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_input_zp
289 Const aten_convolution_default_24_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_shifts
290 Const aten_convolution_default_24_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_multipliers
291 Const layer-25_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25_weight_zp
292 Const layer-25_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25_input_zp
293 Const b__frozen_param47
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param47
294 Const b__frozen_param46
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param46
295 Conv2D layer-25
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-36], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param46
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param47
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25
296 Rescale aten_convolution_default_24
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [86], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24
297 Clamp aten_clamp_default_24
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_24
298 Concat aten_cat_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_24
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_23
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
299 Conv2D layer-26
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [86], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param50
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param51
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26
300 Rescale aten_convolution_default_25
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-55], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25
301 Clamp aten_clamp_default_25
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_25
302 AvgPool aten_avg_pool2d_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-55], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_25
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const-0
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const-1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-55], quantMin: [], quantMax: [], dimension: 0 aten_avg_pool2d_default
303 Reshape aten_view_copy_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_avg_pool2d_default
    Input 01 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_view_copy_default_shape
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_view_copy_default


[ Before Graph Optimisation ]
0     Const                const-1                       
1     Const                const-0                       
2     Const                aten_convolution_default_25_output_zp
3     Const                aten_convolution_default_25_input_zp
4     Const                aten_convolution_default_25_shifts
5     Const                aten_convolution_default_25_multipliers
6     Const                layer-26_weight_zp            
7     Const                layer-26_input_zp             
8     Const                b__frozen_param51             
9     Const                b__frozen_param50             
10    Const                aten_convolution_default_23_output_zp
11    Const                aten_convolution_default_23_input_zp
12    Const                aten_convolution_default_23_shifts
13    Const                aten_convolution_default_23_multipliers
14    Const                layer-24_weight_zp            
15    Const                layer-24_input_zp             
16    Const                b__frozen_param49             
17    Const                b__frozen_param48             
18    Const                aten_convolution_default_22_output_zp
19    Const                aten_convolution_default_22_input_zp
20    Const                aten_convolution_default_22_shifts
21    Const                aten_convolution_default_22_multipliers
22    Const                layer-23_weight_zp            
23    Const                layer-23_input_zp             
24    Const                b__frozen_param45             
25    Const                b__frozen_param44             
26    Const                aten_convolution_default_20_output_zp
27    Const                aten_convolution_default_20_input_zp
28    Const                aten_convolution_default_20_shifts
29    Const                aten_convolution_default_20_multipliers
30    Const                layer-21_weight_zp            
31    Const                layer-21_input_zp             
32    Const                b__frozen_param43             
33    Const                b__frozen_param42             
34    Const                aten_convolution_default_19_output_zp
35    Const                aten_convolution_default_19_input_zp
36    Const                aten_convolution_default_19_shifts
37    Const                aten_convolution_default_19_multipliers
38    Const                layer-20_weight_zp            
39    Const                layer-20_input_zp             
40    Const                b__frozen_param39             
41    Const                b__frozen_param38             
42    Const                aten_convolution_default_17_output_zp
43    Const                aten_convolution_default_17_input_zp
44    Const                aten_convolution_default_17_shifts
45    Const                aten_convolution_default_17_multipliers
46    Const                layer-18_weight_zp            
47    Const                layer-18_input_zp             
48    Const                b__frozen_param37             
49    Const                b__frozen_param36             
50    Const                aten_convolution_default_16_output_zp
51    Const                aten_convolution_default_16_input_zp
52    Const                aten_convolution_default_16_shifts
53    Const                aten_convolution_default_16_multipliers
54    Const                layer-17_weight_zp            
55    Const                layer-17_input_zp             
56    Const                b__frozen_param33             
57    Const                b__frozen_param32             
58    Const                aten_convolution_default_14_output_zp
59    Const                aten_convolution_default_14_input_zp
60    Const                aten_convolution_default_14_shifts
61    Const                aten_convolution_default_14_multipliers
62    Const                layer-15_weight_zp            
63    Const                layer-15_input_zp             
64    Const                b__frozen_param31             
65    Const                b__frozen_param30             
66    Const                aten_convolution_default_13_output_zp
67    Const                aten_convolution_default_13_input_zp
68    Const                aten_convolution_default_13_shifts
69    Const                aten_convolution_default_13_multipliers
70    Const                layer-14_weight_zp            
71    Const                layer-14_input_zp             
72    Const                b__frozen_param27             
73    Const                b__frozen_param26             
74    Const                aten_convolution_default_11_output_zp
75    Const                aten_convolution_default_11_input_zp
76    Const                aten_convolution_default_11_shifts
77    Const                aten_convolution_default_11_multipliers
78    Const                layer-12_weight_zp            
79    Const                layer-12_input_zp             
80    Const                b__frozen_param25             
81    Const                b__frozen_param24             
82    Const                aten_convolution_default_10_output_zp
83    Const                aten_convolution_default_10_input_zp
84    Const                aten_convolution_default_10_shifts
85    Const                aten_convolution_default_10_multipliers
86    Const                layer-11_weight_zp            
87    Const                layer-11_input_zp             
88    Const                b__frozen_param21             
89    Const                b__frozen_param20             
90    Const                aten_convolution_default_8_output_zp
91    Const                aten_convolution_default_8_input_zp
92    Const                aten_convolution_default_8_shifts
93    Const                aten_convolution_default_8_multipliers
94    Const                layer-9_weight_zp             
95    Const                layer-9_input_zp              
96    Const                b__frozen_param19             
97    Const                b__frozen_param18             
98    Const                aten_convolution_default_7_output_zp
99    Const                aten_convolution_default_7_input_zp
100   Const                aten_convolution_default_7_shifts
101   Const                aten_convolution_default_7_multipliers
102   Const                layer-8_weight_zp             
103   Const                layer-8_input_zp              
104   Const                b__frozen_param15             
105   Const                b__frozen_param14             
106   Const                aten_convolution_default_5_output_zp
107   Const                aten_convolution_default_5_input_zp
108   Const                aten_convolution_default_5_shifts
109   Const                aten_convolution_default_5_multipliers
110   Const                layer-6_weight_zp             
111   Const                layer-6_input_zp              
112   Const                b__frozen_param13             
113   Const                b__frozen_param12             
114   Const                aten_convolution_default_4_output_zp
115   Const                aten_convolution_default_4_input_zp
116   Const                aten_convolution_default_4_shifts
117   Const                aten_convolution_default_4_multipliers
118   Const                layer-5_weight_zp             
119   Const                layer-5_input_zp              
120   Const                b__frozen_param9              
121   Const                b__frozen_param8              
122   Const                aten_convolution_default_2_output_zp
123   Const                aten_convolution_default_2_input_zp
124   Const                aten_convolution_default_2_shifts
125   Const                aten_convolution_default_2_multipliers
126   Const                layer-3_weight_zp             
127   Const                layer-3_input_zp              
128   Const                b__frozen_param7              
129   Const                b__frozen_param6              
130   Const                aten_convolution_default_1_output_zp
131   Const                aten_convolution_default_1_input_zp
132   Const                aten_convolution_default_1_shifts
133   Const                aten_convolution_default_1_multipliers
134   Const                layer-2_weight_zp             
135   Const                layer-2_input_zp              
136   Const                b__frozen_param3              
137   Const                b__frozen_param2              
138   Const                aten_convolution_default_output_zp
139   Const                aten_convolution_default_input_zp
140   Const                aten_convolution_default_shifts
141   Const                aten_convolution_default_multipliers
142   Const                layer-1_weight_zp             
143   Const                layer-1_input_zp              
144   Const                b__frozen_param1              
145   Const                b__frozen_param0              
146   Transpose            tosa_transpose_default        
147   Slice                aten_slice_copy_tensor        
148   Slice                aten_slice_copy_tensor_1      
149   Conv2D               layer-1                       
150   Rescale              aten_convolution_default      
151   Clamp                aten_clamp_default            
152   MaxPool              aten_max_pool2d_default       
153   Conv2D               layer-2                       
154   Rescale              aten_convolution_default_1    
155   Clamp                aten_clamp_default_1          
156   Conv2D               layer-3                       
157   Rescale              aten_convolution_default_2    
158   Clamp                aten_clamp_default_2          
159   Const                aten_convolution_default_3_output_zp
160   Const                aten_convolution_default_3_input_zp
161   Const                aten_convolution_default_3_shifts
162   Const                aten_convolution_default_3_multipliers
163   Const                layer-4_weight_zp             
164   Const                layer-4_input_zp              
165   Const                b__frozen_param5              
166   Const                b__frozen_param4              
167   Conv2D               layer-4                       
168   Rescale              aten_convolution_default_3    
169   Clamp                aten_clamp_default_3          
170   Concat               aten_cat_default              
171   Conv2D               layer-5                       
172   Rescale              aten_convolution_default_4    
173   Clamp                aten_clamp_default_4          
174   Conv2D               layer-6                       
175   Rescale              aten_convolution_default_5    
176   Clamp                aten_clamp_default_5          
177   Const                aten_convolution_default_6_output_zp
178   Const                aten_convolution_default_6_input_zp
179   Const                aten_convolution_default_6_shifts
180   Const                aten_convolution_default_6_multipliers
181   Const                layer-7_weight_zp             
182   Const                layer-7_input_zp              
183   Const                b__frozen_param11             
184   Const                b__frozen_param10             
185   Conv2D               layer-7                       
186   Rescale              aten_convolution_default_6    
187   Clamp                aten_clamp_default_6          
188   Concat               aten_cat_default_1            
189   MaxPool              aten_max_pool2d_default_1     
190   Conv2D               layer-8                       
191   Rescale              aten_convolution_default_7    
192   Clamp                aten_clamp_default_7          
193   Conv2D               layer-9                       
194   Rescale              aten_convolution_default_8    
195   Clamp                aten_clamp_default_8          
196   Const                aten_convolution_default_9_output_zp
197   Const                aten_convolution_default_9_input_zp
198   Const                aten_convolution_default_9_shifts
199   Const                aten_convolution_default_9_multipliers
200   Const                layer-10_weight_zp            
201   Const                layer-10_input_zp             
202   Const                b__frozen_param17             
203   Const                b__frozen_param16             
204   Conv2D               layer-10                      
205   Rescale              aten_convolution_default_9    
206   Clamp                aten_clamp_default_9          
207   Concat               aten_cat_default_2            
208   Conv2D               layer-11                      
209   Rescale              aten_convolution_default_10   
210   Clamp                aten_clamp_default_10         
211   Conv2D               layer-12                      
212   Rescale              aten_convolution_default_11   
213   Clamp                aten_clamp_default_11         
214   Const                aten_convolution_default_12_output_zp
215   Const                aten_convolution_default_12_input_zp
216   Const                aten_convolution_default_12_shifts
217   Const                aten_convolution_default_12_multipliers
218   Const                layer-13_weight_zp            
219   Const                layer-13_input_zp             
220   Const                b__frozen_param23             
221   Const                b__frozen_param22             
222   Conv2D               layer-13                      
223   Rescale              aten_convolution_default_12   
224   Clamp                aten_clamp_default_12         
225   Concat               aten_cat_default_3            
226   MaxPool              aten_max_pool2d_default_2     
227   Conv2D               layer-14                      
228   Rescale              aten_convolution_default_13   
229   Clamp                aten_clamp_default_13         
230   Conv2D               layer-15                      
231   Rescale              aten_convolution_default_14   
232   Clamp                aten_clamp_default_14         
233   Const                aten_convolution_default_15_output_zp
234   Const                aten_convolution_default_15_input_zp
235   Const                aten_convolution_default_15_shifts
236   Const                aten_convolution_default_15_multipliers
237   Const                layer-16_weight_zp            
238   Const                layer-16_input_zp             
239   Const                b__frozen_param29             
240   Const                b__frozen_param28             
241   Conv2D               layer-16                      
242   Rescale              aten_convolution_default_15   
243   Clamp                aten_clamp_default_15         
244   Concat               aten_cat_default_4            
245   Conv2D               layer-17                      
246   Rescale              aten_convolution_default_16   
247   Clamp                aten_clamp_default_16         
248   Conv2D               layer-18                      
249   Rescale              aten_convolution_default_17   
250   Clamp                aten_clamp_default_17         
251   Const                aten_convolution_default_18_output_zp
252   Const                aten_convolution_default_18_input_zp
253   Const                aten_convolution_default_18_shifts
254   Const                aten_convolution_default_18_multipliers
255   Const                layer-19_weight_zp            
256   Const                layer-19_input_zp             
257   Const                b__frozen_param35             
258   Const                b__frozen_param34             
259   Conv2D               layer-19                      
260   Rescale              aten_convolution_default_18   
261   Clamp                aten_clamp_default_18         
262   Concat               aten_cat_default_5            
263   Conv2D               layer-20                      
264   Rescale              aten_convolution_default_19   
265   Clamp                aten_clamp_default_19         
266   Conv2D               layer-21                      
267   Rescale              aten_convolution_default_20   
268   Clamp                aten_clamp_default_20         
269   Const                aten_convolution_default_21_output_zp
270   Const                aten_convolution_default_21_input_zp
271   Const                aten_convolution_default_21_shifts
272   Const                aten_convolution_default_21_multipliers
273   Const                layer-22_weight_zp            
274   Const                layer-22_input_zp             
275   Const                b__frozen_param41             
276   Const                b__frozen_param40             
277   Conv2D               layer-22                      
278   Rescale              aten_convolution_default_21   
279   Clamp                aten_clamp_default_21         
280   Concat               aten_cat_default_6            
281   Conv2D               layer-23                      
282   Rescale              aten_convolution_default_22   
283   Clamp                aten_clamp_default_22         
284   Conv2D               layer-24                      
285   Rescale              aten_convolution_default_23   
286   Clamp                aten_clamp_default_23         
287   Const                aten_convolution_default_24_output_zp
288   Const                aten_convolution_default_24_input_zp
289   Const                aten_convolution_default_24_shifts
290   Const                aten_convolution_default_24_multipliers
291   Const                layer-25_weight_zp            
292   Const                layer-25_input_zp             
293   Const                b__frozen_param47             
294   Const                b__frozen_param46             
295   Conv2D               layer-25                      
296   Rescale              aten_convolution_default_24   
297   Clamp                aten_clamp_default_24         
298   Concat               aten_cat_default_7            
299   Conv2D               layer-26                      
300   Rescale              aten_convolution_default_25   
301   Clamp                aten_clamp_default_25         
302   AvgPool              aten_avg_pool2d_default       
303   Reshape              aten_view_copy_default        


[ After Graph Optimization ]
0     Transpose            tosa_transpose_default        
1     MemoryCopy           aten_slice_copy_tensor        
2     Conv2D               aten_convolution_default      
3     Clamp                aten_clamp_default            
4     MaxPool              aten_max_pool2d_default       
5     Conv2D               aten_convolution_default_1    
6     Clamp                aten_clamp_default_1          
7     Conv2D               aten_convolution_default_2    
8     Clamp                aten_clamp_default_2          
9     MemoryCopy           aten_cat_default              
10    Conv2D               aten_convolution_default_3    
11    Clamp                aten_clamp_default_3          
12    MemoryCopy           aten_cat_default              
13    Conv2D               aten_convolution_default_4    
14    Clamp                aten_clamp_default_4          
15    Conv2D               aten_convolution_default_5    
16    Clamp                aten_clamp_default_5          
17    MemoryCopy           aten_cat_default_1            
18    Conv2D               aten_convolution_default_6    
19    Clamp                aten_clamp_default_6          
20    MemoryCopy           aten_cat_default_1            
21    MaxPool              aten_max_pool2d_default_1     
22    Conv2D               aten_convolution_default_7    
23    Clamp                aten_clamp_default_7          
24    Conv2D               aten_convolution_default_8    
25    Clamp                aten_clamp_default_8          
26    MemoryCopy           aten_cat_default_2            
27    Conv2D               aten_convolution_default_9    
28    Clamp                aten_clamp_default_9          
29    MemoryCopy           aten_cat_default_2            
30    Conv2D               aten_convolution_default_10   
31    Clamp                aten_clamp_default_10         
32    Conv2D               aten_convolution_default_11   
33    Clamp                aten_clamp_default_11         
34    MemoryCopy           aten_cat_default_3            
35    Conv2D               aten_convolution_default_12   
36    Clamp                aten_clamp_default_12         
37    MemoryCopy           aten_cat_default_3            
38    MaxPool              aten_max_pool2d_default_2     
39    Conv2D               aten_convolution_default_13   
40    Clamp                aten_clamp_default_13         
41    Conv2D               aten_convolution_default_14   
42    Clamp                aten_clamp_default_14         
43    MemoryCopy           aten_cat_default_4            
44    Conv2D               aten_convolution_default_15   
45    Clamp                aten_clamp_default_15         
46    MemoryCopy           aten_cat_default_4            
47    Conv2D               aten_convolution_default_16   
48    Clamp                aten_clamp_default_16         
49    Conv2D               aten_convolution_default_17   
50    Clamp                aten_clamp_default_17         
51    MemoryCopy           aten_cat_default_5            
52    Conv2D               aten_convolution_default_18   
53    Clamp                aten_clamp_default_18         
54    MemoryCopy           aten_cat_default_5            
55    Conv2D               aten_convolution_default_19   
56    Clamp                aten_clamp_default_19         
57    Conv2D               aten_convolution_default_20   
58    Clamp                aten_clamp_default_20         
59    MemoryCopy           aten_cat_default_6            
60    Conv2D               aten_convolution_default_21   
61    Clamp                aten_clamp_default_21         
62    MemoryCopy           aten_cat_default_6            
63    Conv2D               aten_convolution_default_22   
64    Clamp                aten_clamp_default_22         
65    Conv2D               aten_convolution_default_23   
66    Clamp                aten_clamp_default_23         
67    MemoryCopy           aten_cat_default_7            
68    Conv2D               aten_convolution_default_24   
69    Clamp                aten_clamp_default_24         
70    MemoryCopy           aten_cat_default_7            
71    Conv2D               aten_convolution_default_25   
72    Clamp                aten_clamp_default_25         
73    AvgPool              aten_view_copy_default        


[ Graph With Tensor Quantization ]
0 Transpose tosa_transpose_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 quantized_decomposed_quantize_per_tensor_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
1 MemoryCopy aten_slice_copy_tensor
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor
2 Conv2D aten_convolution_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [8], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1967957596, shift:40), (scale:1104017706, shift:39), (scale:2033804415, shift:39), (scale:1495489376, shift:39), (scale:1315077251, shift:39), (scale:1485326064, shift:40), (scale:1600875919, shift:39), (scale:1344442802, shift:40), (scale:2002292052, shift:39), (scale:1253913150, shift:39), (scale:1838407375, shift:39), (scale:1472567789, shift:41), (scale:1477390478, shift:40), (scale:1103837142, shift:39), (scale:1516173055, shift:39), (scale:1823332314, shift:39), (scale:2083672538, shift:41), (scale:1725834383, shift:39), (scale:1428328512, shift:39), (scale:1320245320, shift:39), (scale:1539575920, shift:39), (scale:1125310754, shift:39), (scale:1705986186, shift:39), (scale:1290330066, shift:39), (scale:1267178090, shift:40), (scale:1617933519, shift:39), (scale:1128436753, shift:40), (scale:2138705879, shift:39), (scale:1639986770, shift:39), (scale:1802798652, shift:39), (scale:2093912963, shift:41), (scale:1630291431, shift:39), (scale:1715718180, shift:39), (scale:1649500866, shift:39), (scale:2090667290, shift:40), (scale:1211903228, shift:39), (scale:1200691830, shift:39), (scale:1252000529, shift:40), (scale:2071789659, shift:39), (scale:2025948793, shift:41), (scale:1427153217, shift:39), (scale:1705932967, shift:39), (scale:1513680864, shift:40), (scale:1615136134, shift:40), (scale:1321495827, shift:40), (scale:1699293369, shift:40), (scale:1347203396, shift:40), (scale:1416029521, shift:39), (scale:1636622169, shift:40), (scale:1396427326, shift:39), (scale:1176682243, shift:39), (scale:2119789827, shift:40), (scale:1470282228, shift:41), (scale:1285990691, shift:39), (scale:1997262189, shift:40), (scale:1239466126, shift:39), (scale:1626673226, shift:40), (scale:1711679791, shift:40), (scale:1182439928, shift:39), (scale:1630792258, shift:40), (scale:1417638849, shift:39), (scale:1396027234, shift:39), (scale:1876666588, shift:41), (scale:1399712641, shift:40)], zero_point: [-11], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default
3 Clamp aten_clamp_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-11], quantMax: [127], dimension: 0 aten_clamp_default
4 MaxPool aten_max_pool2d_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default
5 Conv2D aten_convolution_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-11], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1286195933, shift:38), (scale:1450134712, shift:39), (scale:1760293141, shift:39), (scale:1087965358, shift:39), (scale:1573286282, shift:39), (scale:2021005998, shift:39), (scale:2141216756, shift:40), (scale:2048841812, shift:39), (scale:1963711259, shift:39), (scale:1595183834, shift:39), (scale:1175495960, shift:38), (scale:1599964473, shift:39), (scale:1322207419, shift:39), (scale:1322781673, shift:39), (scale:1483349384, shift:39), (scale:1131825497, shift:38)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1
6 Clamp aten_clamp_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-22], quantMax: [127], dimension: 0 aten_clamp_default_1
7 Conv2D aten_convolution_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:2058349019, shift:40), (scale:1556989282, shift:39), (scale:1556718563, shift:40), (scale:1627110281, shift:40), (scale:1098080186, shift:39), (scale:2140082798, shift:41), (scale:1208676157, shift:39), (scale:1169214759, shift:39), (scale:1267060094, shift:40), (scale:1092292371, shift:39), (scale:1249764620, shift:39), (scale:1967387454, shift:40), (scale:1697190630, shift:39), (scale:1421076296, shift:40), (scale:1443156840, shift:40), (scale:1248925890, shift:39), (scale:1532487924, shift:39), (scale:2104978234, shift:40), (scale:2028828105, shift:40), (scale:1788798423, shift:39), (scale:1464351423, shift:38), (scale:1519258489, shift:39), (scale:1669522937, shift:39), (scale:1198325595, shift:39), (scale:1102828105, shift:39), (scale:1074547503, shift:39), (scale:1817925169, shift:39), (scale:1955164430, shift:39), (scale:1209196688, shift:39), (scale:1574154394, shift:40), (scale:1158459700, shift:39), (scale:2011320637, shift:40), (scale:1115869952, shift:39), (scale:1780785627, shift:40), (scale:1758044528, shift:40), (scale:1711765665, shift:39), (scale:1209915968, shift:39), (scale:1757398300, shift:39), (scale:1901401216, shift:40), (scale:1528411305, shift:40), (scale:1304713919, shift:39), (scale:1095640962, shift:39), (scale:1904395456, shift:40), (scale:1211189516, shift:40), (scale:1437437580, shift:40), (scale:1594881340, shift:40), (scale:1696982385, shift:40), (scale:1487409722, shift:39), (scale:1388090507, shift:40), (scale:2070376958, shift:41), (scale:1164977291, shift:39), (scale:1383258624, shift:40), (scale:1602599427, shift:39), (scale:1395923878, shift:39), (scale:1743796356, shift:40), (scale:2123392487, shift:40), (scale:1076546662, shift:39), (scale:1604429323, shift:40), (scale:2080923688, shift:40), (scale:1660656501, shift:40), (scale:1477773610, shift:39), (scale:1181557316, shift:39), (scale:1086061576, shift:40), (scale:1819332743, shift:40)], zero_point: [43], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2
8 Clamp aten_clamp_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [43], quantMax: [127], dimension: 0 aten_clamp_default_2
9 MemoryCopy aten_cat_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
10 Conv2D aten_convolution_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1141291090, shift:39), (scale:2127262525, shift:40), (scale:1700039932, shift:40), (scale:1603263314, shift:40), (scale:2052339214, shift:40), (scale:1079452273, shift:39), (scale:1887166205, shift:39), (scale:1377799754, shift:39), (scale:1168275904, shift:39), (scale:1742950212, shift:39), (scale:1834822560, shift:40), (scale:1802672419, shift:39), (scale:1662253996, shift:39), (scale:1284182473, shift:39), (scale:1580615675, shift:39), (scale:1380294537, shift:38), (scale:1329915982, shift:39), (scale:1203133905, shift:39), (scale:1129193846, shift:39), (scale:1942191888, shift:39), (scale:1452616027, shift:39), (scale:1265759724, shift:39), (scale:1843591371, shift:40), (scale:1303225961, shift:39), (scale:1988994530, shift:40), (scale:1992740954, shift:40), (scale:1079641777, shift:39), (scale:1123551637, shift:39), (scale:1997474296, shift:40), (scale:1145685824, shift:39), (scale:1733861536, shift:39), (scale:2110247184, shift:40), (scale:2086040202, shift:39), (scale:1259215810, shift:39), (scale:1611592310, shift:39), (scale:1553768637, shift:39), (scale:1235629983, shift:39), (scale:1587942426, shift:38), (scale:1207719143, shift:38), (scale:2097269644, shift:39), (scale:1397513376, shift:43), (scale:1693439375, shift:39), (scale:1261784563, shift:39), (scale:1620386943, shift:39), (scale:2041602564, shift:40), (scale:2125820299, shift:39), (scale:1691933176, shift:39), (scale:1120693589, shift:39), (scale:1972040830, shift:39), (scale:1546731098, shift:39), (scale:1552967141, shift:39), (scale:1821680755, shift:40), (scale:1782614025, shift:39), (scale:1198333341, shift:38), (scale:1671270036, shift:38), (scale:1365608299, shift:39), (scale:1338753514, shift:38), (scale:1608451465, shift:39), (scale:1477457909, shift:38), (scale:1920715590, shift:39), (scale:1462626149, shift:39), (scale:1414040590, shift:39), (scale:1225297830, shift:39), (scale:1230249497, shift:38)], zero_point: [43], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3
11 Clamp aten_clamp_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [43], quantMax: [127], dimension: 0 aten_clamp_default_3
12 MemoryCopy aten_cat_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
13 Conv2D aten_convolution_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [43], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1740551293, shift:38), (scale:1590106950, shift:38), (scale:1270913251, shift:38), (scale:1719670105, shift:38), (scale:1898428927, shift:38), (scale:1910875423, shift:38), (scale:1462223080, shift:38), (scale:1596575745, shift:38), (scale:1395827101, shift:38), (scale:1614416433, shift:38), (scale:1322491952, shift:37), (scale:1936470800, shift:38), (scale:1838503862, shift:38), (scale:1134932346, shift:37), (scale:1604397891, shift:38), (scale:1953995232, shift:38)], zero_point: [-34], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4
14 Clamp aten_clamp_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-34], quantMax: [127], dimension: 0 aten_clamp_default_4
15 Conv2D aten_convolution_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-34], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1661255519, shift:39), (scale:1181310885, shift:39), (scale:1208827217, shift:39), (scale:1347650529, shift:39), (scale:1527247189, shift:39), (scale:1609562688, shift:39), (scale:1327005652, shift:39), (scale:1243249624, shift:39), (scale:1193978972, shift:39), (scale:1621520630, shift:39), (scale:1989585278, shift:40), (scale:1882992916, shift:40), (scale:1399085106, shift:39), (scale:1480522400, shift:39), (scale:1774277409, shift:40), (scale:1375824514, shift:39), (scale:1795505360, shift:40), (scale:1626400430, shift:39), (scale:1256233697, shift:39), (scale:1086180194, shift:39), (scale:1627600580, shift:39), (scale:1408136514, shift:39), (scale:1540905043, shift:39), (scale:1355050818, shift:39), (scale:1497643153, shift:39), (scale:1245232444, shift:39), (scale:1236752605, shift:39), (scale:1900512041, shift:40), (scale:1599453407, shift:39), (scale:1492630219, shift:39), (scale:1084842700, shift:39), (scale:1088070150, shift:39), (scale:1149107877, shift:38), (scale:1975900305, shift:40), (scale:1226395548, shift:39), (scale:1339420520, shift:40), (scale:1515292329, shift:39), (scale:1520313584, shift:39), (scale:1293976623, shift:39), (scale:1227405757, shift:38), (scale:1080387877, shift:39), (scale:1185157672, shift:39), (scale:1262513552, shift:39), (scale:1163451223, shift:39), (scale:1277488253, shift:39), (scale:1302553847, shift:39), (scale:1211517513, shift:39), (scale:2018983040, shift:40), (scale:1555831051, shift:39), (scale:1437521743, shift:39), (scale:1872313504, shift:40), (scale:1117274992, shift:39), (scale:1270219042, shift:39), (scale:1155175295, shift:39), (scale:1962821621, shift:40), (scale:1364426943, shift:39), (scale:1263600498, shift:39), (scale:1238835378, shift:39), (scale:1399532376, shift:39), (scale:1600025078, shift:39), (scale:1913916161, shift:39), (scale:2015122591, shift:40), (scale:1272466793, shift:39), (scale:2139475690, shift:40)], zero_point: [14], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5
16 Clamp aten_clamp_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [14], quantMax: [127], dimension: 0 aten_clamp_default_5
17 MemoryCopy aten_cat_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_5
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
18 Conv2D aten_convolution_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-34], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1086977246, shift:38), (scale:1901941783, shift:39), (scale:2105996618, shift:39), (scale:1898577296, shift:39), (scale:1476016933, shift:38), (scale:1180704287, shift:39), (scale:1884522919, shift:39), (scale:1937927579, shift:38), (scale:1129203657, shift:38), (scale:1722422271, shift:40), (scale:1803773994, shift:39), (scale:1532436914, shift:39), (scale:1255709485, shift:38), (scale:1425584347, shift:39), (scale:1266958821, shift:38), (scale:1284733734, shift:39), (scale:1847555472, shift:38), (scale:1723458983, shift:43), (scale:2012468969, shift:39), (scale:1446940088, shift:39), (scale:1264731718, shift:39), (scale:1448557717, shift:39), (scale:1334879510, shift:38), (scale:2078014833, shift:39), (scale:1522991655, shift:39), (scale:1781382362, shift:39), (scale:2132368683, shift:39), (scale:1427509229, shift:38), (scale:1345475713, shift:39), (scale:1137561149, shift:38), (scale:1287792406, shift:38), (scale:1702582357, shift:39), (scale:1285560372, shift:39), (scale:1443501535, shift:38), (scale:1526360970, shift:39), (scale:1106362289, shift:38), (scale:2060338332, shift:39), (scale:1353070566, shift:39), (scale:1843205225, shift:38), (scale:1925499358, shift:38), (scale:1297919870, shift:39), (scale:1333340672, shift:39), (scale:1256095427, shift:38), (scale:1517326821, shift:38), (scale:1374514550, shift:38), (scale:1584164570, shift:40), (scale:1126986210, shift:39), (scale:1438773769, shift:39), (scale:1131110459, shift:39), (scale:1491424625, shift:39), (scale:1451985483, shift:39), (scale:1660282600, shift:39), (scale:1926347670, shift:39), (scale:1920079320, shift:39), (scale:2097703946, shift:40), (scale:1241111687, shift:38), (scale:2096049438, shift:39), (scale:1422454897, shift:39), (scale:1776113331, shift:39), (scale:1478625459, shift:39), (scale:1142500531, shift:39), (scale:1703284079, shift:40), (scale:1403897621, shift:39), (scale:1226941126, shift:39)], zero_point: [14], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6
19 Clamp aten_clamp_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [14], quantMax: [127], dimension: 0 aten_clamp_default_6
20 MemoryCopy aten_cat_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_6
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
21 MaxPool aten_max_pool2d_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_1
22 Conv2D aten_convolution_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [14], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1385790692, shift:39), (scale:1195365074, shift:39), (scale:1176068131, shift:39), (scale:1374107157, shift:39), (scale:1263685792, shift:39), (scale:1602822919, shift:39), (scale:1413574842, shift:39), (scale:1606504350, shift:40), (scale:1680277913, shift:40), (scale:1884435341, shift:40), (scale:1569144488, shift:40), (scale:1740098266, shift:40), (scale:1219291423, shift:39), (scale:1094553560, shift:39), (scale:1287285601, shift:39), (scale:1136832053, shift:39), (scale:1337527067, shift:39), (scale:1472954894, shift:39), (scale:1387911785, shift:39), (scale:1713572980, shift:39), (scale:1634722904, shift:39), (scale:1147609123, shift:39), (scale:1307318309, shift:39), (scale:1076286709, shift:38), (scale:1752171970, shift:40), (scale:1487616079, shift:39), (scale:1259277217, shift:38), (scale:1407503170, shift:39), (scale:1217932976, shift:39), (scale:1292284988, shift:39), (scale:1267896333, shift:39), (scale:1772035907, shift:39)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7
23 Clamp aten_clamp_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-37], quantMax: [127], dimension: 0 aten_clamp_default_7
24 Conv2D aten_convolution_default_8
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1160227793, shift:39), (scale:1417120990, shift:40), (scale:1086570362, shift:39), (scale:1352451065, shift:40), (scale:1988450119, shift:39), (scale:1746889434, shift:40), (scale:1672129151, shift:40), (scale:2091885789, shift:40), (scale:1503094014, shift:39), (scale:1590564721, shift:40), (scale:1875694398, shift:40), (scale:1464236160, shift:40), (scale:1835698062, shift:41), (scale:1452963332, shift:40), (scale:1197027653, shift:40), (scale:1840230348, shift:40), (scale:1233393093, shift:39), (scale:1875240272, shift:40), (scale:1261122223, shift:40), (scale:1222079094, shift:39), (scale:1313610366, shift:40), (scale:2103506733, shift:40), (scale:2067212024, shift:41), (scale:1380294474, shift:40), (scale:1529997060, shift:39), (scale:1991039393, shift:41), (scale:1779226860, shift:41), (scale:1776001880, shift:40), (scale:1194007566, shift:40), (scale:1092317404, shift:39), (scale:1113672847, shift:40), (scale:1182351521, shift:40), (scale:1571004051, shift:40), (scale:1187825577, shift:39), (scale:1172972175, shift:40), (scale:2069283831, shift:41), (scale:2087112452, shift:40), (scale:2094875349, shift:40), (scale:1728021479, shift:41), (scale:1190042102, shift:40), (scale:1155813745, shift:40), (scale:1141223476, shift:39), (scale:1453245731, shift:39), (scale:1458427359, shift:40), (scale:2113176571, shift:41), (scale:1439120321, shift:40), (scale:1095826015, shift:39), (scale:1579212452, shift:40), (scale:1277397386, shift:40), (scale:1917847173, shift:41), (scale:1827287143, shift:40), (scale:1289358176, shift:39), (scale:1077719129, shift:39), (scale:1376827211, shift:40), (scale:2067402753, shift:41), (scale:1411098499, shift:40), (scale:1915603640, shift:41), (scale:1102617670, shift:40), (scale:2057497670, shift:40), (scale:1713262298, shift:40), (scale:1854193181, shift:41), (scale:2117431912, shift:40), (scale:1988089598, shift:41), (scale:2087099960, shift:41), (scale:1284089981, shift:40), (scale:1382972955, shift:39), (scale:1625951353, shift:39), (scale:1191974996, shift:40), (scale:1497322339, shift:40), (scale:1631149784, shift:40), (scale:1529489974, shift:40), (scale:1921151331, shift:41), (scale:1159546868, shift:40), (scale:1328166764, shift:40), (scale:1741989238, shift:41), (scale:1475704027, shift:40), (scale:1717787018, shift:40), (scale:1595345799, shift:40), (scale:1808325230, shift:40), (scale:1169377788, shift:39), (scale:1446219624, shift:40), (scale:1587256340, shift:41), (scale:2074490531, shift:41), (scale:1818323742, shift:40), (scale:1386230486, shift:40), (scale:2042336797, shift:41), (scale:1484104829, shift:40), (scale:1304746640, shift:39), (scale:2101478386, shift:40), (scale:1594771500, shift:40), (scale:1329221406, shift:40), (scale:1167361406, shift:40), (scale:1355689681, shift:40), (scale:1204279240, shift:39), (scale:1097387656, shift:40), (scale:1165928472, shift:40), (scale:2002565676, shift:40), (scale:1930192149, shift:41), (scale:1515805317, shift:40), (scale:1730509582, shift:40), (scale:1937651357, shift:40), (scale:1434849760, shift:40), (scale:1283348001, shift:40), (scale:1719159073, shift:41), (scale:1110634022, shift:40), (scale:1992964458, shift:39), (scale:1259498472, shift:40), (scale:1159604316, shift:40), (scale:1423811738, shift:40), (scale:1968395383, shift:41), (scale:1114035743, shift:40), (scale:1488574653, shift:40), (scale:1164502841, shift:40), (scale:1098266788, shift:39), (scale:1576308580, shift:40), (scale:1208943752, shift:40), (scale:1875349537, shift:41), (scale:1973630236, shift:41), (scale:1860059340, shift:41), (scale:1871459467, shift:40), (scale:2033443072, shift:40), (scale:1244005846, shift:40), (scale:1451580896, shift:39), (scale:1644938427, shift:40), (scale:1813106309, shift:41), (scale:1746233846, shift:40), (scale:1665461189, shift:40), (scale:1592882505, shift:40)], zero_point: [3], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8
25 Clamp aten_clamp_default_8
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [3], quantMax: [127], dimension: 0 aten_clamp_default_8
26 MemoryCopy aten_cat_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
27 Conv2D aten_convolution_default_9
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1566809061, shift:39), (scale:1615595839, shift:40), (scale:1908755153, shift:39), (scale:2004465052, shift:40), (scale:1227673324, shift:39), (scale:1455008922, shift:40), (scale:1164062087, shift:39), (scale:1519992390, shift:40), (scale:1361587424, shift:39), (scale:1453234559, shift:40), (scale:1285862937, shift:39), (scale:1187114828, shift:40), (scale:2046225635, shift:40), (scale:1887872222, shift:40), (scale:1290358801, shift:42), (scale:1223581088, shift:39), (scale:1659987573, shift:40), (scale:2108353089, shift:39), (scale:1650637083, shift:40), (scale:2133553647, shift:40), (scale:1276377494, shift:39), (scale:1963147510, shift:40), (scale:2114847212, shift:40), (scale:1265185163, shift:39), (scale:1605130181, shift:40), (scale:1319661185, shift:40), (scale:1580902448, shift:40), (scale:1420575409, shift:40), (scale:1368336587, shift:39), (scale:1382407365, shift:39), (scale:1151634766, shift:39), (scale:1682769704, shift:40), (scale:1900266640, shift:40), (scale:1812457758, shift:40), (scale:1621928793, shift:40), (scale:1475972262, shift:43), (scale:2059907828, shift:40), (scale:1507191001, shift:40), (scale:2096355261, shift:41), (scale:1773200762, shift:40), (scale:1852347997, shift:40), (scale:1135967774, shift:39), (scale:1863875511, shift:40), (scale:1450604816, shift:40), (scale:1140712959, shift:39), (scale:1890750758, shift:39), (scale:2129366750, shift:40), (scale:1159604932, shift:39), (scale:1398384469, shift:40), (scale:1709986468, shift:40), (scale:1373773782, shift:39), (scale:1313732739, shift:39), (scale:1966144812, shift:40), (scale:1670469594, shift:40), (scale:1966253900, shift:40), (scale:1475287378, shift:40), (scale:1865226628, shift:40), (scale:1172655202, shift:39), (scale:1741194650, shift:40), (scale:2038395526, shift:40), (scale:1421782687, shift:40), (scale:2119957141, shift:40), (scale:1932561838, shift:40), (scale:1589401166, shift:40), (scale:1907383626, shift:40), (scale:1475972262, shift:43), (scale:1353249436, shift:39), (scale:1136011673, shift:39), (scale:1765632290, shift:40), (scale:1776602924, shift:40), (scale:1795620261, shift:40), (scale:1288990089, shift:39), (scale:1515191781, shift:40), (scale:1937691121, shift:40), (scale:2070394249, shift:42), (scale:1810507885, shift:40), (scale:1609816775, shift:40), (scale:1173562574, shift:38), (scale:1555706289, shift:40), (scale:1781428518, shift:40), (scale:1840904587, shift:40), (scale:1412378709, shift:40), (scale:1902439794, shift:40), (scale:1285006238, shift:39), (scale:1962620893, shift:40), (scale:1392660476, shift:39), (scale:1948937292, shift:40), (scale:1518503153, shift:40), (scale:2096604757, shift:40), (scale:1420430690, shift:39), (scale:1312358220, shift:40), (scale:2050136819, shift:40), (scale:1276099229, shift:39), (scale:1091184465, shift:38), (scale:1968945753, shift:40), (scale:2048567524, shift:40), (scale:1756664139, shift:40), (scale:1350989100, shift:40), (scale:1108791126, shift:39), (scale:1366260118, shift:39), (scale:1217380800, shift:39), (scale:1163880155, shift:39), (scale:1299923246, shift:39), (scale:2095680318, shift:40), (scale:1494127798, shift:39), (scale:1953334273, shift:40), (scale:2015020621, shift:40), (scale:1397089040, shift:40), (scale:1312153415, shift:39), (scale:1912977240, shift:40), (scale:1836778217, shift:40), (scale:1546963177, shift:40), (scale:1133091965, shift:39), (scale:1273651155, shift:39), (scale:1760329873, shift:40), (scale:1611004962, shift:40), (scale:2139579217, shift:40), (scale:1617223901, shift:39), (scale:1622293009, shift:39), (scale:1466520777, shift:39), (scale:1859277069, shift:39), (scale:1675066982, shift:40), (scale:1577164047, shift:40), (scale:1230383915, shift:39), (scale:1145276915, shift:42), (scale:1699328673, shift:40), (scale:1475972262, shift:43), (scale:1351210796, shift:40)], zero_point: [3], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9
28 Clamp aten_clamp_default_9
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [3], quantMax: [127], dimension: 0 aten_clamp_default_9
29 MemoryCopy aten_cat_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_9
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
30 Conv2D aten_convolution_default_10
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [3], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1822336437, shift:39), (scale:2044980104, shift:39), (scale:1433679402, shift:38), (scale:1808461648, shift:39), (scale:1416023496, shift:39), (scale:2120968903, shift:39), (scale:1222150423, shift:38), (scale:1435882411, shift:38), (scale:1117599116, shift:38), (scale:1100631521, shift:38), (scale:1193165533, shift:38), (scale:1127314794, shift:38), (scale:1627475737, shift:39), (scale:1076295285, shift:38), (scale:1944599425, shift:39), (scale:1948708973, shift:38), (scale:1686722626, shift:39), (scale:2134829846, shift:39), (scale:1605532608, shift:38), (scale:1392292493, shift:38), (scale:1233637153, shift:38), (scale:1117387278, shift:38), (scale:1935786224, shift:39), (scale:1847529859, shift:39), (scale:2035735204, shift:39), (scale:1309095776, shift:38), (scale:1375073980, shift:38), (scale:1970060975, shift:39), (scale:1454861494, shift:38), (scale:2000473214, shift:39), (scale:1328810532, shift:38), (scale:1288089547, shift:38)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10
31 Clamp aten_clamp_default_10
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-22], quantMax: [127], dimension: 0 aten_clamp_default_10
32 Conv2D aten_convolution_default_11
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1257314728, shift:40), (scale:1527571043, shift:41), (scale:1957923524, shift:41), (scale:1082684686, shift:39), (scale:1847280244, shift:41), (scale:1103248845, shift:40), (scale:1534112287, shift:40), (scale:1697645244, shift:40), (scale:1224799058, shift:40), (scale:2020605853, shift:41), (scale:2011890246, shift:41), (scale:1392072386, shift:40), (scale:1479748697, shift:41), (scale:1949842509, shift:40), (scale:1651597875, shift:41), (scale:1829752959, shift:41), (scale:1960899576, shift:41), (scale:1859356713, shift:40), (scale:1787352541, shift:41), (scale:1115513611, shift:39), (scale:1811861138, shift:40), (scale:1974893338, shift:41), (scale:1154501355, shift:40), (scale:1342581204, shift:40), (scale:1509970612, shift:41), (scale:1962618501, shift:41), (scale:1286823075, shift:39), (scale:1951351008, shift:41), (scale:1122558527, shift:40), (scale:1428644200, shift:40), (scale:1663253093, shift:41), (scale:1361323742, shift:39), (scale:1118931585, shift:40), (scale:1928787399, shift:41), (scale:1775520687, shift:41), (scale:1096184648, shift:40), (scale:1865301925, shift:41), (scale:1512761811, shift:41), (scale:1186835354, shift:40), (scale:1765029572, shift:41), (scale:1118382329, shift:40), (scale:1856363038, shift:40), (scale:1294382529, shift:40), (scale:1992840548, shift:41), (scale:1690536723, shift:41), (scale:1899998849, shift:41), (scale:1379345954, shift:40), (scale:1338996267, shift:40), (scale:1331836465, shift:40), (scale:2015179684, shift:41), (scale:1177804239, shift:40), (scale:1154317298, shift:39), (scale:1872866812, shift:40), (scale:1248582557, shift:40), (scale:1583298165, shift:40), (scale:1769702552, shift:41), (scale:1339765093, shift:40), (scale:1875411277, shift:41), (scale:1863308041, shift:41), (scale:1123025892, shift:40), (scale:2062075252, shift:41), (scale:1135288670, shift:40), (scale:1728508112, shift:41), (scale:1360033621, shift:40), (scale:1187187699, shift:40), (scale:1438509336, shift:40), (scale:1441641484, shift:41), (scale:1442063132, shift:41), (scale:1711166606, shift:41), (scale:1443835989, shift:40), (scale:1209107704, shift:39), (scale:1400293862, shift:40), (scale:1840215053, shift:41), (scale:1687962443, shift:41), (scale:1546448344, shift:41), (scale:1169125337, shift:40), (scale:1927259819, shift:41), (scale:1868727849, shift:41), (scale:1151397696, shift:40), (scale:1198682448, shift:40), (scale:1617781350, shift:40), (scale:1237913746, shift:40), (scale:1143942396, shift:40), (scale:1169146141, shift:40), (scale:1308318914, shift:41), (scale:1583798524, shift:40), (scale:1356415159, shift:41), (scale:1499122249, shift:40), (scale:1799390450, shift:41), (scale:1157155804, shift:40), (scale:1395474326, shift:40), (scale:1463589979, shift:40), (scale:1914440364, shift:41), (scale:2122649340, shift:41), (scale:1759832911, shift:40), (scale:1427484734, shift:41), (scale:1460709998, shift:41), (scale:1462380292, shift:39), (scale:1153392641, shift:40), (scale:1310810905, shift:39), (scale:1102689585, shift:40), (scale:1111062918, shift:40), (scale:1980665359, shift:40), (scale:1309521711, shift:40), (scale:1839009474, shift:41), (scale:1133695895, shift:40), (scale:1885656452, shift:41), (scale:1105506491, shift:40), (scale:1431800068, shift:40), (scale:1318982557, shift:40), (scale:1686925151, shift:41), (scale:1157967563, shift:40), (scale:1734778240, shift:41), (scale:1438510926, shift:41), (scale:1308604606, shift:40), (scale:1966933835, shift:40), (scale:2100226723, shift:41), (scale:1169179269, shift:40), (scale:1581560026, shift:41), (scale:1833877875, shift:41), (scale:1796092531, shift:41), (scale:1519286624, shift:41), (scale:1238780364, shift:40), (scale:2033346862, shift:41), (scale:1196044165, shift:40), (scale:1803743682, shift:41), (scale:1828805774, shift:40), (scale:2131888761, shift:41)], zero_point: [37], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11
33 Clamp aten_clamp_default_11
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [37], quantMax: [127], dimension: 0 aten_clamp_default_11
34 MemoryCopy aten_cat_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_11
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
35 Conv2D aten_convolution_default_12
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-22], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:2119792018, shift:41), (scale:1301097229, shift:40), (scale:2000732869, shift:39), (scale:1596283258, shift:40), (scale:2100200618, shift:40), (scale:1188936704, shift:40), (scale:1225295442, shift:39), (scale:1188910997, shift:40), (scale:2062891914, shift:40), (scale:2140656710, shift:41), (scale:1661107485, shift:41), (scale:1878162326, shift:40), (scale:1165516549, shift:40), (scale:1764875462, shift:40), (scale:1284663155, shift:39), (scale:1183033233, shift:39), (scale:1173862982, shift:39), (scale:1487836735, shift:40), (scale:1696282639, shift:40), (scale:1367991136, shift:40), (scale:1831479569, shift:40), (scale:1752376683, shift:40), (scale:1222470717, shift:40), (scale:1975178633, shift:41), (scale:1602435716, shift:40), (scale:1630937911, shift:40), (scale:1536377817, shift:40), (scale:1244465591, shift:40), (scale:1834960486, shift:40), (scale:1554853744, shift:39), (scale:1907406447, shift:40), (scale:1638582040, shift:40), (scale:1589999084, shift:40), (scale:1500753453, shift:40), (scale:1266089170, shift:40), (scale:1655380385, shift:40), (scale:1948116694, shift:40), (scale:1426525094, shift:40), (scale:1825545223, shift:40), (scale:1233437677, shift:40), (scale:1149384598, shift:40), (scale:1696314044, shift:41), (scale:1620880636, shift:40), (scale:1139542255, shift:40), (scale:2096892761, shift:41), (scale:1381577296, shift:40), (scale:1621048792, shift:41), (scale:1521759401, shift:40), (scale:1473624065, shift:40), (scale:1327358540, shift:39), (scale:1147577421, shift:40), (scale:1104692082, shift:39), (scale:1613509612, shift:40), (scale:1417294153, shift:39), (scale:2068366184, shift:40), (scale:1188991961, shift:40), (scale:1468112295, shift:40), (scale:2059657865, shift:41), (scale:1256815429, shift:39), (scale:1094416827, shift:40), (scale:1124457137, shift:39), (scale:1861956037, shift:40), (scale:1819849528, shift:40), (scale:1535559035, shift:40), (scale:1870630169, shift:40), (scale:1129821158, shift:40), (scale:1971476424, shift:40), (scale:1825353083, shift:41), (scale:2145780888, shift:40), (scale:1604479954, shift:40), (scale:1807069164, shift:40), (scale:1257829531, shift:40), (scale:1410745489, shift:40), (scale:1079444674, shift:40), (scale:1338739992, shift:40), (scale:1638590123, shift:40), (scale:1183312565, shift:39), (scale:1292101229, shift:40), (scale:1150294150, shift:40), (scale:1464756733, shift:40), (scale:1266113817, shift:39), (scale:1212109065, shift:40), (scale:1528373923, shift:40), (scale:1510205023, shift:40), (scale:2026275841, shift:40), (scale:2115769134, shift:40), (scale:1749087112, shift:40), (scale:1512700194, shift:40), (scale:1252930356, shift:39), (scale:1994196527, shift:41), (scale:1482608934, shift:40), (scale:1921736653, shift:40), (scale:1351555207, shift:40), (scale:1460875106, shift:40), (scale:1594883948, shift:40), (scale:1125667089, shift:40), (scale:2090024481, shift:40), (scale:1788741118, shift:40), (scale:1197010034, shift:40), (scale:2082279380, shift:40), (scale:1659477739, shift:40), (scale:1519675410, shift:40), (scale:1326922714, shift:40), (scale:1417192650, shift:40), (scale:1265752726, shift:40), (scale:2144390854, shift:40), (scale:2120328023, shift:40), (scale:1092210065, shift:40), (scale:1736686920, shift:40), (scale:1162131305, shift:39), (scale:1372264067, shift:40), (scale:1757795829, shift:40), (scale:1519879078, shift:40), (scale:1174589139, shift:39), (scale:1719431546, shift:40), (scale:1329052421, shift:40), (scale:1308575587, shift:40), (scale:1248345363, shift:39), (scale:1487781876, shift:40), (scale:1811439357, shift:40), (scale:1651999249, shift:40), (scale:1720141669, shift:40), (scale:1544907115, shift:40), (scale:1240568592, shift:40), (scale:1425169910, shift:40), (scale:1132146318, shift:39), (scale:1448421645, shift:40), (scale:1344754640, shift:40)], zero_point: [37], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12
36 Clamp aten_clamp_default_12
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [37], quantMax: [127], dimension: 0 aten_clamp_default_12
37 MemoryCopy aten_cat_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_12
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
38 MaxPool aten_max_pool2d_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_2
39 Conv2D aten_convolution_default_13
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [37], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_2
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1529247424, shift:38), (scale:1465654176, shift:38), (scale:2062066981, shift:39), (scale:1211647161, shift:38), (scale:1264804467, shift:37), (scale:1115258275, shift:38), (scale:1580963366, shift:38), (scale:2038866069, shift:39), (scale:1916507862, shift:38), (scale:2001923955, shift:39), (scale:1438099573, shift:38), (scale:2038913080, shift:39), (scale:1811779577, shift:38), (scale:2090643783, shift:38), (scale:1863840157, shift:39), (scale:1109587487, shift:38), (scale:1956097810, shift:39), (scale:2028714838, shift:39), (scale:1086208961, shift:38), (scale:1945565359, shift:39), (scale:1169116247, shift:38), (scale:1873595087, shift:39), (scale:1119914005, shift:37), (scale:1109059701, shift:38), (scale:1886113854, shift:39), (scale:1515248386, shift:38), (scale:1885438057, shift:39), (scale:1765397166, shift:38), (scale:2036858895, shift:39), (scale:1927505444, shift:38), (scale:1220074967, shift:38), (scale:1385392397, shift:38), (scale:1838752244, shift:39), (scale:1170203185, shift:38), (scale:1272880736, shift:38), (scale:1485481078, shift:39), (scale:1846150389, shift:39), (scale:1721297575, shift:38), (scale:1410732530, shift:38), (scale:1684403595, shift:38), (scale:1854659300, shift:39), (scale:1564274619, shift:38), (scale:1589329801, shift:38), (scale:1571030020, shift:38), (scale:2064021850, shift:39), (scale:2066307398, shift:39), (scale:2098350947, shift:39), (scale:1168351964, shift:38)], zero_point: [-15], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13
40 Clamp aten_clamp_default_13
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-15], quantMax: [127], dimension: 0 aten_clamp_default_13
41 Conv2D aten_convolution_default_14
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-15], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1490543780, shift:40), (scale:1214274616, shift:41), (scale:1192662105, shift:41), (scale:1573842120, shift:40), (scale:1280377244, shift:41), (scale:1736747194, shift:41), (scale:1130399529, shift:41), (scale:1963128957, shift:41), (scale:1732023860, shift:40), (scale:1744686683, shift:41), (scale:1135762078, shift:40), (scale:2110450623, shift:41), (scale:1331504249, shift:41), (scale:1511959905, shift:41), (scale:1201828255, shift:41), (scale:1938152108, shift:41), (scale:1246306354, shift:39), (scale:1708404157, shift:40), (scale:1085213208, shift:40), (scale:1325411752, shift:40), (scale:1496123056, shift:40), (scale:1877923798, shift:41), (scale:1514177483, shift:40), (scale:1929711150, shift:41), (scale:1132399853, shift:40), (scale:1762469179, shift:41), (scale:1550839974, shift:41), (scale:1800153491, shift:41), (scale:1365149348, shift:41), (scale:1486385705, shift:41), (scale:1698650693, shift:41), (scale:2064308846, shift:42), (scale:1937244130, shift:41), (scale:1704854921, shift:41), (scale:1369510943, shift:41), (scale:1273822504, shift:40), (scale:1783420155, shift:41), (scale:1256731655, shift:40), (scale:2122245881, shift:41), (scale:1136936308, shift:40), (scale:1896242365, shift:40), (scale:1083062720, shift:39), (scale:1693048569, shift:41), (scale:1593650827, shift:41), (scale:1639596877, shift:41), (scale:1534906318, shift:40), (scale:1923233670, shift:41), (scale:1896709098, shift:41), (scale:1822858349, shift:41), (scale:2003457168, shift:40), (scale:1842363296, shift:41), (scale:1425936047, shift:41), (scale:1172126356, shift:40), (scale:1367825208, shift:41), (scale:2129212943, shift:41), (scale:1316420725, shift:41), (scale:1367059733, shift:41), (scale:1612110840, shift:41), (scale:1276981077, shift:40), (scale:1884206872, shift:40), (scale:1303906480, shift:40), (scale:1596516999, shift:41), (scale:1202416361, shift:41), (scale:1269318274, shift:41), (scale:1171041141, shift:40), (scale:1073997471, shift:40), (scale:1828478302, shift:41), (scale:1222975882, shift:40), (scale:1483403972, shift:41), (scale:1277656085, shift:40), (scale:1516660126, shift:41), (scale:1975619693, shift:41), (scale:1662340828, shift:41), (scale:2094722996, shift:41), (scale:1901408659, shift:40), (scale:1524403624, shift:40), (scale:1454485508, shift:41), (scale:1373366315, shift:41), (scale:1276303693, shift:40), (scale:1339676181, shift:41), (scale:1807309808, shift:40), (scale:1591152204, shift:41), (scale:1694162972, shift:41), (scale:1630523307, shift:41), (scale:1596800156, shift:41), (scale:1579690686, shift:41), (scale:1406367839, shift:41), (scale:1644897222, shift:41), (scale:1218666323, shift:41), (scale:1735671752, shift:40), (scale:1827126439, shift:41), (scale:1083472466, shift:39), (scale:1457327512, shift:41), (scale:1663194129, shift:41), (scale:1769028543, shift:41), (scale:1385046276, shift:41), (scale:1559185313, shift:41), (scale:1634949880, shift:41), (scale:1594762456, shift:41), (scale:1604343965, shift:39), (scale:1579351927, shift:41), (scale:1606171411, shift:41), (scale:1150323796, shift:40), (scale:1698642769, shift:41), (scale:2147421100, shift:41), (scale:1593206149, shift:41), (scale:1802962609, shift:41), (scale:1573164868, shift:41), (scale:1881785378, shift:40), (scale:1648555283, shift:41), (scale:1897389388, shift:41), (scale:1133469748, shift:41), (scale:1385221136, shift:41), (scale:1266672658, shift:40), (scale:1154052118, shift:40), (scale:1094767643, shift:40), (scale:1382875582, shift:41), (scale:1561843211, shift:41), (scale:1501059681, shift:41), (scale:1085388398, shift:40), (scale:1211537740, shift:40), (scale:1847963967, shift:41), (scale:1758842287, shift:41), (scale:1345017467, shift:40), (scale:1183613232, shift:40), (scale:1811918637, shift:40), (scale:1662346375, shift:41), (scale:1152814627, shift:40), (scale:1546491454, shift:41), (scale:2113346775, shift:41), (scale:2089350541, shift:41), (scale:1943862924, shift:41), (scale:1582571781, shift:41), (scale:1508012480, shift:40), (scale:1784440127, shift:40), (scale:1766995201, shift:41), (scale:1757830238, shift:41), (scale:1213093651, shift:41), (scale:1473289826, shift:41), (scale:1198604175, shift:41), (scale:1943384965, shift:40), (scale:1661548279, shift:40), (scale:1533614811, shift:40), (scale:1314358856, shift:41), (scale:2135533525, shift:41), (scale:1553314428, shift:40), (scale:1760165359, shift:41), (scale:2028708062, shift:41), (scale:1889814014, shift:41), (scale:1569394152, shift:41), (scale:1856111201, shift:41), (scale:1752070028, shift:41), (scale:1993713874, shift:41), (scale:1519678442, shift:41), (scale:1699597103, shift:41), (scale:1923584447, shift:41), (scale:1392146332, shift:41), (scale:1533534908, shift:41), (scale:1236142022, shift:40), (scale:1780511192, shift:41), (scale:1230951559, shift:41), (scale:1708534113, shift:40), (scale:1160996596, shift:41), (scale:1565975797, shift:41), (scale:1182529733, shift:41), (scale:1187028284, shift:41), (scale:1544039451, shift:41), (scale:1832516591, shift:41), (scale:1913889754, shift:41), (scale:1186739977, shift:40), (scale:1849698303, shift:41), (scale:1709159198, shift:40), (scale:1435222117, shift:40), (scale:1959300791, shift:41), (scale:1534618407, shift:41), (scale:1339534734, shift:41), (scale:1426961831, shift:40), (scale:2063351078, shift:40), (scale:1085781833, shift:41), (scale:1523092702, shift:41), (scale:1600138080, shift:41), (scale:1720002631, shift:41), (scale:1683487531, shift:40), (scale:1121390673, shift:39), (scale:2057976906, shift:41), (scale:1415221117, shift:41), (scale:2079468970, shift:41), (scale:1456555433, shift:40), (scale:1108949331, shift:40), (scale:2068830905, shift:42), (scale:1435764130, shift:40), (scale:1771659895, shift:41)], zero_point: [24], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14
42 Clamp aten_clamp_default_14
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [24], quantMax: [127], dimension: 0 aten_clamp_default_14
43 MemoryCopy aten_cat_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
44 Conv2D aten_convolution_default_15
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-15], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1128507237, shift:40), (scale:1108346830, shift:40), (scale:1277092940, shift:40), (scale:2134220622, shift:41), (scale:1637483105, shift:41), (scale:1902334598, shift:41), (scale:1107980337, shift:40), (scale:1701269895, shift:41), (scale:1529349891, shift:41), (scale:1560026595, shift:40), (scale:1959171891, shift:41), (scale:1155140899, shift:40), (scale:1440706302, shift:40), (scale:1835833648, shift:41), (scale:2048997898, shift:40), (scale:1382208235, shift:41), (scale:1281814160, shift:40), (scale:1134716881, shift:40), (scale:1085069516, shift:40), (scale:1158797771, shift:40), (scale:1179815309, shift:40), (scale:1910347650, shift:41), (scale:1083750736, shift:40), (scale:1388899008, shift:40), (scale:2139509345, shift:41), (scale:1515018370, shift:41), (scale:1547489371, shift:40), (scale:1129142492, shift:40), (scale:1335901107, shift:40), (scale:1121390541, shift:40), (scale:1139006894, shift:40), (scale:1112018758, shift:40), (scale:1477972350, shift:40), (scale:1927532004, shift:41), (scale:1186600511, shift:40), (scale:1951599819, shift:41), (scale:1086885802, shift:40), (scale:1381893646, shift:40), (scale:1665617076, shift:41), (scale:1906552369, shift:41), (scale:1330263192, shift:40), (scale:1237182465, shift:40), (scale:1497302965, shift:41), (scale:1684038657, shift:41), (scale:1944046501, shift:41), (scale:1683046419, shift:41), (scale:1207640105, shift:40), (scale:2134800143, shift:41), (scale:1987770351, shift:41), (scale:2133665666, shift:41), (scale:1084456714, shift:40), (scale:1854273190, shift:41), (scale:1379383137, shift:40), (scale:1621476944, shift:41), (scale:1150673780, shift:40), (scale:1986487163, shift:41), (scale:1242819587, shift:40), (scale:2145461585, shift:41), (scale:1175550918, shift:40), (scale:1158122367, shift:40), (scale:1563407178, shift:40), (scale:1921513465, shift:41), (scale:1122892171, shift:40), (scale:1859656343, shift:41), (scale:1790717654, shift:40), (scale:1821177633, shift:41), (scale:2086121574, shift:41), (scale:1818607165, shift:40), (scale:2011988329, shift:41), (scale:1112408759, shift:40), (scale:1716313798, shift:41), (scale:1159212733, shift:40), (scale:1440084123, shift:41), (scale:1081743148, shift:39), (scale:1766055262, shift:41), (scale:1115591237, shift:40), (scale:1473825368, shift:41), (scale:1196835501, shift:40), (scale:1923151787, shift:41), (scale:1580821200, shift:41), (scale:2021928143, shift:41), (scale:1893138468, shift:41), (scale:2039646850, shift:41), (scale:1776732156, shift:41), (scale:1094186471, shift:40), (scale:1422960653, shift:41), (scale:1744043108, shift:41), (scale:1209051664, shift:40), (scale:1240811735, shift:40), (scale:1092064115, shift:40), (scale:1090383662, shift:40), (scale:1697585024, shift:40), (scale:1598019818, shift:40), (scale:1646834153, shift:41), (scale:1758360365, shift:41), (scale:2039195304, shift:41), (scale:1622240834, shift:41), (scale:1082045257, shift:40), (scale:2145137882, shift:41), (scale:1927023008, shift:41), (scale:1102665332, shift:40), (scale:1481658541, shift:40), (scale:1827179399, shift:41), (scale:2114794917, shift:40), (scale:1102694718, shift:40), (scale:1597579498, shift:41), (scale:1798186712, shift:41), (scale:1751717798, shift:41), (scale:1811072336, shift:41), (scale:1669777793, shift:41), (scale:1515933876, shift:41), (scale:1112410212, shift:40), (scale:1556698708, shift:40), (scale:1918204200, shift:41), (scale:1082619891, shift:40), (scale:1184645619, shift:40), (scale:1160616104, shift:40), (scale:1208493274, shift:40), (scale:1442796830, shift:41), (scale:1082809807, shift:40), (scale:1288608343, shift:40), (scale:1962789935, shift:41), (scale:1220170859, shift:40), (scale:1314133281, shift:40), (scale:1805516833, shift:41), (scale:1086112337, shift:40), (scale:1168897917, shift:40), (scale:1404363817, shift:40), (scale:1441874853, shift:40), (scale:1954416994, shift:41), (scale:1347811133, shift:40), (scale:1973144051, shift:41), (scale:1125925015, shift:40), (scale:1173331622, shift:40), (scale:1076574477, shift:40), (scale:1076788297, shift:40), (scale:1935178827, shift:40), (scale:1091799249, shift:40), (scale:1653011704, shift:41), (scale:1343320506, shift:40), (scale:1120382190, shift:40), (scale:1808457492, shift:41), (scale:1853537431, shift:41), (scale:1114670581, shift:40), (scale:1125299269, shift:40), (scale:1688487285, shift:41), (scale:1638510474, shift:40), (scale:1993856773, shift:41), (scale:1130551277, shift:40), (scale:1781541466, shift:40), (scale:1833215371, shift:41), (scale:2087096776, shift:41), (scale:1210345945, shift:41), (scale:1233641550, shift:40), (scale:1328176230, shift:40), (scale:1896767605, shift:41), (scale:2121443691, shift:41), (scale:1743174884, shift:41), (scale:1098591979, shift:40), (scale:1741256178, shift:40), (scale:1737134025, shift:41), (scale:2001227571, shift:41), (scale:1390539707, shift:40), (scale:1571409400, shift:41), (scale:1609115371, shift:41), (scale:1279543358, shift:40), (scale:1528229809, shift:41), (scale:1545247623, shift:40), (scale:2058037130, shift:41), (scale:1432726532, shift:40), (scale:1373159098, shift:40), (scale:1889290359, shift:41), (scale:1825756878, shift:41), (scale:1864871503, shift:41), (scale:1428709242, shift:40), (scale:1151198360, shift:40), (scale:2023716628, shift:41), (scale:1079007527, shift:40), (scale:1632123990, shift:41), (scale:1966857147, shift:40), (scale:1857538476, shift:41), (scale:1411123925, shift:41), (scale:1610604323, shift:41), (scale:2002627904, shift:41), (scale:1195861092, shift:41), (scale:1828151827, shift:41), (scale:2071265342, shift:41), (scale:1246215886, shift:40), (scale:1108466089, shift:40), (scale:1771980164, shift:41), (scale:1884560950, shift:41), (scale:1916429054, shift:41)], zero_point: [24], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15
45 Clamp aten_clamp_default_15
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [24], quantMax: [127], dimension: 0 aten_clamp_default_15
46 MemoryCopy aten_cat_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_15
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
47 Conv2D aten_convolution_default_16
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [24], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1369875316, shift:39), (scale:1568548806, shift:39), (scale:1553788332, shift:39), (scale:1499377974, shift:38), (scale:1739514589, shift:39), (scale:1307774941, shift:39), (scale:2038419584, shift:39), (scale:1415908436, shift:38), (scale:1340954455, shift:39), (scale:1422825968, shift:39), (scale:1701426027, shift:39), (scale:1244195260, shift:39), (scale:1538923994, shift:38), (scale:1215401731, shift:38), (scale:1156641215, shift:39), (scale:1123451090, shift:38), (scale:1479993827, shift:39), (scale:1282548473, shift:39), (scale:1148794779, shift:38), (scale:1770080289, shift:39), (scale:1426071078, shift:39), (scale:2023317185, shift:39), (scale:1586791952, shift:39), (scale:2129658036, shift:39), (scale:1674496049, shift:39), (scale:1370401625, shift:39), (scale:1212496799, shift:39), (scale:1725633210, shift:39), (scale:1688763671, shift:39), (scale:1628817089, shift:39), (scale:1557579474, shift:39), (scale:1664443013, shift:39), (scale:1292739453, shift:39), (scale:2071755892, shift:39), (scale:1965711901, shift:39), (scale:1101707795, shift:39), (scale:1163605187, shift:38), (scale:1909031527, shift:39), (scale:1610118791, shift:39), (scale:1679791838, shift:39), (scale:1682955929, shift:39), (scale:1480927600, shift:39), (scale:1249636233, shift:38), (scale:1516625179, shift:39), (scale:1518441167, shift:39), (scale:1482578055, shift:39), (scale:1915530235, shift:39), (scale:1694064578, shift:39)], zero_point: [-42], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16
48 Clamp aten_clamp_default_16
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-42], quantMax: [127], dimension: 0 aten_clamp_default_16
49 Conv2D aten_convolution_default_17
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-42], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1798806942, shift:40), (scale:1414580669, shift:40), (scale:2002083436, shift:41), (scale:2089909143, shift:41), (scale:1998018052, shift:40), (scale:1479030416, shift:40), (scale:1282436008, shift:40), (scale:1130830554, shift:40), (scale:1396725645, shift:40), (scale:1425170530, shift:40), (scale:1362946792, shift:40), (scale:2031748707, shift:41), (scale:1258878184, shift:39), (scale:1396698579, shift:40), (scale:2067727336, shift:41), (scale:1287066978, shift:40), (scale:2108457171, shift:40), (scale:1087581350, shift:40), (scale:1204705784, shift:40), (scale:2007526504, shift:41), (scale:1271210501, shift:40), (scale:1513306980, shift:40), (scale:1100619062, shift:40), (scale:1769658849, shift:40), (scale:1154594293, shift:40), (scale:1080860225, shift:40), (scale:1705225030, shift:40), (scale:1613984888, shift:40), (scale:1654352658, shift:40), (scale:2121157177, shift:41), (scale:1523900589, shift:40), (scale:1302057431, shift:40), (scale:1448033267, shift:40), (scale:1083519610, shift:39), (scale:1634206458, shift:40), (scale:1615720890, shift:40), (scale:1992243941, shift:41), (scale:2108559191, shift:41), (scale:2098610389, shift:41), (scale:1551750948, shift:41), (scale:1450668396, shift:40), (scale:1578168970, shift:40), (scale:1176570498, shift:40), (scale:1112972512, shift:40), (scale:1563684660, shift:40), (scale:1750925519, shift:40), (scale:2137803057, shift:41), (scale:1373935363, shift:40), (scale:1287475890, shift:40), (scale:2052782271, shift:41), (scale:1723290443, shift:41), (scale:2131439313, shift:41), (scale:1828406628, shift:41), (scale:1547175257, shift:40), (scale:1375484503, shift:40), (scale:1492482557, shift:40), (scale:1378107035, shift:40), (scale:1106261901, shift:40), (scale:2084088602, shift:40), (scale:1533579560, shift:40), (scale:1380993362, shift:40), (scale:1846455384, shift:40), (scale:1113051109, shift:40), (scale:1127061548, shift:40), (scale:1074081635, shift:40), (scale:1214197580, shift:39), (scale:1299861195, shift:40), (scale:1128656284, shift:40), (scale:1383461408, shift:40), (scale:1145243763, shift:40), (scale:2083227055, shift:41), (scale:1290260613, shift:40), (scale:1549007032, shift:40), (scale:1291922182, shift:40), (scale:1245786443, shift:40), (scale:1873280132, shift:40), (scale:2105887522, shift:41), (scale:1207140830, shift:39), (scale:1127457135, shift:40), (scale:1176491797, shift:40), (scale:1402729718, shift:40), (scale:1672410471, shift:40), (scale:1491111848, shift:40), (scale:1441102064, shift:40), (scale:1118948581, shift:40), (scale:1166263169, shift:40), (scale:1290706689, shift:40), (scale:1346777802, shift:40), (scale:1855895129, shift:41), (scale:1211759203, shift:40), (scale:1228009705, shift:40), (scale:1696777167, shift:40), (scale:1181728848, shift:40), (scale:1283656914, shift:39), (scale:1559664040, shift:39), (scale:2068912847, shift:41), (scale:1096327152, shift:40), (scale:2050769150, shift:41), (scale:1371098693, shift:40), (scale:1176095586, shift:40), (scale:1501228358, shift:39), (scale:2135578401, shift:41), (scale:1078103713, shift:40), (scale:1139879813, shift:39), (scale:1325500744, shift:40), (scale:1690631100, shift:40), (scale:1209905982, shift:40), (scale:1277467333, shift:40), (scale:1706402421, shift:40), (scale:1327099123, shift:40), (scale:1530572162, shift:40), (scale:1104916593, shift:40), (scale:1344982982, shift:40), (scale:1245308720, shift:40), (scale:1285099766, shift:39), (scale:1703812264, shift:40), (scale:1357242741, shift:40), (scale:1110102217, shift:40), (scale:1423085579, shift:40), (scale:1437424771, shift:40), (scale:1195448738, shift:40), (scale:1092366390, shift:39), (scale:1109186746, shift:39), (scale:1102355064, shift:40), (scale:1645150682, shift:41), (scale:1787741127, shift:41), (scale:1263099929, shift:40), (scale:1662060044, shift:40), (scale:1961927618, shift:40), (scale:1686259552, shift:40), (scale:1138356908, shift:40), (scale:1857940313, shift:41), (scale:1311753995, shift:40), (scale:1378653674, shift:40), (scale:1248139144, shift:40), (scale:1400146223, shift:40), (scale:1234535952, shift:39), (scale:1522842082, shift:40), (scale:2050017326, shift:41), (scale:1106791363, shift:39), (scale:2086005533, shift:40), (scale:1331207189, shift:40), (scale:1129703860, shift:40), (scale:1912595849, shift:41), (scale:1314506759, shift:40), (scale:1188760509, shift:40), (scale:1198272604, shift:40), (scale:1076850015, shift:40), (scale:2064110006, shift:41), (scale:1783899353, shift:41), (scale:2053339215, shift:41), (scale:1078982124, shift:40), (scale:1343416458, shift:40), (scale:1402702443, shift:40), (scale:1915331020, shift:41), (scale:1168157718, shift:39), (scale:1204232121, shift:40), (scale:1124710721, shift:40), (scale:1314919835, shift:40), (scale:1643695859, shift:40), (scale:1525689162, shift:40), (scale:1307285008, shift:40), (scale:1160461679, shift:40), (scale:1531487113, shift:39), (scale:1306820193, shift:40), (scale:1458250756, shift:40), (scale:1570820423, shift:41), (scale:1233881568, shift:40), (scale:1151793329, shift:39), (scale:1197669230, shift:40), (scale:1734549471, shift:40), (scale:1116046535, shift:40), (scale:1593290488, shift:40), (scale:1107401399, shift:40), (scale:1713177992, shift:39), (scale:1173414444, shift:40), (scale:1201599178, shift:39), (scale:1687297344, shift:40), (scale:1716287722, shift:41), (scale:1303499970, shift:40), (scale:1520734645, shift:40), (scale:1643887198, shift:40), (scale:1534183975, shift:40), (scale:2129176972, shift:40), (scale:1326426834, shift:39), (scale:2125750773, shift:41), (scale:1661530374, shift:41), (scale:1832585275, shift:40), (scale:1104754403, shift:40), (scale:1212973030, shift:40), (scale:1318922654, shift:40), (scale:1867729007, shift:40)], zero_point: [-2], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17
50 Clamp aten_clamp_default_17
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-2], quantMax: [127], dimension: 0 aten_clamp_default_17
51 MemoryCopy aten_cat_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_17
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
52 Conv2D aten_convolution_default_18
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-42], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1959193280, shift:40), (scale:1518128976, shift:40), (scale:1568868722, shift:40), (scale:1185933208, shift:39), (scale:1098792492, shift:39), (scale:1636563323, shift:40), (scale:1486511485, shift:40), (scale:2023095555, shift:40), (scale:1358658318, shift:40), (scale:1366467411, shift:40), (scale:1500067207, shift:40), (scale:1268654385, shift:40), (scale:1309322072, shift:40), (scale:1438311198, shift:40), (scale:1254205574, shift:40), (scale:1172747255, shift:39), (scale:1761164766, shift:39), (scale:1979350931, shift:40), (scale:1542889593, shift:40), (scale:1097461029, shift:39), (scale:1297660691, shift:40), (scale:1631872599, shift:40), (scale:1741119961, shift:40), (scale:1768881209, shift:40), (scale:2082334903, shift:40), (scale:1132990563, shift:39), (scale:2023023725, shift:40), (scale:1078969424, shift:39), (scale:1097524323, shift:39), (scale:2129724964, shift:40), (scale:1539933726, shift:40), (scale:2046012113, shift:40), (scale:1430789738, shift:40), (scale:1160044231, shift:39), (scale:1985971806, shift:40), (scale:1563601170, shift:40), (scale:1547445713, shift:40), (scale:1863214944, shift:40), (scale:1130563117, shift:40), (scale:1433917165, shift:40), (scale:1154685069, shift:39), (scale:1409122194, shift:40), (scale:1814875057, shift:40), (scale:1196579700, shift:40), (scale:1364112732, shift:40), (scale:1980217266, shift:40), (scale:1962547024, shift:40), (scale:2127212988, shift:40), (scale:1722591607, shift:40), (scale:1477946091, shift:40), (scale:1112977405, shift:40), (scale:1655495800, shift:40), (scale:1718288767, shift:40), (scale:1083759252, shift:40), (scale:2126267119, shift:40), (scale:1768998427, shift:40), (scale:1077449953, shift:39), (scale:1080352104, shift:39), (scale:1406119481, shift:39), (scale:1568288667, shift:40), (scale:1382574253, shift:39), (scale:1748350873, shift:40), (scale:1549528270, shift:40), (scale:1336452359, shift:40), (scale:1155636560, shift:39), (scale:1452487992, shift:40), (scale:1565771693, shift:40), (scale:1364060057, shift:40), (scale:1430081533, shift:40), (scale:1159021326, shift:39), (scale:1375789417, shift:40), (scale:1307844763, shift:40), (scale:1506649981, shift:40), (scale:1706205461, shift:40), (scale:1525444523, shift:39), (scale:1737779854, shift:40), (scale:1523676666, shift:40), (scale:2110363692, shift:40), (scale:1647227097, shift:40), (scale:1811793019, shift:40), (scale:1386764975, shift:40), (scale:1330369898, shift:40), (scale:1726041645, shift:40), (scale:1329236854, shift:40), (scale:1243045026, shift:40), (scale:1353598657, shift:40), (scale:1645406668, shift:40), (scale:1932555499, shift:40), (scale:1343185665, shift:40), (scale:1866721301, shift:40), (scale:1657696304, shift:40), (scale:1122858750, shift:39), (scale:1405592518, shift:40), (scale:1905031811, shift:40), (scale:1776992406, shift:40), (scale:1170487413, shift:40), (scale:2101946436, shift:40), (scale:1523902879, shift:40), (scale:1877454407, shift:40), (scale:1363216312, shift:40), (scale:1622779826, shift:40), (scale:1333300468, shift:40), (scale:1831443694, shift:41), (scale:1431323780, shift:39), (scale:1458932311, shift:40), (scale:1368301165, shift:38), (scale:1461961153, shift:40), (scale:1347733769, shift:39), (scale:1496753958, shift:40), (scale:1344443423, shift:40), (scale:1208514349, shift:40), (scale:1091778319, shift:40), (scale:1633505957, shift:40), (scale:1156223278, shift:39), (scale:1736194800, shift:40), (scale:1090361494, shift:39), (scale:1577402989, shift:40), (scale:1719242964, shift:40), (scale:1368770352, shift:40), (scale:1603187344, shift:40), (scale:1449390755, shift:40), (scale:1440724174, shift:40), (scale:1505318311, shift:40), (scale:1074192712, shift:39), (scale:1631865000, shift:42), (scale:1796290801, shift:40), (scale:1424789726, shift:40), (scale:1657542233, shift:40), (scale:1427598601, shift:40), (scale:1089934052, shift:40), (scale:1079326597, shift:39), (scale:1101225143, shift:39), (scale:1190733863, shift:40), (scale:1438438202, shift:40), (scale:1091101449, shift:39), (scale:1315256084, shift:40), (scale:1898299130, shift:40), (scale:1665796050, shift:40), (scale:2121889637, shift:40), (scale:1566730263, shift:40), (scale:1532861465, shift:40), (scale:1651819757, shift:40), (scale:1368753175, shift:39), (scale:1086590197, shift:39), (scale:1267269935, shift:39), (scale:1113512905, shift:40), (scale:1102451046, shift:39), (scale:1978694881, shift:40), (scale:1821165513, shift:40), (scale:1837846372, shift:41), (scale:1798371171, shift:40), (scale:1201092514, shift:39), (scale:1364963140, shift:40), (scale:1514859554, shift:39), (scale:1318292734, shift:40), (scale:1711926376, shift:40), (scale:1104634790, shift:39), (scale:1905902310, shift:40), (scale:1640358771, shift:40), (scale:1983679484, shift:40), (scale:1076265587, shift:39), (scale:1310985411, shift:39), (scale:1717163114, shift:40), (scale:1195659024, shift:39), (scale:1450902001, shift:40), (scale:1430722592, shift:40), (scale:1693386883, shift:40), (scale:1632294316, shift:40), (scale:1086153594, shift:39), (scale:1709582315, shift:40), (scale:1826044141, shift:40), (scale:1772994063, shift:40), (scale:1293148814, shift:40), (scale:1863226395, shift:39), (scale:1617257849, shift:40), (scale:1423968779, shift:40), (scale:1580448592, shift:40), (scale:1380958905, shift:40), (scale:1276959732, shift:40), (scale:1257681741, shift:40), (scale:1223364265, shift:40), (scale:1984247880, shift:40), (scale:1461833733, shift:40), (scale:1582914243, shift:40), (scale:1631449217, shift:40), (scale:1805535458, shift:41), (scale:1963331327, shift:39), (scale:2070618659, shift:40), (scale:1732400080, shift:40), (scale:1435777464, shift:40), (scale:1775663234, shift:40), (scale:1073816904, shift:39)], zero_point: [-2], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18
53 Clamp aten_clamp_default_18
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-2], quantMax: [127], dimension: 0 aten_clamp_default_18
54 MemoryCopy aten_cat_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_18
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
55 Conv2D aten_convolution_default_19
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-2], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1825281797, shift:38), (scale:2040638115, shift:39), (scale:1869856751, shift:38), (scale:1972177571, shift:38), (scale:1147348435, shift:37), (scale:1129042059, shift:38), (scale:1769060896, shift:39), (scale:1772104748, shift:38), (scale:1945501816, shift:39), (scale:1210676252, shift:38), (scale:1625757078, shift:38), (scale:2043342771, shift:39), (scale:1810859869, shift:39), (scale:1550870545, shift:38), (scale:1797148956, shift:39), (scale:2115427895, shift:39), (scale:2058306320, shift:39), (scale:1825326983, shift:39), (scale:1688336495, shift:39), (scale:1618759784, shift:39), (scale:1912737748, shift:39), (scale:2017354325, shift:39), (scale:1354010970, shift:38), (scale:1677129436, shift:39), (scale:1622449406, shift:38), (scale:1111153802, shift:38), (scale:2021769350, shift:39), (scale:1175815914, shift:38), (scale:1263608934, shift:38), (scale:1176117219, shift:38), (scale:1544830558, shift:38), (scale:1102547554, shift:38), (scale:1304835994, shift:38), (scale:1667283620, shift:38), (scale:1649004903, shift:38), (scale:1315270337, shift:38), (scale:2000702899, shift:38), (scale:1969784149, shift:39), (scale:1140279512, shift:38), (scale:2029345345, shift:39), (scale:1703874388, shift:39), (scale:1093008717, shift:38), (scale:1665174382, shift:38), (scale:1224945096, shift:38), (scale:2076983198, shift:39), (scale:1727552489, shift:39), (scale:1170134373, shift:38), (scale:2102912883, shift:39), (scale:1645240511, shift:39), (scale:1828641747, shift:39), (scale:2059222799, shift:38), (scale:1129468081, shift:38), (scale:1731350517, shift:39), (scale:1378310121, shift:38), (scale:1212147117, shift:38), (scale:1906539409, shift:39), (scale:2114099274, shift:38), (scale:1899976950, shift:39), (scale:1627861657, shift:39), (scale:1096651330, shift:38), (scale:1734481904, shift:39), (scale:1079340964, shift:38), (scale:1696762204, shift:38), (scale:1985031577, shift:39)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19
56 Clamp aten_clamp_default_19
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-8], quantMax: [127], dimension: 0 aten_clamp_default_19
57 Conv2D aten_convolution_default_20
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1435152420, shift:41), (scale:1891745874, shift:41), (scale:1719400945, shift:41), (scale:1625007168, shift:41), (scale:1620140459, shift:41), (scale:2140501554, shift:41), (scale:2118960677, shift:41), (scale:1252199026, shift:40), (scale:1598155477, shift:41), (scale:1936559996, shift:41), (scale:1169297717, shift:40), (scale:1580018986, shift:41), (scale:1604571002, shift:40), (scale:1617629476, shift:41), (scale:1816895935, shift:41), (scale:2015763416, shift:41), (scale:1919912330, shift:41), (scale:1513107780, shift:41), (scale:1841506150, shift:41), (scale:1088980632, shift:40), (scale:1272495783, shift:40), (scale:1580554418, shift:41), (scale:1559009780, shift:41), (scale:1086829409, shift:40), (scale:2030869070, shift:41), (scale:2115044853, shift:41), (scale:1102307808, shift:40), (scale:1893845164, shift:41), (scale:1390920827, shift:41), (scale:1103891722, shift:40), (scale:1334969611, shift:41), (scale:1749628550, shift:41), (scale:1509868679, shift:41), (scale:1794758739, shift:41), (scale:1669487138, shift:41), (scale:1711048371, shift:41), (scale:1254203228, shift:40), (scale:1960731837, shift:41), (scale:1203795712, shift:39), (scale:1644827319, shift:41), (scale:1092118913, shift:40), (scale:1228516059, shift:40), (scale:1271109209, shift:41), (scale:1825615075, shift:40), (scale:1966179106, shift:41), (scale:1174454795, shift:40), (scale:1422738525, shift:41), (scale:1087064266, shift:39), (scale:1399945647, shift:40), (scale:1229677357, shift:40), (scale:1702138875, shift:40), (scale:1674054076, shift:41), (scale:1903732408, shift:41), (scale:1859573698, shift:41), (scale:1943114483, shift:40), (scale:1324834439, shift:40), (scale:2074235375, shift:41), (scale:1683645667, shift:41), (scale:1203080847, shift:39), (scale:2038979715, shift:41), (scale:1633155240, shift:41), (scale:1965386701, shift:41), (scale:2111633125, shift:41), (scale:1752531707, shift:41), (scale:1450203099, shift:41), (scale:2076068024, shift:41), (scale:2097638270, shift:41), (scale:1662702719, shift:41), (scale:2001780559, shift:41), (scale:1136028172, shift:40), (scale:1572362295, shift:41), (scale:1105503662, shift:40), (scale:1126831441, shift:41), (scale:1459041771, shift:41), (scale:2044385438, shift:41), (scale:1176967211, shift:40), (scale:1494661848, shift:41), (scale:1410979594, shift:40), (scale:1627234498, shift:41), (scale:1630531618, shift:41), (scale:1115659964, shift:40), (scale:1236224412, shift:40), (scale:1599164562, shift:41), (scale:1691769563, shift:40), (scale:1470707851, shift:41), (scale:1577086998, shift:41), (scale:1992903654, shift:41), (scale:1616506679, shift:41), (scale:2071820376, shift:41), (scale:1585536810, shift:41), (scale:1831459889, shift:41), (scale:1867803606, shift:41), (scale:2003697731, shift:41), (scale:1597100011, shift:41), (scale:1327187658, shift:40), (scale:1549562165, shift:40), (scale:1574119554, shift:41), (scale:1146140512, shift:41), (scale:1733482651, shift:41), (scale:1908701905, shift:40), (scale:1423711795, shift:41), (scale:1197212842, shift:40), (scale:2055377751, shift:41), (scale:1614613323, shift:41), (scale:2032994327, shift:41), (scale:1917463307, shift:41), (scale:1813702319, shift:41), (scale:1754306156, shift:41), (scale:2104698284, shift:41), (scale:2040839583, shift:41), (scale:1737432141, shift:41), (scale:1915911984, shift:41), (scale:1666720078, shift:41), (scale:1837007619, shift:41), (scale:1184777010, shift:40), (scale:1828232787, shift:41), (scale:1562165790, shift:41), (scale:1154136549, shift:40), (scale:1545297147, shift:41), (scale:1372289195, shift:40), (scale:2084951554, shift:41), (scale:1991797153, shift:41), (scale:1115916220, shift:40), (scale:1834536032, shift:41), (scale:1531134857, shift:41), (scale:1918275231, shift:41), (scale:2052996238, shift:41), (scale:1884729195, shift:41), (scale:1194520904, shift:41), (scale:1621383416, shift:41), (scale:1082527322, shift:40), (scale:1704009129, shift:41), (scale:2105369814, shift:41), (scale:2119117546, shift:41), (scale:2037759680, shift:41), (scale:1982126769, shift:41), (scale:1095669245, shift:40), (scale:1953721605, shift:40), (scale:1174398745, shift:40), (scale:1540138904, shift:41), (scale:2049447518, shift:41), (scale:1742511590, shift:41), (scale:2047981793, shift:41), (scale:1221259689, shift:40), (scale:1574396582, shift:41), (scale:1564105884, shift:41), (scale:1757516247, shift:41), (scale:2055793024, shift:41), (scale:1392635645, shift:41), (scale:1083050131, shift:40), (scale:1976758651, shift:40), (scale:1421300377, shift:40), (scale:1601519392, shift:41), (scale:1410696119, shift:40), (scale:1718211532, shift:41), (scale:1115354463, shift:40), (scale:1762128670, shift:41), (scale:2141503476, shift:41), (scale:2053246584, shift:41), (scale:1218235299, shift:41), (scale:1482039151, shift:41), (scale:1425019845, shift:41), (scale:1899381612, shift:40), (scale:1941938500, shift:41), (scale:1487544530, shift:41), (scale:1391111183, shift:41), (scale:1655967188, shift:41), (scale:1358690721, shift:40), (scale:1762099302, shift:41), (scale:1182821964, shift:40), (scale:1324632622, shift:41), (scale:1178312061, shift:40), (scale:2024559021, shift:41), (scale:1251066648, shift:40), (scale:1282832593, shift:40), (scale:1508306254, shift:41), (scale:1633976475, shift:41), (scale:1651226727, shift:41), (scale:1119415426, shift:40), (scale:1335518116, shift:41), (scale:1271670966, shift:40), (scale:1799657323, shift:41), (scale:1752991749, shift:41), (scale:1791948343, shift:41), (scale:1932369831, shift:41), (scale:1776501372, shift:41), (scale:1121638816, shift:40), (scale:2062198342, shift:41), (scale:1156636071, shift:41), (scale:1200327396, shift:40), (scale:1572408317, shift:41), (scale:1462041091, shift:41), (scale:1916306664, shift:41), (scale:1647848664, shift:41), (scale:1351338099, shift:41), (scale:1876200412, shift:41), (scale:1104671234, shift:40), (scale:1695648318, shift:41), (scale:1824706092, shift:41), (scale:1907033647, shift:41), (scale:1240919299, shift:40), (scale:1412005333, shift:41), (scale:2135912411, shift:41), (scale:1985746224, shift:41), (scale:1769118845, shift:41), (scale:1475159644, shift:41), (scale:1451721651, shift:41), (scale:1598827902, shift:41), (scale:1672406769, shift:41), (scale:2137199061, shift:41), (scale:1752128968, shift:40), (scale:1472341101, shift:41), (scale:1884925282, shift:41), (scale:1808807854, shift:41), (scale:1369293367, shift:41), (scale:1559956547, shift:40), (scale:1767435365, shift:41), (scale:1138093260, shift:40), (scale:1783147545, shift:41), (scale:1099044980, shift:40), (scale:1427713395, shift:41), (scale:1613672645, shift:41), (scale:1142428118, shift:40), (scale:1228093980, shift:41), (scale:1916516540, shift:41), (scale:1515830429, shift:40), (scale:1878616486, shift:41), (scale:1184260291, shift:40), (scale:1693094536, shift:41), (scale:1536625462, shift:41), (scale:1967038663, shift:41), (scale:1810630833, shift:41), (scale:2054934720, shift:40), (scale:1224807962, shift:40), (scale:1744377905, shift:41), (scale:1880693930, shift:41), (scale:1783810121, shift:41), (scale:1773544852, shift:41), (scale:2037027981, shift:41), (scale:1190987136, shift:40), (scale:1952464860, shift:41), (scale:1088381090, shift:40), (scale:1677108730, shift:41), (scale:1879932146, shift:41), (scale:1709650694, shift:41), (scale:1863283586, shift:41), (scale:1475554056, shift:41), (scale:2031577489, shift:41), (scale:1520600080, shift:41), (scale:1744324899, shift:41), (scale:1427691637, shift:40), (scale:1700335952, shift:41), (scale:1495786615, shift:41), (scale:1248187487, shift:40), (scale:1830830263, shift:41), (scale:1792449931, shift:41)], zero_point: [-7], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20
58 Clamp aten_clamp_default_20
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-7], quantMax: [127], dimension: 0 aten_clamp_default_20
59 MemoryCopy aten_cat_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
60 Conv2D aten_convolution_default_21
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1806499762, shift:41), (scale:1109753011, shift:40), (scale:2082705243, shift:41), (scale:1955119999, shift:40), (scale:1170261227, shift:40), (scale:1314903502, shift:40), (scale:1534050907, shift:40), (scale:1287121966, shift:40), (scale:1578194753, shift:41), (scale:1683233079, shift:41), (scale:1541144766, shift:40), (scale:1434098298, shift:40), (scale:1649902471, shift:41), (scale:1309523207, shift:40), (scale:1981127891, shift:41), (scale:1218628368, shift:40), (scale:1129577280, shift:40), (scale:1350563333, shift:40), (scale:2029356786, shift:41), (scale:1315436069, shift:40), (scale:1141879075, shift:40), (scale:1663282205, shift:41), (scale:1080564307, shift:40), (scale:1223254132, shift:40), (scale:1582738949, shift:40), (scale:1305567181, shift:39), (scale:2053772168, shift:41), (scale:1320702204, shift:40), (scale:1120781765, shift:40), (scale:1941259987, shift:41), (scale:1456134585, shift:40), (scale:1697850577, shift:41), (scale:1222997070, shift:40), (scale:1563741109, shift:40), (scale:1591578785, shift:40), (scale:1997225977, shift:41), (scale:1774798553, shift:41), (scale:2068410975, shift:41), (scale:1372870829, shift:40), (scale:2128086853, shift:41), (scale:1562128184, shift:40), (scale:1126558620, shift:40), (scale:1118410012, shift:40), (scale:1583839362, shift:40), (scale:1696009153, shift:40), (scale:1960817793, shift:40), (scale:1523854581, shift:40), (scale:1282528435, shift:40), (scale:1314052181, shift:40), (scale:2130717279, shift:41), (scale:1293063659, shift:40), (scale:1152586480, shift:40), (scale:2142931954, shift:41), (scale:1089825148, shift:40), (scale:1108821286, shift:40), (scale:1459950306, shift:39), (scale:1804183791, shift:40), (scale:2105068610, shift:40), (scale:1926731847, shift:41), (scale:1819194536, shift:40), (scale:1354606478, shift:40), (scale:1112934808, shift:40), (scale:1262195863, shift:40), (scale:1532947628, shift:40), (scale:1954056117, shift:40), (scale:1928415148, shift:40), (scale:1903926883, shift:41), (scale:1812562510, shift:40), (scale:1657977300, shift:41), (scale:1233722382, shift:40), (scale:1298608882, shift:40), (scale:1946081211, shift:41), (scale:1626835520, shift:41), (scale:1170118952, shift:40), (scale:1179458496, shift:40), (scale:1304273815, shift:39), (scale:1768550283, shift:41), (scale:1151422584, shift:40), (scale:1231784527, shift:40), (scale:2008994219, shift:41), (scale:1822394956, shift:41), (scale:1905049680, shift:41), (scale:2106338249, shift:41), (scale:2113059096, shift:41), (scale:2143769665, shift:41), (scale:1846427835, shift:41), (scale:1364488976, shift:41), (scale:1144720809, shift:40), (scale:1569334503, shift:40), (scale:1199708067, shift:40), (scale:2146111064, shift:41), (scale:1951230141, shift:41), (scale:2013129408, shift:40), (scale:1197401676, shift:40), (scale:1263706088, shift:39), (scale:1903679043, shift:41), (scale:1116844990, shift:40), (scale:2017577620, shift:41), (scale:1740203857, shift:41), (scale:1159989063, shift:40), (scale:1728712822, shift:40), (scale:1239826855, shift:40), (scale:1906985655, shift:41), (scale:1368104670, shift:41), (scale:1093686353, shift:40), (scale:1371105422, shift:40), (scale:1299257221, shift:40), (scale:1772315863, shift:41), (scale:1092133150, shift:40), (scale:1278630967, shift:40), (scale:1158781205, shift:40), (scale:1829680963, shift:41), (scale:2114704433, shift:41), (scale:1953721785, shift:41), (scale:1806671315, shift:40), (scale:1946026235, shift:41), (scale:2109908637, shift:41), (scale:1286096495, shift:40), (scale:1218802160, shift:40), (scale:2014274948, shift:40), (scale:1763945560, shift:40), (scale:2067936786, shift:40), (scale:1329419016, shift:40), (scale:1897504016, shift:41), (scale:2062189209, shift:41), (scale:1095005506, shift:40), (scale:1492318210, shift:40), (scale:1594559839, shift:40), (scale:1145201893, shift:40), (scale:1289133420, shift:40), (scale:1862418476, shift:41), (scale:1424376340, shift:40), (scale:1996129682, shift:41), (scale:2086169619, shift:41), (scale:1258535310, shift:40), (scale:1167927528, shift:40), (scale:2068742084, shift:41), (scale:1343251898, shift:40), (scale:1195989315, shift:40), (scale:1815108771, shift:41), (scale:2090229060, shift:41), (scale:1596353808, shift:40), (scale:1892079489, shift:40), (scale:1381073965, shift:40), (scale:1223926557, shift:40), (scale:1717202805, shift:41), (scale:1887267756, shift:40), (scale:1627574918, shift:40), (scale:1279158252, shift:40), (scale:1144372957, shift:40), (scale:1907049764, shift:41), (scale:1112290677, shift:40), (scale:1299509895, shift:40), (scale:1172269996, shift:40), (scale:1080924605, shift:40), (scale:1511391888, shift:41), (scale:1881602912, shift:40), (scale:1911790046, shift:41), (scale:1911540416, shift:41), (scale:1160688528, shift:40), (scale:2108485532, shift:41), (scale:1091256222, shift:40), (scale:2055688266, shift:41), (scale:1619451201, shift:41), (scale:1087080472, shift:39), (scale:1179168753, shift:40), (scale:1156615836, shift:40), (scale:1126483767, shift:40), (scale:2080638544, shift:41), (scale:1104184240, shift:40), (scale:2076217909, shift:40), (scale:1207469158, shift:40), (scale:1829331230, shift:41), (scale:1555582472, shift:40), (scale:1554591832, shift:40), (scale:1762861085, shift:40), (scale:1594433233, shift:40), (scale:1243015367, shift:40), (scale:1250790246, shift:40), (scale:1112118317, shift:40), (scale:1679328180, shift:40), (scale:2117157933, shift:40), (scale:1367713750, shift:40), (scale:1691180766, shift:41), (scale:1716388374, shift:41), (scale:1424744786, shift:40), (scale:1183745004, shift:40), (scale:2129379413, shift:41), (scale:1179427695, shift:40), (scale:1985975976, shift:41), (scale:1292570130, shift:40), (scale:1551002999, shift:40), (scale:1830075106, shift:41), (scale:1244443397, shift:40), (scale:2084345924, shift:40), (scale:1379910787, shift:40), (scale:2098995834, shift:41), (scale:2030409386, shift:40), (scale:1446495271, shift:40), (scale:1430417062, shift:41), (scale:1393700064, shift:40), (scale:1987661247, shift:41), (scale:1440744739, shift:40), (scale:2070896709, shift:41), (scale:1893374198, shift:40), (scale:1407702351, shift:40), (scale:1560596918, shift:40), (scale:1486709147, shift:40), (scale:1684967774, shift:40), (scale:1870588215, shift:41), (scale:1429321932, shift:40), (scale:1114147948, shift:40), (scale:1765563678, shift:41), (scale:1081723457, shift:40), (scale:2056148667, shift:41), (scale:1448173557, shift:40), (scale:1309785909, shift:40), (scale:1418377970, shift:40), (scale:1981510215, shift:41), (scale:1193696266, shift:40), (scale:1165558371, shift:40), (scale:1372530856, shift:40), (scale:1174791813, shift:40), (scale:1089909671, shift:40), (scale:1528338429, shift:40), (scale:1863644779, shift:41), (scale:1528262322, shift:41), (scale:1670833420, shift:40), (scale:1388740953, shift:40), (scale:2045477256, shift:41), (scale:1543021825, shift:40), (scale:1166185759, shift:40), (scale:1170818597, shift:40), (scale:1170336618, shift:40), (scale:1252296084, shift:40), (scale:1755508105, shift:41), (scale:2097064336, shift:40), (scale:1102261159, shift:39), (scale:2079547801, shift:41), (scale:1435944467, shift:40), (scale:2047953678, shift:41), (scale:1477419476, shift:40), (scale:1667172062, shift:41), (scale:1120137455, shift:40), (scale:1388587934, shift:40), (scale:1199872637, shift:40), (scale:1632428734, shift:40), (scale:1772148070, shift:41), (scale:1102398688, shift:40), (scale:2074036961, shift:41), (scale:1363012059, shift:40), (scale:1291613782, shift:40), (scale:1944174962, shift:41), (scale:1869444824, shift:41), (scale:1222636325, shift:40), (scale:2035697995, shift:40)], zero_point: [-7], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21
61 Clamp aten_clamp_default_21
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-7], quantMax: [127], dimension: 0 aten_clamp_default_21
62 MemoryCopy aten_cat_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_21
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
63 Conv2D aten_convolution_default_22
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-7], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1690352354, shift:40), (scale:1421300132, shift:40), (scale:2137307384, shift:41), (scale:2006070535, shift:40), (scale:1134799457, shift:40), (scale:1956411906, shift:40), (scale:1630647195, shift:40), (scale:1320385040, shift:40), (scale:1290109812, shift:40), (scale:1937415038, shift:40), (scale:1819008765, shift:40), (scale:1455538013, shift:40), (scale:1520283924, shift:40), (scale:1688022204, shift:40), (scale:1407885650, shift:40), (scale:1266441011, shift:40), (scale:1627707441, shift:40), (scale:1530282100, shift:40), (scale:1248965112, shift:40), (scale:1939048150, shift:40), (scale:1844904696, shift:40), (scale:1091315331, shift:39), (scale:1674049818, shift:40), (scale:1602887326, shift:40), (scale:1373906268, shift:40), (scale:1690931394, shift:40), (scale:1909607726, shift:40), (scale:1495878474, shift:40), (scale:1363754665, shift:40), (scale:1134015822, shift:40), (scale:1147312382, shift:39), (scale:1349121279, shift:40), (scale:1319731338, shift:40), (scale:1099811545, shift:39), (scale:1753843019, shift:40), (scale:1459893892, shift:40), (scale:2088750270, shift:40), (scale:1277107714, shift:39), (scale:1671420111, shift:40), (scale:1175365725, shift:39), (scale:1308510001, shift:40), (scale:1508394136, shift:40), (scale:1480168663, shift:40), (scale:1897959408, shift:40), (scale:1916806802, shift:40), (scale:1776340587, shift:40), (scale:1145584461, shift:39), (scale:1374911529, shift:40), (scale:1906180167, shift:40), (scale:1335576634, shift:40), (scale:1236067326, shift:40), (scale:1288979863, shift:40), (scale:1126201516, shift:39), (scale:1231762995, shift:40), (scale:1237240308, shift:40), (scale:1096627736, shift:40), (scale:1536177575, shift:40), (scale:1489322303, shift:40), (scale:1594148806, shift:40), (scale:1698678794, shift:40), (scale:1311653058, shift:40), (scale:1220702004, shift:40), (scale:1379986038, shift:40), (scale:1712254155, shift:40)], zero_point: [-36], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22
64 Clamp aten_clamp_default_22
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-36], quantMax: [127], dimension: 0 aten_clamp_default_22
65 Conv2D aten_convolution_default_23
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-36], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1427079179, shift:40), (scale:1256297511, shift:41), (scale:1371668481, shift:41), (scale:1499442799, shift:41), (scale:1653590356, shift:41), (scale:2034333148, shift:41), (scale:1293282427, shift:40), (scale:1263269203, shift:40), (scale:1599080258, shift:41), (scale:1672220750, shift:41), (scale:1751454028, shift:41), (scale:1976505687, shift:41), (scale:1405544186, shift:41), (scale:1591427737, shift:41), (scale:1965092992, shift:41), (scale:1467818642, shift:41), (scale:1106785222, shift:40), (scale:1198227948, shift:40), (scale:1229037379, shift:41), (scale:1281878355, shift:41), (scale:1772446990, shift:41), (scale:1261775010, shift:41), (scale:1653251393, shift:41), (scale:1550223586, shift:41), (scale:1323986194, shift:41), (scale:1900881894, shift:41), (scale:1317804264, shift:41), (scale:1520768097, shift:41), (scale:1594586785, shift:41), (scale:1368839590, shift:41), (scale:1448414009, shift:41), (scale:1851098386, shift:41), (scale:1208270996, shift:40), (scale:1544758070, shift:41), (scale:1347667433, shift:41), (scale:1515217069, shift:41), (scale:2117877982, shift:41), (scale:1462911225, shift:41), (scale:1327721133, shift:41), (scale:1835924426, shift:41), (scale:1973041083, shift:41), (scale:1541363728, shift:41), (scale:2021442780, shift:40), (scale:1514492031, shift:41), (scale:1127744960, shift:40), (scale:1676030852, shift:41), (scale:1707722457, shift:41), (scale:1907142074, shift:41), (scale:1716508604, shift:41), (scale:1421903424, shift:41), (scale:1795892754, shift:41), (scale:1833795155, shift:41), (scale:1948703524, shift:41), (scale:1677248175, shift:41), (scale:2029045011, shift:41), (scale:1572508672, shift:41), (scale:2129669763, shift:41), (scale:1889282560, shift:41), (scale:1527053059, shift:41), (scale:1344910438, shift:41), (scale:1443640851, shift:41), (scale:1458871274, shift:41), (scale:1294888004, shift:41), (scale:2045050127, shift:41), (scale:1397989795, shift:41), (scale:1669856910, shift:41), (scale:1779695553, shift:41), (scale:1462239655, shift:41), (scale:1343169331, shift:41), (scale:1920445841, shift:41), (scale:1117636007, shift:41), (scale:1945679190, shift:41), (scale:1544441802, shift:41), (scale:1274074962, shift:41), (scale:1690118571, shift:41), (scale:1327251542, shift:41), (scale:1498349387, shift:41), (scale:1428166418, shift:41), (scale:1598216422, shift:41), (scale:1931325493, shift:41), (scale:1562672413, shift:41), (scale:1887033645, shift:41), (scale:1720914568, shift:41), (scale:1502684000, shift:41), (scale:1827512463, shift:41), (scale:1638637529, shift:41), (scale:1510335160, shift:41), (scale:1427190382, shift:41), (scale:1305445125, shift:41), (scale:1710007864, shift:41), (scale:1370501721, shift:41), (scale:1085903371, shift:40), (scale:1493383328, shift:41), (scale:1484232891, shift:41), (scale:1957428851, shift:41), (scale:1103372724, shift:40), (scale:1579224190, shift:41), (scale:1144408756, shift:41), (scale:1335047309, shift:41), (scale:1851463855, shift:41), (scale:1441187233, shift:41), (scale:1506664219, shift:41), (scale:1223610806, shift:41), (scale:2035180826, shift:41), (scale:1911304573, shift:41), (scale:1283446078, shift:41), (scale:1707504773, shift:41), (scale:1542304906, shift:41), (scale:2075251714, shift:41), (scale:2101088187, shift:41), (scale:1614624226, shift:41), (scale:1156765353, shift:41), (scale:1362345387, shift:41), (scale:1496560531, shift:41), (scale:1714026391, shift:41), (scale:1570986882, shift:41), (scale:1555260996, shift:40), (scale:1604463712, shift:41), (scale:1678319709, shift:41), (scale:2026191338, shift:41), (scale:1903413671, shift:41), (scale:1514008915, shift:41), (scale:1749831475, shift:41), (scale:1373406048, shift:41), (scale:1326912580, shift:41), (scale:1604576457, shift:41), (scale:1405990084, shift:41), (scale:1638926927, shift:41), (scale:1214827201, shift:41), (scale:1659138297, shift:41), (scale:1529068495, shift:41), (scale:1367596941, shift:41), (scale:1328878088, shift:41), (scale:1630253888, shift:41), (scale:1274761964, shift:41), (scale:1430305039, shift:41), (scale:1473940659, shift:41), (scale:1114020077, shift:40), (scale:1872319199, shift:41), (scale:1417685188, shift:41), (scale:1300656716, shift:41), (scale:1497606920, shift:41), (scale:1542430179, shift:41), (scale:1596643616, shift:41), (scale:2030579691, shift:41), (scale:1554945999, shift:40), (scale:2101479618, shift:41), (scale:1562559305, shift:41), (scale:1819904059, shift:41), (scale:1114611672, shift:40), (scale:1725745551, shift:41), (scale:1817410590, shift:41), (scale:1143317613, shift:41), (scale:1478601616, shift:41), (scale:1580552443, shift:41), (scale:1442033912, shift:40), (scale:1757945326, shift:41), (scale:1319534750, shift:41), (scale:1287470778, shift:41), (scale:1619149472, shift:41), (scale:1461216777, shift:41), (scale:1950844596, shift:41), (scale:1775944819, shift:41), (scale:1375397973, shift:41), (scale:1621791452, shift:41), (scale:1435746045, shift:41), (scale:1220023562, shift:41), (scale:1081419974, shift:40), (scale:1479308771, shift:41), (scale:1695625300, shift:41), (scale:1839363976, shift:41), (scale:1304222083, shift:40), (scale:1522679230, shift:41), (scale:2097560947, shift:41), (scale:1525962824, shift:41), (scale:1747181506, shift:41), (scale:1785795238, shift:41), (scale:1077597526, shift:40), (scale:1796253865, shift:41), (scale:1443380048, shift:41), (scale:1370976032, shift:41), (scale:2022781563, shift:41), (scale:1426841434, shift:41), (scale:1538575414, shift:41), (scale:1435567305, shift:41), (scale:1369386251, shift:41), (scale:2020742888, shift:41), (scale:1439385214, shift:41), (scale:1562942022, shift:41), (scale:1436092088, shift:41), (scale:1726656228, shift:41), (scale:1742624489, shift:41), (scale:1278989188, shift:40), (scale:1293835261, shift:41), (scale:1391499586, shift:41), (scale:1490535737, shift:41), (scale:1910013903, shift:41), (scale:1859659950, shift:41), (scale:1464133177, shift:41), (scale:1343035616, shift:41), (scale:1257229067, shift:41), (scale:2061056696, shift:41), (scale:2013058595, shift:41), (scale:1442150561, shift:41), (scale:1526940858, shift:41), (scale:1636219405, shift:41), (scale:1460517247, shift:41), (scale:1653152446, shift:41), (scale:1252006925, shift:41), (scale:1348495322, shift:41), (scale:1201247198, shift:41), (scale:1323550372, shift:40), (scale:1337430030, shift:41), (scale:1910484673, shift:41), (scale:1941382340, shift:41), (scale:1541295826, shift:41), (scale:1485538540, shift:41), (scale:1768581877, shift:41), (scale:1997234307, shift:41), (scale:2081643884, shift:41), (scale:1304815948, shift:41), (scale:1147717586, shift:40), (scale:1845771396, shift:41), (scale:1612905995, shift:41), (scale:1716741720, shift:41), (scale:1608311940, shift:41), (scale:1518428223, shift:41), (scale:1989946165, shift:41), (scale:1200695273, shift:41), (scale:1915536427, shift:41), (scale:1615853895, shift:41), (scale:1274376615, shift:41), (scale:1962593168, shift:41), (scale:1553951807, shift:41), (scale:1301184313, shift:41), (scale:1150106298, shift:41), (scale:1301569481, shift:40), (scale:1106368827, shift:40), (scale:1079316847, shift:40), (scale:1755884138, shift:41), (scale:1379478773, shift:41), (scale:1823860131, shift:41), (scale:1626066697, shift:41), (scale:1081472171, shift:40), (scale:1552994833, shift:41), (scale:1322922648, shift:41), (scale:1585584588, shift:41), (scale:1381431664, shift:41), (scale:1077628118, shift:40), (scale:1541164925, shift:41), (scale:1446229182, shift:41), (scale:1972482985, shift:41), (scale:1080788890, shift:40), (scale:1359246070, shift:41), (scale:1534772392, shift:41), (scale:1989837777, shift:41)], zero_point: [86], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23
66 Clamp aten_clamp_default_23
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [86], quantMax: [127], dimension: 0 aten_clamp_default_23
67 MemoryCopy aten_cat_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_23
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
68 Conv2D aten_convolution_default_24
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-36], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:2034672292, shift:40), (scale:1381909334, shift:40), (scale:1804705589, shift:40), (scale:1474920690, shift:40), (scale:1214132937, shift:40), (scale:1905317088, shift:40), (scale:1501617640, shift:40), (scale:1088346822, shift:39), (scale:1736492486, shift:40), (scale:1515199004, shift:40), (scale:1161685298, shift:40), (scale:1469864035, shift:40), (scale:1474462265, shift:40), (scale:1470805576, shift:40), (scale:1859808825, shift:40), (scale:1398297893, shift:40), (scale:1555148614, shift:40), (scale:2008975616, shift:40), (scale:1876828286, shift:40), (scale:1478940034, shift:40), (scale:1296246122, shift:40), (scale:1861769795, shift:40), (scale:1111781420, shift:39), (scale:1480642016, shift:40), (scale:1712328130, shift:40), (scale:1222911277, shift:40), (scale:1342149902, shift:39), (scale:1459229481, shift:40), (scale:1735789144, shift:40), (scale:1530543444, shift:40), (scale:1814873911, shift:40), (scale:2081832882, shift:40), (scale:1400848098, shift:40), (scale:1496737819, shift:40), (scale:1598511630, shift:40), (scale:1698568298, shift:40), (scale:1520034072, shift:40), (scale:1814055826, shift:40), (scale:1321492452, shift:40), (scale:1121405985, shift:39), (scale:1613437042, shift:40), (scale:1823953813, shift:40), (scale:1398200035, shift:40), (scale:1353593462, shift:40), (scale:1753154104, shift:40), (scale:2067169817, shift:40), (scale:2127726223, shift:40), (scale:1157950632, shift:39), (scale:1283672023, shift:39), (scale:1164345525, shift:40), (scale:1946469497, shift:40), (scale:1421623831, shift:40), (scale:1427393359, shift:40), (scale:1709820682, shift:40), (scale:1589959143, shift:40), (scale:1589352933, shift:40), (scale:1476873762, shift:40), (scale:1343103699, shift:39), (scale:1342075556, shift:40), (scale:1304102075, shift:40), (scale:1455698882, shift:40), (scale:1512388087, shift:40), (scale:1515333627, shift:40), (scale:1509201988, shift:40), (scale:1489258229, shift:40), (scale:1792345542, shift:40), (scale:1798870610, shift:40), (scale:1513305210, shift:40), (scale:1962959182, shift:40), (scale:1907626461, shift:40), (scale:1663596004, shift:40), (scale:1713360631, shift:40), (scale:1944464409, shift:40), (scale:2143002398, shift:40), (scale:1402948411, shift:40), (scale:1770538853, shift:40), (scale:1932145575, shift:40), (scale:1721390241, shift:40), (scale:1863991114, shift:40), (scale:1809805091, shift:40), (scale:1133885586, shift:39), (scale:1615446123, shift:40), (scale:1083285355, shift:39), (scale:1699417610, shift:40), (scale:1639196172, shift:40), (scale:1676003074, shift:40), (scale:1713599375, shift:40), (scale:1399571861, shift:39), (scale:1699195024, shift:40), (scale:1542957594, shift:40), (scale:1703844453, shift:40), (scale:1592731117, shift:40), (scale:1599538683, shift:40), (scale:1291162688, shift:40), (scale:1239673294, shift:40), (scale:2132722420, shift:40), (scale:1510422124, shift:40), (scale:1539325234, shift:40), (scale:1638392067, shift:40), (scale:1955665049, shift:40), (scale:1333809925, shift:40), (scale:1809024407, shift:40), (scale:1379055479, shift:40), (scale:1206854962, shift:39), (scale:1123372311, shift:40), (scale:1450700324, shift:40), (scale:1441569224, shift:40), (scale:1112143076, shift:39), (scale:1670978009, shift:40), (scale:1567234333, shift:40), (scale:1946588233, shift:40), (scale:1571127405, shift:40), (scale:1184824598, shift:39), (scale:1259994959, shift:39), (scale:1392592907, shift:40), (scale:1538660200, shift:40), (scale:1598413046, shift:40), (scale:1335240483, shift:40), (scale:2142992594, shift:40), (scale:1694345704, shift:40), (scale:1117178671, shift:39), (scale:1263254860, shift:40), (scale:1283948530, shift:39), (scale:1462053924, shift:40), (scale:1347346082, shift:40), (scale:1668062971, shift:40), (scale:1984438892, shift:40), (scale:1639263529, shift:40), (scale:1248197458, shift:40), (scale:1209421416, shift:39), (scale:1294737405, shift:40), (scale:1763208954, shift:40), (scale:1241917761, shift:39), (scale:1713489171, shift:39), (scale:1504123727, shift:40), (scale:1932676258, shift:40), (scale:1371934822, shift:40), (scale:1671562978, shift:40), (scale:1399882773, shift:40), (scale:2000728686, shift:40), (scale:1567483970, shift:40), (scale:1520838268, shift:39), (scale:1640230851, shift:40), (scale:2109398843, shift:40), (scale:1546269693, shift:40), (scale:1322115729, shift:39), (scale:1948488564, shift:40), (scale:1481844452, shift:40), (scale:1662104171, shift:40), (scale:1866154699, shift:40), (scale:1589232199, shift:40), (scale:1832095442, shift:40), (scale:1294205178, shift:40), (scale:1830204371, shift:40), (scale:1614253492, shift:40), (scale:1254021817, shift:39), (scale:1113000013, shift:39), (scale:1809628439, shift:40), (scale:1941772501, shift:40), (scale:1668711665, shift:40), (scale:1496192611, shift:40), (scale:1505330702, shift:40), (scale:1246907332, shift:40), (scale:1814657498, shift:40), (scale:1758853462, shift:40), (scale:1378510180, shift:40), (scale:2118103836, shift:40), (scale:1922113238, shift:40), (scale:1486924890, shift:40), (scale:1406271856, shift:40), (scale:1906600860, shift:40), (scale:1681900690, shift:40), (scale:1486990250, shift:40), (scale:1949819721, shift:40), (scale:1617520746, shift:40), (scale:1735290596, shift:40), (scale:1580906655, shift:40), (scale:1700245499, shift:40), (scale:1857923745, shift:40), (scale:1202954809, shift:40), (scale:1587074242, shift:40), (scale:2128589514, shift:40), (scale:1546341588, shift:40), (scale:1664845462, shift:40), (scale:1157186469, shift:39), (scale:1770344408, shift:40), (scale:1459065265, shift:40), (scale:1154693908, shift:39), (scale:1579973284, shift:40), (scale:1989724306, shift:40), (scale:1805549455, shift:40), (scale:1503503991, shift:40), (scale:1410658213, shift:40), (scale:1329604851, shift:40), (scale:1247137997, shift:40), (scale:1281197254, shift:40), (scale:1361334491, shift:40), (scale:1613625314, shift:40), (scale:1748911902, shift:40), (scale:1761791195, shift:40), (scale:1694304673, shift:40), (scale:1462562640, shift:40), (scale:1464443090, shift:40), (scale:1537860452, shift:40), (scale:1678590588, shift:40), (scale:1554661685, shift:40), (scale:1580808616, shift:40), (scale:1285848588, shift:40), (scale:1737617580, shift:40), (scale:1585223476, shift:40), (scale:1476465628, shift:40), (scale:1279256073, shift:39), (scale:1358657743, shift:40), (scale:1982452141, shift:40), (scale:1551557467, shift:40), (scale:1588716221, shift:40), (scale:1610414160, shift:40), (scale:1397603175, shift:40), (scale:1579718018, shift:40), (scale:1427792144, shift:40), (scale:1503509347, shift:40), (scale:1246862851, shift:40), (scale:1990057458, shift:40), (scale:1451799001, shift:40), (scale:1394775736, shift:40), (scale:1195049836, shift:39), (scale:2060989702, shift:40), (scale:1592350397, shift:40), (scale:1312307975, shift:40), (scale:1491115077, shift:40), (scale:1803882603, shift:40), (scale:1294154796, shift:40), (scale:1972940865, shift:40), (scale:1845582943, shift:40), (scale:1242492381, shift:39), (scale:1187684171, shift:40), (scale:1747219996, shift:40), (scale:1134246153, shift:39), (scale:1663405735, shift:40), (scale:1296748302, shift:40), (scale:1384890277, shift:39), (scale:1484440317, shift:40), (scale:1223772118, shift:40), (scale:1304164258, shift:40), (scale:1482206925, shift:40), (scale:1774530146, shift:40), (scale:1905998825, shift:40), (scale:1346923877, shift:40), (scale:1355630050, shift:39), (scale:1192940355, shift:39), (scale:1846995255, shift:40), (scale:1611099528, shift:40), (scale:2120475663, shift:40), (scale:1525951568, shift:40), (scale:1556328537, shift:40), (scale:1873164880, shift:40)], zero_point: [86], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24
69 Clamp aten_clamp_default_24
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [86], quantMax: [127], dimension: 0 aten_clamp_default_24
70 MemoryCopy aten_cat_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_24
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
71 Conv2D aten_convolution_default_25
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [86], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1946136008, shift:36), (scale:1669399953, shift:36), (scale:1889889793, shift:36), (scale:1514589674, shift:36)], zero_point: [-55], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25
72 Clamp aten_clamp_default_25
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-55], quantMax: [127], dimension: 0 aten_clamp_default_25
73 AvgPool aten_view_copy_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-55], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_25
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-55], quantMin: [], quantMax: [], dimension: 0 aten_view_copy_default

Schedule: 'graph_MAX_BUFFERED'
	0: Operation Transpose  - OFM 1, 224, 224, 3
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 0
		Operator Config = OFM Block=[16, 2, 128], IFM Block=[1, 16, 2, 128], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 3, 224, 224]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 224, 224, 3]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=150528 Cycles=2140919
		Memory Used: 953344 bytes
	1: Operation MemoryCopy  - OFM 1, 223, 224, 3
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 2
		Operator Config = OFM Block=[1, 10, 6, 16], IFM Block=[1, 10, 6, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 223, 224, 3]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 223, 224, 3]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=149856 Cycles=399751
		Memory Used: 952672 bytes
	2: Operation Conv2D  - OFM 1, 111, 111, 64
		Kernel: size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 4
		Operator Config = OFM Block=[1, 4, 16, 32], IFM Block=[1, 9, 36, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 223, 223, 3]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 111, 111, 64]
		Assigned Cascade = 0
		Encoded Weights = 3248 bytes
		Weight buffer = 3248 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=21290688 Cycles=296216
		Memory Used: 941648 bytes
	3: Operation MaxPool  - OFM 1, 55, 55, 64
		Kernel: size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 6
		Operator Config = OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 111, 111, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=1742400 Cycles=48859
		Memory Used: 982144 bytes
	4: Operation Conv2D  - OFM 1, 55, 55, 16
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 8
		Operator Config = OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 16]
		Assigned Cascade = 0
		Encoded Weights = 1168 bytes
		Weight buffer = 1168 bytes
		Depth slices = [0, 16]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=3097600 Cycles=12760
		Memory Used: 243168 bytes
	5: Operation Conv2D  - OFM 1, 55, 55, 64
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 10
		Operator Config = OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 16]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		Encoded Weights = 10368 bytes
		Weight buffer = 10368 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=27878400 Cycles=145754
		Memory Used: 252368 bytes
	6: Operation MemoryCopy  - OFM 1, 55, 55, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 12
		Operator Config = OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=193600 Cycles=97167
		Memory Used: 629200 bytes
	7: Operation Conv2D  - OFM 1, 55, 55, 64
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 14
		Operator Config = OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 16]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		Encoded Weights = 2144 bytes
		Weight buffer = 2144 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=3097600 Cycles=48912
		Memory Used: 631344 bytes
	8: Operation MemoryCopy  - OFM 1, 55, 55, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 16
		Operator Config = OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=193600 Cycles=97167
		Memory Used: 580800 bytes
	9: Operation Conv2D  - OFM 1, 55, 55, 16
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 18
		Operator Config = OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 16]
		Assigned Cascade = 0
		Encoded Weights = 2048 bytes
		Weight buffer = 2048 bytes
		Depth slices = [0, 16]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=6195200 Cycles=25080
		Memory Used: 437648 bytes
	10: Operation Conv2D  - OFM 1, 55, 55, 64
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 20
		Operator Config = OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 16]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		Encoded Weights = 10208 bytes
		Weight buffer = 10208 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=27878400 Cycles=145754
		Memory Used: 252208 bytes
	11: Operation MemoryCopy  - OFM 1, 55, 55, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 22
		Operator Config = OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=193600 Cycles=97167
		Memory Used: 629200 bytes
	12: Operation Conv2D  - OFM 1, 55, 55, 64
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 24
		Operator Config = OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 16]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		Encoded Weights = 2176 bytes
		Weight buffer = 2176 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=3097600 Cycles=48912
		Memory Used: 631376 bytes
	13: Operation MemoryCopy  - OFM 1, 55, 55, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 26
		Operator Config = OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=193600 Cycles=97167
		Memory Used: 580800 bytes
	14: Operation MaxPool  - OFM 1, 27, 27, 128
		Kernel: size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 28
		Operator Config = OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=839808 Cycles=12697
		Memory Used: 480512 bytes
	15: Operation Conv2D  - OFM 1, 27, 27, 32
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 30
		Operator Config = OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 32]
		Assigned Cascade = 0
		Encoded Weights = 4000 bytes
		Weight buffer = 4000 bytes
		Depth slices = [0, 32]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2985984 Cycles=12528
		Memory Used: 120640 bytes
	16: Operation Conv2D  - OFM 1, 27, 27, 128
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 32
		Operator Config = OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 32]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		Encoded Weights = 38624 bytes
		Weight buffer = 38624 bytes
		Depth slices = [0, 80, 128]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=26873856 Cycles=145966
		Memory Used: 155264 bytes
	17: Operation MemoryCopy  - OFM 1, 27, 27, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 34
		Operator Config = OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=93312 Cycles=46960
		Memory Used: 303264 bytes
	18: Operation Conv2D  - OFM 1, 27, 27, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 36
		Operator Config = OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 32]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		Encoded Weights = 7264 bytes
		Weight buffer = 7264 bytes
		Depth slices = [0, 128]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2985984 Cycles=48816
		Memory Used: 310528 bytes
	19: Operation MemoryCopy  - OFM 1, 27, 27, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 38
		Operator Config = OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=93312 Cycles=46960
		Memory Used: 279936 bytes
	20: Operation Conv2D  - OFM 1, 27, 27, 32
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 40
		Operator Config = OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 32]
		Assigned Cascade = 0
		Encoded Weights = 7520 bytes
		Weight buffer = 7520 bytes
		Depth slices = [0, 32]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=5971968 Cycles=24789
		Memory Used: 217472 bytes
	21: Operation Conv2D  - OFM 1, 27, 27, 128
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 42
		Operator Config = OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 32]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		Encoded Weights = 38720 bytes
		Weight buffer = 38720 bytes
		Depth slices = [0, 128]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=26873856 Cycles=145966
		Memory Used: 155360 bytes
	22: Operation MemoryCopy  - OFM 1, 27, 27, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 44
		Operator Config = OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=93312 Cycles=46960
		Memory Used: 303264 bytes
	23: Operation Conv2D  - OFM 1, 27, 27, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 46
		Operator Config = OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 32]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		Encoded Weights = 7232 bytes
		Weight buffer = 7232 bytes
		Depth slices = [0, 128]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2985984 Cycles=48816
		Memory Used: 310496 bytes
	24: Operation MemoryCopy  - OFM 1, 27, 27, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 48
		Operator Config = OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=93312 Cycles=46960
		Memory Used: 279936 bytes
	25: Operation MaxPool  - OFM 1, 13, 13, 256
		Kernel: size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 50
		Operator Config = OFM Block=[13, 14, 16], IFM Block=[1, 28, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=389376 Cycles=4377
		Memory Used: 229888 bytes
	26: Operation Conv2D  - OFM 1, 13, 13, 48
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 52
		Operator Config = OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 48]
		Assigned Cascade = 0
		Encoded Weights = 11376 bytes
		Weight buffer = 11376 bytes
		Depth slices = [0, 32, 48]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2076672 Cycles=9173
		Memory Used: 62752 bytes
	27: Operation Conv2D  - OFM 1, 13, 13, 192
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 54
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 48]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		Encoded Weights = 85584 bytes
		Weight buffer = 42848 bytes
		Depth slices = [0, 16, 64, 112, 160, 192]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=14017536 Cycles=79766
		Memory Used: 83408 bytes
	28: Operation MemoryCopy  - OFM 1, 13, 13, 384
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 56
		Operator Config = OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 192]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=32448 Cycles=16303
		Memory Used: 105456 bytes
	29: Operation Conv2D  - OFM 1, 13, 13, 192
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 58
		Operator Config = OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 48]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		Encoded Weights = 12304 bytes
		Weight buffer = 12304 bytes
		Depth slices = [0, 192]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=1557504 Cycles=8496
		Memory Used: 117760 bytes
	30: Operation MemoryCopy  - OFM 1, 13, 13, 384
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 60
		Operator Config = OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 192]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=32448 Cycles=16303
		Memory Used: 97344 bytes
	31: Operation Conv2D  - OFM 1, 13, 13, 48
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 62
		Operator Config = OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 384]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 48]
		Assigned Cascade = 0
		Encoded Weights = 16528 bytes
		Weight buffer = 16528 bytes
		Depth slices = [0, 48]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=3115008 Cycles=13709
		Memory Used: 89536 bytes
	32: Operation Conv2D  - OFM 1, 13, 13, 192
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 64
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 48]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		Encoded Weights = 86544 bytes
		Weight buffer = 64992 bytes
		Depth slices = [0, 48, 144, 192]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=14017536 Cycles=79766
		Memory Used: 105552 bytes
	33: Operation MemoryCopy  - OFM 1, 13, 13, 384
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 66
		Operator Config = OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 192]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=32448 Cycles=16303
		Memory Used: 105456 bytes
	34: Operation Conv2D  - OFM 1, 13, 13, 192
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 68
		Operator Config = OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 48]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		Encoded Weights = 12336 bytes
		Weight buffer = 12336 bytes
		Depth slices = [0, 192]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=1557504 Cycles=8496
		Memory Used: 117792 bytes
	35: Operation MemoryCopy  - OFM 1, 13, 13, 384
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 70
		Operator Config = OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 192]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=32448 Cycles=16303
		Memory Used: 97344 bytes
	36: Operation Conv2D  - OFM 1, 13, 13, 64
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 72
		Operator Config = OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 384]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 64]
		Assigned Cascade = 0
		Encoded Weights = 22272 bytes
		Weight buffer = 22272 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=4153344 Cycles=18288
		Memory Used: 97984 bytes
	37: Operation Conv2D  - OFM 1, 13, 13, 256
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 74
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		Encoded Weights = 153088 bytes
		Weight buffer = 114816 bytes
		Depth slices = [0, 48, 144, 240, 256]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=24920064 Cycles=141254
		Memory Used: 168896 bytes
	38: Operation MemoryCopy  - OFM 1, 13, 13, 512
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 76
		Operator Config = OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=43264 Cycles=21808
		Memory Used: 140608 bytes
	39: Operation Conv2D  - OFM 1, 13, 13, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 78
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		Encoded Weights = 18464 bytes
		Weight buffer = 18464 bytes
		Depth slices = [0, 256]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2768896 Cycles=12064
		Memory Used: 159072 bytes
	40: Operation MemoryCopy  - OFM 1, 13, 13, 512
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 80
		Operator Config = OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=43264 Cycles=21808
		Memory Used: 129792 bytes
	41: Operation Conv2D  - OFM 1, 13, 13, 64
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 82
		Operator Config = OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 512]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 64]
		Assigned Cascade = 0
		Encoded Weights = 29728 bytes
		Weight buffer = 29728 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=5537792 Cycles=24336
		Memory Used: 127072 bytes
	42: Operation Conv2D  - OFM 1, 13, 13, 256
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 84
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		Encoded Weights = 156688 bytes
		Weight buffer = 117616 bytes
		Depth slices = [0, 64, 192, 256]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=24920064 Cycles=141254
		Memory Used: 171696 bytes
	43: Operation MemoryCopy  - OFM 1, 13, 13, 512
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 86
		Operator Config = OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=43264 Cycles=21808
		Memory Used: 140608 bytes
	44: Operation Conv2D  - OFM 1, 13, 13, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 88
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		Encoded Weights = 18800 bytes
		Weight buffer = 18800 bytes
		Depth slices = [0, 256]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2768896 Cycles=12064
		Memory Used: 159408 bytes
	45: Operation MemoryCopy  - OFM 1, 13, 13, 512
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 90
		Operator Config = OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=43264 Cycles=21808
		Memory Used: 129792 bytes
	46: Operation Conv2D  - OFM 1, 13, 13, 4
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 92
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 512]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 4]
		Assigned Cascade = 0
		Encoded Weights = 2576 bytes
		Weight buffer = 2576 bytes
		Depth slices = [0, 4]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=346112 Cycles=3442
		Memory Used: 91808 bytes
	47: Operation AvgPool  - OFM 1, 1, 1, 4
		Kernel: size=13,13 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 94
		Operator Config = OFM Block=[1, 2, 16], IFM Block=[1, 8, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 4]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 4]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=676 Cycles=858
		Memory Used: 2720 bytes
	Cascades:
################################################################################
Allocation, memory Sram, usage mask: FeatureMap|Staging
Start Time - End Time  : Start Addr -   End Addr: Tensor Size: Memory Usage : Name
         0 -          1:    0xc4000 -    0xe8c00:      150528:       953344 : quantized_decomposed_quantize_per_tensor_default
         0 -          3:        0x0 -    0xc4000:      802816:       955920 : tosa_transpose_default
         2 -          5:    0xc4000 -    0xe8960:      149856:       955920 : aten_slice_copy_tensor
         3 -          5:    0xe8960 -    0xe9610:        3248:       955920 : const_values
         4 -          7:        0x0 -    0xc0840:      788544:       983312 : aten_clamp_default
         6 -          9:    0xc0840 -    0xefc80:      193600:       983312 : aten_max_pool2d_default
         7 -          9:    0xefc80 -    0xf0110:        1168:       983312 : const_values
         8 -         15:        0x0 -     0xbd10:       48400:       631344 : aten_clamp_default_1
         9 -         11:     0xbd10 -     0xe590:       10368:       253536 : const_values
        10 -         13:    0x6a590 -    0x999d0:      193600:       631344 : aten_clamp_default_2
        12 -         19:     0xbd10 -    0x6a590:      387200:       631344 : aten_cat_default
        13 -         15:    0x999d0 -    0x9a230:        2144:       631344 : const_values
        14 -         17:    0x6a590 -    0x999d0:      193600:       631344 : aten_clamp_default_3
        17 -         19:    0x999d0 -    0x9a1d0:        2048:       582848 : const_values
        18 -         25:        0x0 -     0xbd10:       48400:       631376 : aten_clamp_default_4
        19 -         21:    0x9a1d0 -    0x9c9b0:       10208:       447856 : const_values
        20 -         23:    0x6a590 -    0x999d0:      193600:       631376 : aten_clamp_default_5
        22 -         29:     0xbd10 -    0x6a590:      387200:       631376 : aten_cat_default_1
        23 -         25:    0x999d0 -    0x9a250:        2176:       631376 : const_values
        24 -         27:    0x6a590 -    0x999d0:      193600:       631376 : aten_clamp_default_6
        28 -         31:    0x6a590 -    0x81210:       93312:       484512 : aten_max_pool2d_default_1
        29 -         31:        0x0 -      0xfa0:        4000:       484512 : const_values
        30 -         37:      0xfa0 -     0x6ac0:       23328:       310528 : aten_clamp_default_7
        31 -         33:     0x6ac0 -    0x101a0:       38624:       159264 : const_values
        32 -         35:    0x343c0 -    0x4b040:       93312:       310528 : aten_clamp_default_8
        34 -         41:     0x6ac0 -    0x343c0:      186624:       310528 : aten_cat_default_2
        35 -         37:    0x4b040 -    0x4cca0:        7264:       310528 : const_values
        36 -         39:    0x343c0 -    0x4b040:       93312:       310528 : aten_clamp_default_9
        39 -         41:    0x4b040 -    0x4cda0:        7520:       287456 : const_values
        40 -         47:        0x0 -     0x5b20:       23328:       310496 : aten_clamp_default_10
        41 -         43:    0x4cda0 -    0x564e0:       38720:       256192 : const_values
        42 -         45:    0x33420 -    0x4a0a0:       93312:       310496 : aten_clamp_default_11
        44 -         51:     0x5b20 -    0x33420:      186624:       310496 : aten_cat_default_3
        45 -         47:    0x4a0a0 -    0x4bce0:        7232:       310496 : const_values
        46 -         49:    0x33420 -    0x4a0a0:       93312:       310496 : aten_clamp_default_12
        50 -         53:    0x33420 -    0x3dd20:       43264:       241264 : aten_max_pool2d_default_2
        51 -         53:        0x0 -     0x2c70:       11376:       241264 : const_values
        52 -         59:     0xfd80 -    0x11d30:        8112:       117760 : aten_clamp_default_13
        53 -         55:     0x2c70 -     0xd3d0:       42848:       105600 : const_values
        54 -         57:    0x11d30 -    0x19bf0:       32448:       117760 : aten_clamp_default_14
        56 -         63:        0x0 -     0xfd80:       64896:       154528 : aten_cat_default_4
        57 -         59:    0x1dc80 -    0x20c90:       12304:       117760 : const_values
        58 -         61:    0x15dc0 -    0x1dc80:       32448:       117760 : aten_clamp_default_15
        61 -         63:    0x11d30 -    0x15dc0:       16528:       154528 : const_values
        62 -         69:     0xfd80 -    0x11d30:        8112:       154528 : aten_clamp_default_16
        63 -         65:    0x15dc0 -    0x25ba0:       64992:       154528 : const_values
        64 -         67:    0x25ba0 -    0x2da60:       32448:       117792 : aten_clamp_default_17
        66 -         73:        0x0 -     0xfd80:       64896:       212800 : aten_cat_default_5
        67 -         69:    0x11d30 -    0x14d60:       12336:       117792 : const_values
        68 -         71:    0x1d340 -    0x25200:       32448:       119616 : aten_clamp_default_18
        71 -         73:    0x17c40 -    0x1d340:       22272:       212800 : const_values
        72 -         79:    0x15200 -    0x17c40:       10816:       212800 : aten_clamp_default_19
        73 -         75:    0x1d340 -    0x393c0:      114816:       212800 : const_values
        74 -         77:    0x393c0 -    0x43cc0:       43264:       168896 : aten_clamp_default_20
        76 -         83:        0x0 -    0x15200:       86528:       244688 : aten_cat_default_6
        77 -         79:    0x17c40 -    0x1c460:       18464:       159072 : const_values
        78 -         81:    0x1f060 -    0x29960:       43264:       159520 : aten_clamp_default_21
        81 -         83:    0x17c40 -    0x1f060:       29728:       244688 : const_values
        82 -         89:    0x15200 -    0x17c40:       10816:       244688 : aten_clamp_default_22
        83 -         85:    0x1f060 -    0x3bbd0:      117616:       244688 : const_values
        84 -         87:        0x0 -     0xa900:       43264:       171696 : aten_clamp_default_23
        86 -         93:    0x17c40 -    0x2ce40:       86528:       159408 : aten_cat_default_7
        87 -         89:     0xa900 -     0xf270:       18800:       159408 : const_values
        88 -         91:        0x0 -     0xa900:       43264:       159408 : aten_clamp_default_24
        91 -         93:     0xa900 -     0xb310:        2576:       132368 : const_values
        92 -         95:        0x0 -      0xa90:        2704:        91808 : aten_clamp_default_25
        94 -         97:      0xa90 -      0xaa0:          16:         2720 : aten_view_copy_default
Allocation Peak Tensor Size: 983312 bytes == 960.265625 KiB
################################################################################
Tensor Allocation for read-only NPU tensors:
Start Time - End Time  : Start Addr -   End Addr: Tensor Size: Memory Usage : Name
         4 -          5:        0x0 -      0xcb0:        3248:         3248 : const_values
         8 -          9:      0xcb0 -     0x1140:        1168:         1168 : const_values
        10 -         11:     0x1140 -     0x39c0:       10368:        10368 : const_values
        14 -         15:     0x39c0 -     0x4220:        2144:         2144 : const_values
        18 -         19:     0x4220 -     0x4a20:        2048:         2048 : const_values
        20 -         21:     0x4a20 -     0x7200:       10208:        10208 : const_values
        24 -         25:     0x7200 -     0x7a80:        2176:         2176 : const_values
        30 -         31:     0x7a80 -     0x8a20:        4000:         4000 : const_values
        32 -         33:     0x8a20 -    0x12100:       38624:        38624 : const_values
        36 -         37:    0x12100 -    0x13d60:        7264:         7264 : const_values
        40 -         41:    0x13d60 -    0x15ac0:        7520:         7520 : const_values
        42 -         43:    0x15ac0 -    0x1f200:       38720:        38720 : const_values
        46 -         47:    0x1f200 -    0x20e40:        7232:         7232 : const_values
        52 -         53:    0x20e40 -    0x23ab0:       11376:        11376 : const_values
        54 -         55:    0x23ab0 -    0x38900:       85584:        85584 : const_values
        58 -         59:    0x38900 -    0x3b910:       12304:        12304 : const_values
        62 -         63:    0x3b910 -    0x3f9a0:       16528:        16528 : const_values
        64 -         65:    0x3f9a0 -    0x54bb0:       86544:        86544 : const_values
        68 -         69:    0x54bb0 -    0x57be0:       12336:        12336 : const_values
        72 -         73:    0x57be0 -    0x5d2e0:       22272:        22272 : const_values
        74 -         75:    0x5d2e0 -    0x828e0:      153088:       153088 : const_values
        78 -         79:    0x828e0 -    0x87100:       18464:        18464 : const_values
        82 -         83:    0x87100 -    0x8e520:       29728:        29728 : const_values
        84 -         85:    0x8e520 -    0xb4930:      156688:       156688 : const_values
        88 -         89:    0xb4930 -    0xb92a0:       18800:        18800 : const_values
        92 -         93:    0xb92a0 -    0xb9cb0:        2576:         2576 : const_values
Allocation Peak Tensor Size: 761008 bytes == 743.171875 KiB
High level NPU operations:
0 Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[16, 2, 128], IFM Block=[1, 16, 2, 128], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: quantized_decomposed_quantize_per_tensor_default, [1, 3, 224, 224], format: 1, Sram:FeatureMap|Staging, address: 0xc4000
  OFM: tosa_transpose_default, [1, 224, 224, 3], format: 2, Sram:FeatureMap|Staging, address: 0x0
1 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 10, 6, 16], IFM Block=[1, 10, 6, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: tosa_transpose_default, [1, 224, 224, 3], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_slice_copy_tensor, [1, 223, 224, 3], format: 1, Sram:FeatureMap|Staging, address: 0xc4000
2 Conv2D , subOps: Clamp, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 16, 32], IFM Block=[1, 9, 36, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_slice_copy_tensor, [1, 223, 224, 3], format: 1, Sram:FeatureMap|Staging, address: 0xc4000
  OFM: aten_clamp_default, [1, 111, 111, 64], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xe8960, format: Default
3 MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default, [1, 111, 111, 64], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_max_pool2d_default, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0xc0840
4 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_max_pool2d_default, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0xc0840
  OFM: aten_clamp_default_1, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xefc80, format: Default
5 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_1, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_2, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xbd10, format: Default
6 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_2, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_cat_default, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
7 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_1, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_3, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x999d0, format: Default
8 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_3, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_cat_default, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
9 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
  OFM: aten_clamp_default_4, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x999d0, format: Default
10 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_4, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_5, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x9a1d0, format: Default
11 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_5, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_cat_default_1, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
12 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_4, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_6, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x999d0, format: Default
13 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_6, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_cat_default_1, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
14 MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_1, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
  OFM: aten_max_pool2d_default_1, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
15 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_max_pool2d_default_1, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_clamp_default_7, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0xfa0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x0, format: Default
16 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_7, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0xfa0
  OFM: aten_clamp_default_8, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x343c0
  Weights: const_values, 2 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x6ac0, format: Default
17 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_8, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x343c0
  OFM: aten_cat_default_2, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x6ac0
18 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_7, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0xfa0
  OFM: aten_clamp_default_9, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x343c0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x4b040, format: Default
19 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_9, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x343c0
  OFM: aten_cat_default_2, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x6ac0
20 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_2, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x6ac0
  OFM: aten_clamp_default_10, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x4b040, format: Default
21 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_10, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_11, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x4cda0, format: Default
22 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_11, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  OFM: aten_cat_default_3, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x5b20
23 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_10, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_12, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x4a0a0, format: Default
24 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_12, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  OFM: aten_cat_default_3, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x5b20
25 MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[13, 14, 16], IFM Block=[1, 28, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_3, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x5b20
  OFM: aten_max_pool2d_default_2, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x33420
26 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_max_pool2d_default_2, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  OFM: aten_clamp_default_13, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  Weights: const_values, 2 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x0, format: Default
27 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_13, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  OFM: aten_clamp_default_14, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x11d30
  Weights: const_values, 5 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x2c70, format: Default
28 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_14, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x11d30
  OFM: aten_cat_default_4, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
29 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_13, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  OFM: aten_clamp_default_15, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x15dc0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x1dc80, format: Default
30 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_15, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x15dc0
  OFM: aten_cat_default_4, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
31 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_4, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_16, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x11d30, format: Default
32 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_16, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  OFM: aten_clamp_default_17, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x25ba0
  Weights: const_values, 3 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x15dc0, format: Default
33 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_17, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x25ba0
  OFM: aten_cat_default_5, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
34 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_16, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  OFM: aten_clamp_default_18, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x1d340
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x11d30, format: Default
35 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_18, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x1d340
  OFM: aten_cat_default_5, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
36 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_5, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_19, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x17c40, format: Default
37 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_19, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  OFM: aten_clamp_default_20, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x393c0
  Weights: const_values, 4 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x1d340, format: Default
38 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_20, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x393c0
  OFM: aten_cat_default_6, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x0
39 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_19, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  OFM: aten_clamp_default_21, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x1f060
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x17c40, format: Default
40 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_21, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x1f060
  OFM: aten_cat_default_6, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x0
41 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_6, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_22, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x17c40, format: Default
42 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_22, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  OFM: aten_clamp_default_23, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 3 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x1f060, format: Default
43 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_23, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_cat_default_7, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x17c40
44 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_22, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  OFM: aten_clamp_default_24, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xa900, format: Default
45 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_24, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_cat_default_7, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x17c40
46 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_7, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x17c40
  OFM: aten_clamp_default_25, [1, 13, 13, 4], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xa900, format: Default
47 AvgPool , subOps: -, size=13,13 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 16], IFM Block=[1, 8, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_25, [1, 13, 13, 4], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_view_copy_default, [1, 1, 1, 4], format: 1, Sram:FeatureMap|Staging, address: 0xa90
High level command stream:
0 Transpose OFM area [0, 0, 0, 0 - 1, 224, 224, 3], IFM [0, 0, 0, 0 - 1, 3, 224, 224]
1 MemoryCopy OFM area [0, 0, 0, 0 - 1, 223, 224, 3], IFM [0, 0, 0, 0 - 1, 223, 224, 3]
2 DMA src: OffChipFlash:ReadOnly, address: 0x0, dest: Sram:FeatureMap|Staging, address: 0xe8960, sizes: (N/A), length: 3248
3 Conv2D OFM area [0, 0, 0, 0 - 1, 111, 111, 64], IFM [0, 0, 0, 0 - 1, 223, 224, 3], Weight depth: 0
4 MaxPool OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 111, 111, 64]
5 DMA src: OffChipFlash:ReadOnly, address: 0xcb0, dest: Sram:FeatureMap|Staging, address: 0xefc80, sizes: (N/A), length: 1168
6 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 16], IFM [0, 0, 0, 0 - 1, 55, 55, 64], Weight depth: 0
7 DMA src: OffChipFlash:ReadOnly, address: 0x1140, dest: Sram:FeatureMap|Staging, address: 0xbd10, sizes: (N/A), length: 10368
8 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 16], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1]
9 MemoryCopy OFM area [0, 0, 0, 64 - 1, 55, 55, 128], IFM [0, 0, 0, 0 - 1, 55, 55, 64], buffered
10 DMA src: OffChipFlash:ReadOnly, address: 0x39c0, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2144
11 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 16], Weight depth: 0
12 MemoryCopy OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 64], buffered
13 DMA src: OffChipFlash:ReadOnly, address: 0x4220, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2048
14 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 16], IFM [0, 0, 0, 0 - 1, 55, 55, 128], Weight depth: 0
15 DMA src: OffChipFlash:ReadOnly, address: 0x4a20, dest: Sram:FeatureMap|Staging, address: 0x9a1d0, sizes: (N/A), length: 10208
16 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 16], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1]
17 MemoryCopy OFM area [0, 0, 0, 64 - 1, 55, 55, 128], IFM [0, 0, 0, 0 - 1, 55, 55, 64], buffered
18 DMA src: OffChipFlash:ReadOnly, address: 0x7200, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2176
19 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 16], Weight depth: 0
20 MemoryCopy OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 64], buffered
21 MaxPool OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 55, 55, 128]
22 DMA src: OffChipFlash:ReadOnly, address: 0x7a80, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 4000
23 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 32], IFM [0, 0, 0, 0 - 1, 27, 27, 128], Weight depth: 0
24 DMA src: OffChipFlash:ReadOnly, address: 0x8a20, dest: Sram:FeatureMap|Staging, address: 0x6ac0, sizes: (N/A), length: 23952
25 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 80], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
26 DMA src: OffChipFlash:ReadOnly, address: 0xe7b0, dest: Sram:FeatureMap|Staging, address: 0xc850, sizes: (N/A), length: 14672
27 Conv2D OFM area [0, 0, 0, 80 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 80, padding: [top:1,left:1,bottom:1,right:1], buffered
28 MemoryCopy OFM area [0, 0, 0, 128 - 1, 27, 27, 256], IFM [0, 0, 0, 0 - 1, 27, 27, 128], buffered
29 DMA src: OffChipFlash:ReadOnly, address: 0x12100, dest: Sram:FeatureMap|Staging, address: 0x4b040, sizes: (N/A), length: 7264
30 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 0
31 MemoryCopy OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 128], buffered
32 DMA src: OffChipFlash:ReadOnly, address: 0x13d60, dest: Sram:FeatureMap|Staging, address: 0x4b040, sizes: (N/A), length: 7520
33 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 32], IFM [0, 0, 0, 0 - 1, 27, 27, 256], Weight depth: 0
34 DMA src: OffChipFlash:ReadOnly, address: 0x15ac0, dest: Sram:FeatureMap|Staging, address: 0x4cda0, sizes: (N/A), length: 38720
35 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1]
36 MemoryCopy OFM area [0, 0, 0, 128 - 1, 27, 27, 256], IFM [0, 0, 0, 0 - 1, 27, 27, 128], buffered
37 DMA src: OffChipFlash:ReadOnly, address: 0x1f200, dest: Sram:FeatureMap|Staging, address: 0x4a0a0, sizes: (N/A), length: 7232
38 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 0
39 MemoryCopy OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 128], buffered
40 MaxPool OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 27, 27, 256]
41 DMA src: OffChipFlash:ReadOnly, address: 0x20e40, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 7568
42 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 32], IFM [0, 0, 0, 0 - 1, 13, 13, 256], Weight depth: 0, buffered
43 DMA src: OffChipFlash:ReadOnly, address: 0x22bd0, dest: Sram:FeatureMap|Staging, address: 0x1d90, sizes: (N/A), length: 3808
44 Conv2D OFM area [0, 0, 0, 32 - 1, 13, 13, 48], IFM [0, 0, 0, 0 - 1, 13, 13, 256], Weight depth: 32, buffered
45 DMA src: OffChipFlash:ReadOnly, address: 0x23ab0, dest: Sram:FeatureMap|Staging, address: 0x2c70, sizes: (N/A), length: 7200
46 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 16], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
47 DMA src: OffChipFlash:ReadOnly, address: 0x256d0, dest: Sram:FeatureMap|Staging, address: 0x8090, sizes: (N/A), length: 21216
48 Conv2D OFM area [0, 0, 0, 16 - 1, 13, 13, 64], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 16, padding: [top:1,left:1,bottom:1,right:1], buffered
49 DMA src: OffChipFlash:ReadOnly, address: 0x2a9b0, dest: Sram:FeatureMap|Staging, address: 0x2c70, sizes: (N/A), length: 21536
50 Conv2D OFM area [0, 0, 0, 64 - 1, 13, 13, 112], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 64, padding: [top:1,left:1,bottom:1,right:1], buffered
51 DMA src: OffChipFlash:ReadOnly, address: 0x2fdd0, dest: Sram:FeatureMap|Staging, address: 0x8090, sizes: (N/A), length: 21312
52 Conv2D OFM area [0, 0, 0, 112 - 1, 13, 13, 160], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 112, padding: [top:1,left:1,bottom:1,right:1], buffered
53 DMA src: OffChipFlash:ReadOnly, address: 0x35110, dest: Sram:FeatureMap|Staging, address: 0x2c70, sizes: (N/A), length: 14320
54 Conv2D OFM area [0, 0, 0, 160 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 160, padding: [top:1,left:1,bottom:1,right:1], buffered
55 MemoryCopy OFM area [0, 0, 0, 192 - 1, 13, 13, 384], IFM [0, 0, 0, 0 - 1, 13, 13, 192], buffered
56 DMA src: OffChipFlash:ReadOnly, address: 0x38900, dest: Sram:FeatureMap|Staging, address: 0x1dc80, sizes: (N/A), length: 12304
57 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 0
58 MemoryCopy OFM area [0, 0, 0, 0 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 192], buffered
59 DMA src: OffChipFlash:ReadOnly, address: 0x3b910, dest: Sram:FeatureMap|Staging, address: 0x11d30, sizes: (N/A), length: 16528
60 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 48], IFM [0, 0, 0, 0 - 1, 13, 13, 384], Weight depth: 0
61 DMA src: OffChipFlash:ReadOnly, address: 0x3f9a0, dest: Sram:FeatureMap|Staging, address: 0x15dc0, sizes: (N/A), length: 21744
62 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 48], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
63 DMA src: OffChipFlash:ReadOnly, address: 0x44e90, dest: Sram:FeatureMap|Staging, address: 0x1b2b0, sizes: (N/A), length: 43248
64 Conv2D OFM area [0, 0, 0, 48 - 1, 13, 13, 144], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 48, padding: [top:1,left:1,bottom:1,right:1], buffered
65 DMA src: OffChipFlash:ReadOnly, address: 0x4f780, dest: Sram:FeatureMap|Staging, address: 0x15dc0, sizes: (N/A), length: 21552
66 Conv2D OFM area [0, 0, 0, 144 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 144, padding: [top:1,left:1,bottom:1,right:1], buffered
67 MemoryCopy OFM area [0, 0, 0, 192 - 1, 13, 13, 384], IFM [0, 0, 0, 0 - 1, 13, 13, 192], buffered
68 DMA src: OffChipFlash:ReadOnly, address: 0x54bb0, dest: Sram:FeatureMap|Staging, address: 0x11d30, sizes: (N/A), length: 12336
69 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 0
70 MemoryCopy OFM area [0, 0, 0, 0 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 192], buffered
71 DMA src: OffChipFlash:ReadOnly, address: 0x57be0, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 22272
72 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 64], IFM [0, 0, 0, 0 - 1, 13, 13, 384], Weight depth: 0
73 DMA src: OffChipFlash:ReadOnly, address: 0x5d2e0, dest: Sram:FeatureMap|Staging, address: 0x1d340, sizes: (N/A), length: 28560
74 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 48], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
75 DMA src: OffChipFlash:ReadOnly, address: 0x64270, dest: Sram:FeatureMap|Staging, address: 0x2b470, sizes: (N/A), length: 57168
76 Conv2D OFM area [0, 0, 0, 48 - 1, 13, 13, 144], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 48, padding: [top:1,left:1,bottom:1,right:1], buffered
77 DMA src: OffChipFlash:ReadOnly, address: 0x721c0, dest: Sram:FeatureMap|Staging, address: 0x1d340, sizes: (N/A), length: 57648
78 Conv2D OFM area [0, 0, 0, 144 - 1, 13, 13, 240], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 144, padding: [top:1,left:1,bottom:1,right:1], buffered
79 DMA src: OffChipFlash:ReadOnly, address: 0x802f0, dest: Sram:FeatureMap|Staging, address: 0x2b470, sizes: (N/A), length: 9712
80 Conv2D OFM area [0, 0, 0, 240 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 240, padding: [top:1,left:1,bottom:1,right:1], buffered
81 MemoryCopy OFM area [0, 0, 0, 256 - 1, 13, 13, 512], IFM [0, 0, 0, 0 - 1, 13, 13, 256], buffered
82 DMA src: OffChipFlash:ReadOnly, address: 0x828e0, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 18464
83 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 0
84 MemoryCopy OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 256], buffered
85 DMA src: OffChipFlash:ReadOnly, address: 0x87100, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 29728
86 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 64], IFM [0, 0, 0, 0 - 1, 13, 13, 512], Weight depth: 0
87 DMA src: OffChipFlash:ReadOnly, address: 0x8e520, dest: Sram:FeatureMap|Staging, address: 0x1f060, sizes: (N/A), length: 39072
88 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 64], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
89 DMA src: OffChipFlash:ReadOnly, address: 0x97dc0, dest: Sram:FeatureMap|Staging, address: 0x28910, sizes: (N/A), length: 78528
90 Conv2D OFM area [0, 0, 0, 64 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 64, padding: [top:1,left:1,bottom:1,right:1], buffered
91 DMA src: OffChipFlash:ReadOnly, address: 0xab080, dest: Sram:FeatureMap|Staging, address: 0x1f060, sizes: (N/A), length: 39088
92 Conv2D OFM area [0, 0, 0, 192 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 192, padding: [top:1,left:1,bottom:1,right:1], buffered
93 MemoryCopy OFM area [0, 0, 0, 256 - 1, 13, 13, 512], IFM [0, 0, 0, 0 - 1, 13, 13, 256], buffered
94 DMA src: OffChipFlash:ReadOnly, address: 0xb4930, dest: Sram:FeatureMap|Staging, address: 0xa900, sizes: (N/A), length: 18800
95 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 0
96 MemoryCopy OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 256], buffered
97 DMA src: OffChipFlash:ReadOnly, address: 0xb92a0, dest: Sram:FeatureMap|Staging, address: 0xa900, sizes: (N/A), length: 2576
98 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 4], IFM [0, 0, 0, 0 - 1, 13, 13, 512], Weight depth: 0
99 AvgPool OFM area [0, 0, 0, 0 - 1, 1, 1, 4], IFM [0, 0, 0, 0 - 1, 13, 13, 4]
Register command stream: 1866 words
  Offset: Payload Param Code - Command                        Param, Fields
// Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[16, 2, 128], IFM Block=[1, 16, 2, 128], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000000:          0001 0105 - NPU_SET_IFM_PRECISION              1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000004:          0001 010f - NPU_SET_IFM_REGION                 1, region = 1
0x000008: 000c4000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xc4000
0x000010: 00000000 0000 4001 - NPU_SET_IFM_BASE1                  0, addr = 0x0
0x000018: 00000000 0000 4002 - NPU_SET_IFM_BASE2                  0, addr = 0x0
0x000020: 00000000 0000 4003 - NPU_SET_IFM_BASE3                  0, addr = 0x0
0x000028:          0002 010b - NPU_SET_IFM_HEIGHT0_M1             2, height_m1 = 2
0x00002c:          0002 010c - NPU_SET_IFM_HEIGHT1_M1             2, height_m1 = 2
0x000030:          00df 010a - NPU_SET_IFM_WIDTH0_M1            223, width_m1 = 223
0x000034:          00df 0104 - NPU_SET_IFM_DEPTH_M1             223, depth_m1 = 223
0x000038: 0000c400 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xc400
0x000040: 000000e0 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0xe0
0x000048: 00000001 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1
0x000050:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000054:          0000 0107 - NPU_SET_IFM_UPSCALE                0, mode = IFM_UPSCALE_MODE_NONE
0x000058:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x00005c:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x000060:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x000064:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000068:          1941 0114 - NPU_SET_OFM_PRECISION           6465, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_WCH, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00006c:          0001 011f - NPU_SET_OFM_REGION                 1, region = 1
0x000070: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000078: 00000000 0000 4011 - NPU_SET_OFM_BASE1                  0, addr = 0x0
0x000080: 00000000 0000 4012 - NPU_SET_OFM_BASE2                  0, addr = 0x0
0x000088: 00000000 0000 4013 - NPU_SET_OFM_BASE3                  0, addr = 0x0
0x000090:          0002 0112 - NPU_SET_OFM_HEIGHT_M1              2, height_m1 = 2
0x000094:          00df 0111 - NPU_SET_OFM_WIDTH_M1             223, width_m1 = 223
0x000098:          00df 0113 - NPU_SET_OFM_DEPTH_M1             223, depth_m1 = 223
0x00009c:          00df 011b - NPU_SET_OFM_HEIGHT0_M1           223, height_m1 = 223
0x0000a0:          00df 011c - NPU_SET_OFM_HEIGHT1_M1           223, height_m1 = 223
0x0000a4:          00df 011a - NPU_SET_OFM_WIDTH0_M1            223, width_m1 = 223
0x0000a8: 00000e00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xe00
0x0000b0: 00000010 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x10
0x0000b8: 00000e00 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0xe00
0x0000c0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0000c4:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x0000c8:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x0000cc:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0000d0:          0000 0125 - NPU_SET_ACTIVATION                 0, activation_function = ACTIVATION_FUNCTION_LUT_NONE, table = 0, activation_clip_range = ACTIVATION_CLIP_RANGE_B16
0x0000d4:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0000d8:          007f 0127 - NPU_SET_ACTIVATION_MAX           127, clip_boundary = 127
0x0000dc: 00000001 0000 4024 - NPU_SET_OFM_SCALE                  0, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_DOUBLE_SYMMETRIC, scale = 1
0x0000e4:          000f 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         15, height_m1 = 15
0x0000e8:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x0000ec:          007f 0117 - NPU_SET_OFM_BLK_DEPTH_M1         127, depth_m1 = 127
0x0000f0:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x0000f4:          0000 012f - NPU_SET_BLOCKDEP                   0, blockdep = 0
0x0000f8:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 10, 6, 16], IFM Block=[1, 10, 6, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0000fc:          0041 0105 - NPU_SET_IFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000100: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000108:          00de 010b - NPU_SET_IFM_HEIGHT0_M1           222, height_m1 = 222
0x00010c:          00de 010c - NPU_SET_IFM_HEIGHT1_M1           222, height_m1 = 222
0x000110:          0002 0104 - NPU_SET_IFM_DEPTH_M1               2, depth_m1 = 2
0x000114: 00000e00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xe00
0x00011c: 00000010 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x10
0x000124: 00000e00 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0xe00
0x00012c:          0101 0114 - NPU_SET_OFM_PRECISION            257, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000130: 000c4000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xc4000
0x000138:          00de 0112 - NPU_SET_OFM_HEIGHT_M1            222, height_m1 = 222
0x00013c:          0002 0113 - NPU_SET_OFM_DEPTH_M1               2, depth_m1 = 2
0x000140:          00de 011b - NPU_SET_OFM_HEIGHT0_M1           222, height_m1 = 222
0x000144:          00de 011c - NPU_SET_OFM_HEIGHT1_M1           222, height_m1 = 222
0x000148: 000002a0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x2a0
0x000150: 00000003 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x3
0x000158: 00000001 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x1
0x000160: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000168:          0009 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          9, height_m1 = 9
0x00016c:          0005 0115 - NPU_SET_OFM_BLK_WIDTH_M1           5, width_m1 = 5
0x000170:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000174:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x0, dest: Sram:FeatureMap|Staging, address: 0xe8960, sizes: (N/A), length: 3248
0x000178:          0000 0130 - NPU_SET_DMA0_SRC_REGION            0, region = 0, region_mode = DMA_REGION_MODE_EXTERNAL, stride_mode = DMA_STRIDE_MODE_D1, idx_mode = DMA_IDX_MODE_DISABLED
0x00017c: 00000000 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x0
0x000184:          0001 0131 - NPU_SET_DMA0_DST_REGION            1, region = 1, region_mode = DMA_REGION_MODE_EXTERNAL, idx_mode = DMA_IDX_MODE_DISABLED
0x000188: 000e8960 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xe8960
0x000190: 00000cb0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xcb0
0x000198:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x00019c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 16, 32], IFM Block=[1, 9, 36, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x0001a0: 754ca65c 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1967957596
0x0001a8:          0001 0105 - NPU_SET_IFM_PRECISION              1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0001ac: 000c4000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xc4000
0x0001b4: 000002a0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x2a0
0x0001bc: 00000003 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x3
0x0001c4: 00000001 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1
0x0001cc:          0008 0109 - NPU_SET_IFM_ZERO_POINT             8, zero_point = 8
0x0001d0:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0001d4: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x0001dc:          006e 0112 - NPU_SET_OFM_HEIGHT_M1            110, height_m1 = 110
0x0001e0:          006e 0111 - NPU_SET_OFM_WIDTH_M1             110, width_m1 = 110
0x0001e4:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x0001e8:          006e 011b - NPU_SET_OFM_HEIGHT0_M1           110, height_m1 = 110
0x0001ec:          006e 011c - NPU_SET_OFM_HEIGHT1_M1           110, height_m1 = 110
0x0001f0:          006e 011a - NPU_SET_OFM_WIDTH0_M1            110, width_m1 = 110
0x0001f4: 00001bc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1bc0
0x0001fc: 00000010 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x10
0x000204: 000006f0 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x6f0
0x00020c:          fff5 0118 - NPU_SET_OFM_ZERO_POINT         65525, zero_point = 65525
0x000210:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000214:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000218:          0007 0122 - NPU_SET_KERNEL_STRIDE              7, stride_x_lsb = 1, stride_y_lsb = 1, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x00021c:          0000 012e - NPU_SET_WEIGHT_FORMAT              0, weight_format = WEIGHT_FORMAT_SWD, weight_sparsity = WEIGHT_SPARSITY_NONE
0x000220:          0001 0128 - NPU_SET_WEIGHT_REGION              1, region = 1
0x000224: 000e8be0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xe8be0
0x00022c: 00000a30 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 2608
0x000234:          0001 0129 - NPU_SET_SCALE_REGION               1, region = 1
0x000238: 000e8960 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xe8960
0x000240: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x000248:          fff5 0126 - NPU_SET_ACTIVATION_MIN         65525, clip_boundary = 65525
0x00024c:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x000250:          000f 0115 - NPU_SET_OFM_BLK_WIDTH_M1          15, width_m1 = 15
0x000254:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000258:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x00025c:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x000260:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000264:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000268:          0041 0105 - NPU_SET_IFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00026c: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000274:          006e 010b - NPU_SET_IFM_HEIGHT0_M1           110, height_m1 = 110
0x000278:          006e 010c - NPU_SET_IFM_HEIGHT1_M1           110, height_m1 = 110
0x00027c:          006e 010a - NPU_SET_IFM_WIDTH0_M1            110, width_m1 = 110
0x000280:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x000284: 00001bc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1bc0
0x00028c: 00000010 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x10
0x000294: 000006f0 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x6f0
0x00029c:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0002a0:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0002a4: 000c0840 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xc0840
0x0002ac:          0036 0112 - NPU_SET_OFM_HEIGHT_M1             54, height_m1 = 54
0x0002b0:          0036 0111 - NPU_SET_OFM_WIDTH_M1              54, width_m1 = 54
0x0002b4:          0036 011b - NPU_SET_OFM_HEIGHT0_M1            54, height_m1 = 54
0x0002b8:          0036 011c - NPU_SET_OFM_HEIGHT1_M1            54, height_m1 = 54
0x0002bc:          0036 011a - NPU_SET_OFM_WIDTH0_M1             54, width_m1 = 54
0x0002c0: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x0002c8: 00000370 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x370
0x0002d0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0002d4:          0003 0122 - NPU_SET_KERNEL_STRIDE              3, stride_x_lsb = 1, stride_y_lsb = 1, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0002d8:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0002dc: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0002e4:          000d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         13, height_m1 = 13
0x0002e8:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x0002ec:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x0002f0:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x0002f4:          0000 0005 - NPU_OP_POOL                        0, pooling_mode = POOLING_MODE_MAX
// DMA src: OffChipFlash:ReadOnly, address: 0xcb0, dest: Sram:FeatureMap|Staging, address: 0xefc80, sizes: (N/A), length: 1168
0x0002f8: 00000cb0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xcb0
0x000300: 000efc80 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xefc80
0x000308: 00000490 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x490
0x000310:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x000314: 4ca9cadd 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1286195933
0x00031c: 000c0840 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xc0840
0x000324:          0036 010b - NPU_SET_IFM_HEIGHT0_M1            54, height_m1 = 54
0x000328:          0036 010c - NPU_SET_IFM_HEIGHT1_M1            54, height_m1 = 54
0x00032c:          0036 010a - NPU_SET_IFM_WIDTH0_M1             54, width_m1 = 54
0x000330: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x000338: 00000370 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x370
0x000340:          fff5 0109 - NPU_SET_IFM_ZERO_POINT         65525, zero_point = 65525
0x000344:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000348: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000350:          000f 0113 - NPU_SET_OFM_DEPTH_M1              15, depth_m1 = 15
0x000354: 00000370 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x370
0x00035c:          ffea 0118 - NPU_SET_OFM_ZERO_POINT         65514, zero_point = 65514
0x000360:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000364:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000368:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x00036c: 000efd20 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xefd20
0x000374: 000003f0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 1008
0x00037c: 000efc80 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xefc80
0x000384: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x00038c:          ffea 0126 - NPU_SET_ACTIVATION_MIN         65514, clip_boundary = 65514
0x000390:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000394:          0037 0115 - NPU_SET_OFM_BLK_WIDTH_M1          55, width_m1 = 55
0x000398:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x00039c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0003a0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x1140, dest: Sram:FeatureMap|Staging, address: 0xbd10, sizes: (N/A), length: 10368
0x0003a4: 00001140 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x1140
0x0003ac: 0000bd10 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xbd10
0x0003b4: 00002880 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x2880
0x0003bc:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0003c0:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x0003c4: 7aafe9db 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 2058349019
0x0003cc: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x0003d4:          000f 0104 - NPU_SET_IFM_DEPTH_M1              15, depth_m1 = 15
0x0003d8: 00000370 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x370
0x0003e0:          ffea 0109 - NPU_SET_IFM_ZERO_POINT         65514, zero_point = 65514
0x0003e4:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x0003e8:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x0003ec:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x0003f0:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x0003f4: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x0003fc:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x000400: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x000408:          002b 0118 - NPU_SET_OFM_ZERO_POINT            43, zero_point = 43
0x00040c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000410:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000414:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000418: 0000bf90 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xbf90
0x000420: 00002600 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 9728
0x000428: 0000bd10 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xbd10
0x000430: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x000438:          002b 0126 - NPU_SET_ACTIVATION_MIN            43, clip_boundary = 43
0x00043c:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x000440:          000f 0115 - NPU_SET_OFM_BLK_WIDTH_M1          15, width_m1 = 15
0x000444:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000448:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x00044c: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x000454:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x000458: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x000460:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000464:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x000468:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x00046c:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x000470:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000474:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000478: 0000cad0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xcad0
0x000480: 00001b80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b80
0x000488:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x00048c:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000490:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000494:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000498:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x00049c: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0004a4:          002d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         45, height_m1 = 45
0x0004a8:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x0004ac:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x0004b0:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x39c0, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2144
0x0004b4: 000039c0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x39c0
0x0004bc: 000999d0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x999d0
0x0004c4: 00000860 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x860
0x0004cc:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x0004d0: 4406b852 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1141291090
0x0004d8: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x0004e0:          000f 0104 - NPU_SET_IFM_DEPTH_M1              15, depth_m1 = 15
0x0004e4: 00000370 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x370
0x0004ec:          ffea 0109 - NPU_SET_IFM_ZERO_POINT         65514, zero_point = 65514
0x0004f0:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0004f4: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x0004fc: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x000504:          002b 0118 - NPU_SET_OFM_ZERO_POINT            43, zero_point = 43
0x000508:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x00050c: 00099c50 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x99c50
0x000514: 000005e0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 1504
0x00051c: 000999d0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x999d0
0x000524:          002b 0126 - NPU_SET_ACTIVATION_MIN            43, clip_boundary = 43
0x000528:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x00052c:          0007 0115 - NPU_SET_OFM_BLK_WIDTH_M1           7, width_m1 = 7
0x000530:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000534:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000538:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x00053c:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000540: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x000548:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x00054c: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x000554:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000558:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00055c: 0000bd10 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xbd10
0x000564: 00001b80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b80
0x00056c:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000570:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000574:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000578: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000580:          002d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         45, height_m1 = 45
0x000584:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000588:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x00058c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000590:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x4220, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2048
0x000594: 00004220 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x4220
0x00059c: 00000800 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x800
0x0005a4:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0005a8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x0005ac: 67beb47d 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1740551293
0x0005b4: 0000bd10 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xbd10
0x0005bc:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x0005c0: 00001b80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1b80
0x0005c8:          002b 0109 - NPU_SET_IFM_ZERO_POINT            43, zero_point = 43
0x0005cc:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0005d0: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x0005d8:          000f 0113 - NPU_SET_OFM_DEPTH_M1              15, depth_m1 = 15
0x0005dc: 00000370 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x370
0x0005e4:          ffde 0118 - NPU_SET_OFM_ZERO_POINT         65502, zero_point = 65502
0x0005e8: 00099a70 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x99a70
0x0005f0: 00000760 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 1888
0x0005f8: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x000600:          ffde 0126 - NPU_SET_ACTIVATION_MIN         65502, clip_boundary = 65502
0x000604:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000608:          0037 0115 - NPU_SET_OFM_BLK_WIDTH_M1          55, width_m1 = 55
0x00060c:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000610:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000614:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x4a20, dest: Sram:FeatureMap|Staging, address: 0x9a1d0, sizes: (N/A), length: 10208
0x000618: 00004a20 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x4a20
0x000620: 0009a1d0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x9a1d0
0x000628: 000027e0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x27e0
0x000630:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000634: 6304bf5f 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1661255519
0x00063c: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000644:          000f 0104 - NPU_SET_IFM_DEPTH_M1              15, depth_m1 = 15
0x000648: 00000370 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x370
0x000650:          ffde 0109 - NPU_SET_IFM_ZERO_POINT         65502, zero_point = 65502
0x000654:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x000658:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x00065c:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x000660:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x000664: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x00066c:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x000670: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x000678:          000e 0118 - NPU_SET_OFM_ZERO_POINT            14, zero_point = 14
0x00067c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000680:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000684:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000688: 0009a450 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x9a450
0x000690: 00002560 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 9568
0x000698: 0009a1d0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x9a1d0
0x0006a0: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x0006a8:          000e 0126 - NPU_SET_ACTIVATION_MIN            14, clip_boundary = 14
0x0006ac:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0006b0:          000f 0115 - NPU_SET_OFM_BLK_WIDTH_M1          15, width_m1 = 15
0x0006b4:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0006b8:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0006bc: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x0006c4:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x0006c8: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x0006d0:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0006d4:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x0006d8:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x0006dc:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x0006e0:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x0006e4:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0006e8: 0000cad0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xcad0
0x0006f0: 00001b80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b80
0x0006f8:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0006fc:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000700:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000704:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000708:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x00070c: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000714:          002d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         45, height_m1 = 45
0x000718:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x00071c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000720:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x7200, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2176
0x000724: 00007200 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x7200
0x00072c: 000999d0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x999d0
0x000734: 00000880 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x880
0x00073c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x000740:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000744: 40c9f4de 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1086977246
0x00074c: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000754:          000f 0104 - NPU_SET_IFM_DEPTH_M1              15, depth_m1 = 15
0x000758: 00000370 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x370
0x000760:          ffde 0109 - NPU_SET_IFM_ZERO_POINT         65502, zero_point = 65502
0x000764:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000768: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x000770: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x000778:          000e 0118 - NPU_SET_OFM_ZERO_POINT            14, zero_point = 14
0x00077c:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000780: 00099c50 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x99c50
0x000788: 00000600 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 1536
0x000790: 000999d0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x999d0
0x000798:          000e 0126 - NPU_SET_ACTIVATION_MIN            14, clip_boundary = 14
0x00079c:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0007a0:          0007 0115 - NPU_SET_OFM_BLK_WIDTH_M1           7, width_m1 = 7
0x0007a4:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x0007a8:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x0007ac:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0007b0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0007b4: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x0007bc:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x0007c0: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x0007c8:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0007cc:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0007d0: 0000bd10 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xbd10
0x0007d8: 00001b80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b80
0x0007e0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0007e4:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0007e8:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0007ec: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0007f4:          002d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         45, height_m1 = 45
0x0007f8:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x0007fc:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000800:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000804:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000808: 0000bd10 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xbd10
0x000810:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000814: 00001b80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1b80
0x00081c: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x000824:          001a 0112 - NPU_SET_OFM_HEIGHT_M1             26, height_m1 = 26
0x000828:          001a 0111 - NPU_SET_OFM_WIDTH_M1              26, width_m1 = 26
0x00082c:          007f 0113 - NPU_SET_OFM_DEPTH_M1             127, depth_m1 = 127
0x000830:          001a 011b - NPU_SET_OFM_HEIGHT0_M1            26, height_m1 = 26
0x000834:          001a 011c - NPU_SET_OFM_HEIGHT1_M1            26, height_m1 = 26
0x000838:          001a 011a - NPU_SET_OFM_WIDTH0_M1             26, width_m1 = 26
0x00083c: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000844: 000001b0 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x1b0
0x00084c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000850:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000854:          0003 0122 - NPU_SET_KERNEL_STRIDE              3, stride_x_lsb = 1, stride_y_lsb = 1, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000858:          000d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         13, height_m1 = 13
0x00085c:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x000860:          0000 0005 - NPU_OP_POOL                        0, pooling_mode = POOLING_MODE_MAX
// DMA src: OffChipFlash:ReadOnly, address: 0x7a80, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 4000
0x000864: 00007a80 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x7a80
0x00086c: 00000000 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x0
0x000874: 00000fa0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xfa0
0x00087c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x000880: 52997ce4 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1385790692
0x000888: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x000890:          001a 010b - NPU_SET_IFM_HEIGHT0_M1            26, height_m1 = 26
0x000894:          001a 010c - NPU_SET_IFM_HEIGHT1_M1            26, height_m1 = 26
0x000898:          001a 010a - NPU_SET_IFM_WIDTH0_M1             26, width_m1 = 26
0x00089c: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x0008a4: 000001b0 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1b0
0x0008ac:          000e 0109 - NPU_SET_IFM_ZERO_POINT            14, zero_point = 14
0x0008b0:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0008b4: 00000fa0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xfa0
0x0008bc:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x0008c0: 00000360 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x360
0x0008c8:          ffdb 0118 - NPU_SET_OFM_ZERO_POINT         65499, zero_point = 65499
0x0008cc:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x0008d0:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x0008d4:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0008d8: 00000140 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x140
0x0008e0: 00000e60 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 3680
0x0008e8: 00000000 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x0
0x0008f0: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x0008f8:          ffdb 0126 - NPU_SET_ACTIVATION_MIN         65499, clip_boundary = 65499
0x0008fc:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000900:          001b 0115 - NPU_SET_OFM_BLK_WIDTH_M1          27, width_m1 = 27
0x000904:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000908:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x00090c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000910:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x8a20, dest: Sram:FeatureMap|Staging, address: 0x6ac0, sizes: (N/A), length: 23952
0x000914: 00008a20 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x8a20
0x00091c: 00006ac0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x6ac0
0x000924: 00005d90 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x5d90
0x00092c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x000930:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000934: 4527abd1 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1160227793
0x00093c: 00000fa0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfa0
0x000944:          001f 0104 - NPU_SET_IFM_DEPTH_M1              31, depth_m1 = 31
0x000948: 00000360 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x360
0x000950:          ffdb 0109 - NPU_SET_IFM_ZERO_POINT         65499, zero_point = 65499
0x000954:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x000958:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x00095c:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x000960:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x000964: 000343c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x343c0
0x00096c:          004f 0113 - NPU_SET_OFM_DEPTH_M1              79, depth_m1 = 79
0x000970: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000978:          0003 0118 - NPU_SET_OFM_ZERO_POINT             3, zero_point = 3
0x00097c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000980:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000984:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000988: 00006de0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x6de0
0x000990: 00005a70 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 23152
0x000998: 00006ac0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x6ac0
0x0009a0: 00000320 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 800
0x0009a8:          0003 0126 - NPU_SET_ACTIVATION_MIN             3, clip_boundary = 3
0x0009ac:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x0009b0:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x0009b4:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0009b8:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0xe7b0, dest: Sram:FeatureMap|Staging, address: 0xc850, sizes: (N/A), length: 14672
0x0009bc: 0000e7b0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xe7b0
0x0009c4: 0000c850 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xc850
0x0009cc: 00003950 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x3950
0x0009d4:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x0009d8: 00034c30 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x34c30
0x0009e0:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x0009e4: 0000ca30 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xca30
0x0009ec: 00003770 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 14192
0x0009f4: 0000c850 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xc850
0x0009fc: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x000a04:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000a08:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000a0c: 000343c0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x343c0
0x000a14:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000a18: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x000a20:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000a24:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x000a28:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x000a2c:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x000a30:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000a34:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000a38: 00007840 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x7840
0x000a40:          007f 0113 - NPU_SET_OFM_DEPTH_M1             127, depth_m1 = 127
0x000a44: 00001b00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b00
0x000a4c:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000a50:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000a54:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000a58:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000a5c:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000a60: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000a68:          0012 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         18, height_m1 = 18
0x000a6c:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000a70:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000a74:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000a78:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x12100, dest: Sram:FeatureMap|Staging, address: 0x4b040, sizes: (N/A), length: 7264
0x000a7c: 00012100 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x12100
0x000a84: 0004b040 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x4b040
0x000a8c: 00001c60 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1c60
0x000a94:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000a98: 5d639be5 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1566809061
0x000aa0: 00000fa0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfa0
0x000aa8:          001f 0104 - NPU_SET_IFM_DEPTH_M1              31, depth_m1 = 31
0x000aac: 00000360 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x360
0x000ab4:          ffdb 0109 - NPU_SET_IFM_ZERO_POINT         65499, zero_point = 65499
0x000ab8:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000abc: 000343c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x343c0
0x000ac4: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000acc:          0003 0118 - NPU_SET_OFM_ZERO_POINT             3, zero_point = 3
0x000ad0:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000ad4: 0004b540 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x4b540
0x000adc: 00001760 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 5984
0x000ae4: 0004b040 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x4b040
0x000aec: 00000500 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1280
0x000af4:          0003 0126 - NPU_SET_ACTIVATION_MIN             3, clip_boundary = 3
0x000af8:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000afc:          001b 0115 - NPU_SET_OFM_BLK_WIDTH_M1          27, width_m1 = 27
0x000b00:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000b04:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000b08:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000b0c: 000343c0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x343c0
0x000b14:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000b18: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x000b20:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000b24:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000b28: 00006ac0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6ac0
0x000b30: 00001b00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b00
0x000b38:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000b3c:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000b40:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000b44: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000b4c:          0012 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         18, height_m1 = 18
0x000b50:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000b54:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000b58:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x13d60, dest: Sram:FeatureMap|Staging, address: 0x4b040, sizes: (N/A), length: 7520
0x000b5c: 00013d60 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x13d60
0x000b64: 00001d60 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1d60
0x000b6c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x000b70:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x000b74: 6c9ea5b5 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1822336437
0x000b7c: 00006ac0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6ac0
0x000b84:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x000b88: 00001b00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1b00
0x000b90:          0003 0109 - NPU_SET_IFM_ZERO_POINT             3, zero_point = 3
0x000b94:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000b98: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000ba0:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x000ba4: 00000360 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x360
0x000bac:          ffea 0118 - NPU_SET_OFM_ZERO_POINT         65514, zero_point = 65514
0x000bb0: 0004b180 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x4b180
0x000bb8: 00001c20 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 7200
0x000bc0: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x000bc8:          ffea 0126 - NPU_SET_ACTIVATION_MIN         65514, clip_boundary = 65514
0x000bcc:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000bd0:          001b 0115 - NPU_SET_OFM_BLK_WIDTH_M1          27, width_m1 = 27
0x000bd4:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000bd8:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000bdc:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x15ac0, dest: Sram:FeatureMap|Staging, address: 0x4cda0, sizes: (N/A), length: 38720
0x000be0: 00015ac0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x15ac0
0x000be8: 0004cda0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x4cda0
0x000bf0: 00009740 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x9740
0x000bf8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000bfc: 4af119a8 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1257314728
0x000c04: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000c0c:          001f 0104 - NPU_SET_IFM_DEPTH_M1              31, depth_m1 = 31
0x000c10: 00000360 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x360
0x000c18:          ffea 0109 - NPU_SET_IFM_ZERO_POINT         65514, zero_point = 65514
0x000c1c:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x000c20:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x000c24:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x000c28:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x000c2c: 00033420 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x33420
0x000c34:          007f 0113 - NPU_SET_OFM_DEPTH_M1             127, depth_m1 = 127
0x000c38: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000c40:          0025 0118 - NPU_SET_OFM_ZERO_POINT            37, zero_point = 37
0x000c44:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000c48:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000c4c:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000c50: 0004d2a0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x4d2a0
0x000c58: 00009240 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 37440
0x000c60: 0004cda0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x4cda0
0x000c68: 00000500 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1280
0x000c70:          0025 0126 - NPU_SET_ACTIVATION_MIN            37, clip_boundary = 37
0x000c74:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x000c78:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000c7c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000c80:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000c84: 00033420 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x33420
0x000c8c:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000c90: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x000c98:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000c9c:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x000ca0:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x000ca4:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x000ca8:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000cac:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000cb0: 000068a0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x68a0
0x000cb8: 00001b00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b00
0x000cc0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000cc4:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000cc8:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000ccc:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000cd0:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000cd4: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000cdc:          0012 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         18, height_m1 = 18
0x000ce0:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000ce4:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000ce8:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000cec:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x1f200, dest: Sram:FeatureMap|Staging, address: 0x4a0a0, sizes: (N/A), length: 7232
0x000cf0: 0001f200 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x1f200
0x000cf8: 0004a0a0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x4a0a0
0x000d00: 00001c40 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1c40
0x000d08:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000d0c: 7e597592 2029 4024 - NPU_SET_OFM_SCALE               8233, shift = 41, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 2119792018
0x000d14: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000d1c:          001f 0104 - NPU_SET_IFM_DEPTH_M1              31, depth_m1 = 31
0x000d20: 00000360 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x360
0x000d28:          ffea 0109 - NPU_SET_IFM_ZERO_POINT         65514, zero_point = 65514
0x000d2c:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000d30: 00033420 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x33420
0x000d38: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000d40:          0025 0118 - NPU_SET_OFM_ZERO_POINT            37, zero_point = 37
0x000d44:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000d48: 0004a5a0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x4a5a0
0x000d50: 00001740 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 5952
0x000d58: 0004a0a0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x4a0a0
0x000d60:          0025 0126 - NPU_SET_ACTIVATION_MIN            37, clip_boundary = 37
0x000d64:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000d68:          001b 0115 - NPU_SET_OFM_BLK_WIDTH_M1          27, width_m1 = 27
0x000d6c:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000d70:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000d74:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000d78: 00033420 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x33420
0x000d80:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000d84: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x000d8c:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000d90:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000d94: 00005b20 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x5b20
0x000d9c: 00001b00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b00
0x000da4:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000da8:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000dac:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000db0: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000db8:          0012 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         18, height_m1 = 18
0x000dbc:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000dc0:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000dc4:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[13, 14, 16], IFM Block=[1, 28, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000dc8: 00005b20 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x5b20
0x000dd0:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x000dd4: 00001b00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1b00
0x000ddc: 00033420 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x33420
0x000de4:          000c 0112 - NPU_SET_OFM_HEIGHT_M1             12, height_m1 = 12
0x000de8:          000c 0111 - NPU_SET_OFM_WIDTH_M1              12, width_m1 = 12
0x000dec:          00ff 0113 - NPU_SET_OFM_DEPTH_M1             255, depth_m1 = 255
0x000df0:          000c 011b - NPU_SET_OFM_HEIGHT0_M1            12, height_m1 = 12
0x000df4:          000c 011c - NPU_SET_OFM_HEIGHT1_M1            12, height_m1 = 12
0x000df8:          000c 011a - NPU_SET_OFM_WIDTH0_M1             12, width_m1 = 12
0x000dfc: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x000e04: 000000d0 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0xd0
0x000e0c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000e10:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000e14:          0003 0122 - NPU_SET_KERNEL_STRIDE              3, stride_x_lsb = 1, stride_y_lsb = 1, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000e18:          000c 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         12, height_m1 = 12
0x000e1c:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x000e20:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000e24:          0003 012f - NPU_SET_BLOCKDEP                   3, blockdep = 3
0x000e28:          0000 0005 - NPU_OP_POOL                        0, pooling_mode = POOLING_MODE_MAX
// DMA src: OffChipFlash:ReadOnly, address: 0x20e40, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 7568
0x000e2c: 00020e40 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x20e40
0x000e34: 00000000 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x0
0x000e3c: 00001d90 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1d90
0x000e44:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x000e48: 5b2676c0 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1529247424
0x000e50: 00033420 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x33420
0x000e58:          000c 010b - NPU_SET_IFM_HEIGHT0_M1            12, height_m1 = 12
0x000e5c:          000c 010c - NPU_SET_IFM_HEIGHT1_M1            12, height_m1 = 12
0x000e60:          000c 010a - NPU_SET_IFM_WIDTH0_M1             12, width_m1 = 12
0x000e64: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x000e6c: 000000d0 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0xd0
0x000e74:          0025 0109 - NPU_SET_IFM_ZERO_POINT            37, zero_point = 37
0x000e78:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000e7c: 0000fd80 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xfd80
0x000e84:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x000e88: 00000270 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x270
0x000e90:          fff1 0118 - NPU_SET_OFM_ZERO_POINT         65521, zero_point = 65521
0x000e94:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000e98:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000e9c:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000ea0: 00000140 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x140
0x000ea8: 00001c50 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 7248
0x000eb0: 00000000 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x0
0x000eb8: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x000ec0:          fff1 0126 - NPU_SET_ACTIVATION_MIN         65521, clip_boundary = 65521
0x000ec4:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000ec8:          002f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          47, depth_m1 = 47
0x000ecc:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x000ed0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000ed4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x22bd0, dest: Sram:FeatureMap|Staging, address: 0x1d90, sizes: (N/A), length: 3808
0x000ed8: 00022bd0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x22bd0
0x000ee0: 00001d90 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1d90
0x000ee8: 00000ee0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xee0
0x000ef0:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x000ef4: 0000ff20 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xff20
0x000efc:          000f 0113 - NPU_SET_OFM_DEPTH_M1              15, depth_m1 = 15
0x000f00: 00001e30 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1e30
0x000f08: 00000e40 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 3648
0x000f10: 00001d90 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1d90
0x000f18: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x000f20:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x000f24:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000f28:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x23ab0, dest: Sram:FeatureMap|Staging, address: 0x2c70, sizes: (N/A), length: 7200
0x000f2c: 00023ab0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x23ab0
0x000f34: 00002c70 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2c70
0x000f3c: 00001c20 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1c20
0x000f44:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x000f48: 58d7e4a4 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1490543780
0x000f50: 0000fd80 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfd80
0x000f58:          002f 0104 - NPU_SET_IFM_DEPTH_M1              47, depth_m1 = 47
0x000f5c: 00000270 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x270
0x000f64:          fff1 0109 - NPU_SET_IFM_ZERO_POINT         65521, zero_point = 65521
0x000f68:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x000f6c:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x000f70:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x000f74:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x000f78: 00011d30 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x11d30
0x000f80: 000009c0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x9c0
0x000f88:          0018 0118 - NPU_SET_OFM_ZERO_POINT            24, zero_point = 24
0x000f8c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000f90:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000f94:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000f98: 00002d10 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2d10
0x000fa0: 00001b80 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 7040
0x000fa8: 00002c70 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2c70
0x000fb0:          0018 0126 - NPU_SET_ACTIVATION_MIN            24, clip_boundary = 24
0x000fb4:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x000fb8:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000fbc:          0004 012f - NPU_SET_BLOCKDEP                   4, blockdep = 4
0x000fc0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000fc4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x256d0, dest: Sram:FeatureMap|Staging, address: 0x8090, sizes: (N/A), length: 21216
0x000fc8: 000256d0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x256d0
0x000fd0: 00008090 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x8090
0x000fd8: 000052e0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x52e0
0x000fe0:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x000fe4: 00011e00 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x11e00
0x000fec:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x000ff0: 00008270 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x8270
0x000ff8: 00005100 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 20736
0x001000: 00008090 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x8090
0x001008: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x001010:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x001014:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001018:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x2a9b0, dest: Sram:FeatureMap|Staging, address: 0x2c70, sizes: (N/A), length: 21536
0x00101c: 0002a9b0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x2a9b0
0x001024: 00002c70 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2c70
0x00102c: 00005420 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x5420
0x001034:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001038:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x00103c: 00012070 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x12070
0x001044: 00002e50 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2e50
0x00104c: 00005240 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21056
0x001054: 00002c70 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2c70
0x00105c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001060:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x2fdd0, dest: Sram:FeatureMap|Staging, address: 0x8090, sizes: (N/A), length: 21312
0x001064: 0002fdd0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x2fdd0
0x00106c: 00008090 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x8090
0x001074: 00005340 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x5340
0x00107c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001080:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001084: 000122e0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x122e0
0x00108c: 00008270 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x8270
0x001094: 00005160 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 20832
0x00109c: 00008090 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x8090
0x0010a4:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0010a8:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x35110, dest: Sram:FeatureMap|Staging, address: 0x2c70, sizes: (N/A), length: 14320
0x0010ac: 00035110 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x35110
0x0010b4: 00002c70 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2c70
0x0010bc: 000037f0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x37f0
0x0010c4:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0010c8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0010cc: 00012550 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x12550
0x0010d4:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x0010d8: 00002db0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2db0
0x0010e0: 000036b0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 14000
0x0010e8: 00002c70 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2c70
0x0010f0: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x0010f8:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0010fc:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001100: 00011d30 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x11d30
0x001108:          00bf 0104 - NPU_SET_IFM_DEPTH_M1             191, depth_m1 = 191
0x00110c: 000009c0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x9c0
0x001114:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001118:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x00111c:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x001120:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x001124:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x001128:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00112c: 000009c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x9c0
0x001134:          00bf 0113 - NPU_SET_OFM_DEPTH_M1             191, depth_m1 = 191
0x001138: 00001380 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1380
0x001140:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001144:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x001148:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x00114c:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001150:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001154: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x00115c:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x001160:          0009 0115 - NPU_SET_OFM_BLK_WIDTH_M1           9, width_m1 = 9
0x001164:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001168:          0004 012f - NPU_SET_BLOCKDEP                   4, blockdep = 4
0x00116c:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x38900, dest: Sram:FeatureMap|Staging, address: 0x1dc80, sizes: (N/A), length: 12304
0x001170: 00038900 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x38900
0x001178: 0001dc80 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1dc80
0x001180: 00003010 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x3010
0x001188:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x00118c: 4343a765 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1128507237
0x001194: 0000fd80 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfd80
0x00119c:          002f 0104 - NPU_SET_IFM_DEPTH_M1              47, depth_m1 = 47
0x0011a0: 00000270 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x270
0x0011a8:          fff1 0109 - NPU_SET_IFM_ZERO_POINT         65521, zero_point = 65521
0x0011ac:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0011b0: 00015dc0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x15dc0
0x0011b8: 000009c0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x9c0
0x0011c0:          0018 0118 - NPU_SET_OFM_ZERO_POINT            24, zero_point = 24
0x0011c4: 0001e400 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1e400
0x0011cc: 00002890 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 10384
0x0011d4: 0001dc80 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1dc80
0x0011dc: 00000780 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1920
0x0011e4:          0018 0126 - NPU_SET_ACTIVATION_MIN            24, clip_boundary = 24
0x0011e8:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0011ec:          0007 0115 - NPU_SET_OFM_BLK_WIDTH_M1           7, width_m1 = 7
0x0011f0:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x0011f4:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x0011f8:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x0011fc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001200:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001204: 00015dc0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15dc0
0x00120c:          00bf 0104 - NPU_SET_IFM_DEPTH_M1             191, depth_m1 = 191
0x001210: 000009c0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x9c0
0x001218:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x00121c:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001220: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x001228: 00001380 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1380
0x001230:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001234:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001238: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x001240:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x001244:          0009 0115 - NPU_SET_OFM_BLK_WIDTH_M1           9, width_m1 = 9
0x001248:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x00124c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001250:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x3b910, dest: Sram:FeatureMap|Staging, address: 0x11d30, sizes: (N/A), length: 16528
0x001254: 0003b910 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x3b910
0x00125c: 00011d30 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x11d30
0x001264: 00004090 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x4090
0x00126c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001270: 51a6a374 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1369875316
0x001278: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001280:          017f 0104 - NPU_SET_IFM_DEPTH_M1             383, depth_m1 = 383
0x001284: 00001380 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1380
0x00128c:          0018 0109 - NPU_SET_IFM_ZERO_POINT            24, zero_point = 24
0x001290:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001294: 0000fd80 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xfd80
0x00129c:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x0012a0: 00000270 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x270
0x0012a8:          ffd6 0118 - NPU_SET_OFM_ZERO_POINT         65494, zero_point = 65494
0x0012ac: 00011f10 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x11f10
0x0012b4: 00003eb0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 16048
0x0012bc: 00011d30 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x11d30
0x0012c4: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x0012cc:          ffd6 0126 - NPU_SET_ACTIVATION_MIN         65494, clip_boundary = 65494
0x0012d0:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x0012d4:          002f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          47, depth_m1 = 47
0x0012d8:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x0012dc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0012e0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x3f9a0, dest: Sram:FeatureMap|Staging, address: 0x15dc0, sizes: (N/A), length: 21744
0x0012e4: 0003f9a0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x3f9a0
0x0012ec: 00015dc0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x15dc0
0x0012f4: 000054f0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x54f0
0x0012fc:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001300:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001304: 6b379d9e 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1798806942
0x00130c: 0000fd80 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfd80
0x001314:          002f 0104 - NPU_SET_IFM_DEPTH_M1              47, depth_m1 = 47
0x001318: 00000270 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x270
0x001320:          ffd6 0109 - NPU_SET_IFM_ZERO_POINT         65494, zero_point = 65494
0x001324:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x001328:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x00132c:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x001330:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x001334: 00025ba0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x25ba0
0x00133c: 000009c0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x9c0
0x001344:          fffe 0118 - NPU_SET_OFM_ZERO_POINT         65534, zero_point = 65534
0x001348:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x00134c:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x001350:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001354: 00015fa0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x15fa0
0x00135c: 00005310 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21264
0x001364: 00015dc0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x15dc0
0x00136c:          fffe 0126 - NPU_SET_ACTIVATION_MIN         65534, clip_boundary = 65534
0x001370:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001374:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001378:          0002 012f - NPU_SET_BLOCKDEP                   2, blockdep = 2
0x00137c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001380:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x44e90, dest: Sram:FeatureMap|Staging, address: 0x1b2b0, sizes: (N/A), length: 43248
0x001384: 00044e90 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x44e90
0x00138c: 0001b2b0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1b2b0
0x001394: 0000a8f0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xa8f0
0x00139c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0013a0: 00025e10 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x25e10
0x0013a8:          005f 0113 - NPU_SET_OFM_DEPTH_M1              95, depth_m1 = 95
0x0013ac: 0001b670 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1b670
0x0013b4: 0000a530 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 42288
0x0013bc: 0001b2b0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1b2b0
0x0013c4: 000003c0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 960
0x0013cc:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x0013d0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0013d4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x4f780, dest: Sram:FeatureMap|Staging, address: 0x15dc0, sizes: (N/A), length: 21552
0x0013d8: 0004f780 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x4f780
0x0013e0: 00015dc0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x15dc0
0x0013e8: 00005430 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x5430
0x0013f0:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0013f4:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0013f8: 000262f0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x262f0
0x001400:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x001404: 00015fa0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x15fa0
0x00140c: 00005250 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21072
0x001414: 00015dc0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x15dc0
0x00141c: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x001424:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001428:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x00142c: 00025ba0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x25ba0
0x001434:          00bf 0104 - NPU_SET_IFM_DEPTH_M1             191, depth_m1 = 191
0x001438: 000009c0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x9c0
0x001440:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001444:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x001448:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x00144c:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x001450:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x001454:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001458: 000009c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x9c0
0x001460:          00bf 0113 - NPU_SET_OFM_DEPTH_M1             191, depth_m1 = 191
0x001464: 00001380 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1380
0x00146c:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001470:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x001474:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x001478:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x00147c:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001480: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x001488:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x00148c:          0009 0115 - NPU_SET_OFM_BLK_WIDTH_M1           9, width_m1 = 9
0x001490:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001494:          0006 012f - NPU_SET_BLOCKDEP                   6, blockdep = 6
0x001498:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x54bb0, dest: Sram:FeatureMap|Staging, address: 0x11d30, sizes: (N/A), length: 12336
0x00149c: 00054bb0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x54bb0
0x0014a4: 00011d30 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x11d30
0x0014ac: 00003030 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x3030
0x0014b4:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x0014b8: 74c6eac0 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1959193280
0x0014c0: 0000fd80 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfd80
0x0014c8:          002f 0104 - NPU_SET_IFM_DEPTH_M1              47, depth_m1 = 47
0x0014cc: 00000270 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x270
0x0014d4:          ffd6 0109 - NPU_SET_IFM_ZERO_POINT         65494, zero_point = 65494
0x0014d8:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0014dc: 0001d340 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x1d340
0x0014e4: 000009c0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x9c0
0x0014ec:          fffe 0118 - NPU_SET_OFM_ZERO_POINT         65534, zero_point = 65534
0x0014f0: 000124b0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x124b0
0x0014f8: 000028b0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 10416
0x001500: 00011d30 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x11d30
0x001508: 00000780 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1920
0x001510:          fffe 0126 - NPU_SET_ACTIVATION_MIN         65534, clip_boundary = 65534
0x001514:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001518:          0007 0115 - NPU_SET_OFM_BLK_WIDTH_M1           7, width_m1 = 7
0x00151c:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001520:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x001524:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x001528:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x00152c:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001530: 0001d340 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x1d340
0x001538:          00bf 0104 - NPU_SET_IFM_DEPTH_M1             191, depth_m1 = 191
0x00153c: 000009c0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x9c0
0x001544:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001548:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00154c: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x001554: 00001380 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1380
0x00155c:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001560:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001564: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x00156c:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x001570:          0009 0115 - NPU_SET_OFM_BLK_WIDTH_M1           9, width_m1 = 9
0x001574:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001578:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x00157c:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x57be0, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 22272
0x001580: 00057be0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x57be0
0x001588: 00017c40 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x17c40
0x001590: 00005700 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x5700
0x001598:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x00159c: 6ccb9705 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1825281797
0x0015a4: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x0015ac:          017f 0104 - NPU_SET_IFM_DEPTH_M1             383, depth_m1 = 383
0x0015b0: 00001380 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1380
0x0015b8:          fffe 0109 - NPU_SET_IFM_ZERO_POINT         65534, zero_point = 65534
0x0015bc:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0015c0: 00015200 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x15200
0x0015c8:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x0015cc: 00000340 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x340
0x0015d4:          fff8 0118 - NPU_SET_OFM_ZERO_POINT         65528, zero_point = 65528
0x0015d8: 00017ec0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x17ec0
0x0015e0: 00005480 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21632
0x0015e8: 00017c40 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x17c40
0x0015f0: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x0015f8:          fff8 0126 - NPU_SET_ACTIVATION_MIN         65528, clip_boundary = 65528
0x0015fc:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x001600:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x001604:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001608:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x00160c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001610:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x5d2e0, dest: Sram:FeatureMap|Staging, address: 0x1d340, sizes: (N/A), length: 28560
0x001614: 0005d2e0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x5d2e0
0x00161c: 0001d340 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1d340
0x001624: 00006f90 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x6f90
0x00162c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001630:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001634: 558ab024 2029 4024 - NPU_SET_OFM_SCALE               8233, shift = 41, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1435152420
0x00163c: 00015200 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15200
0x001644:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x001648: 00000340 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x340
0x001650:          fff8 0109 - NPU_SET_IFM_ZERO_POINT         65528, zero_point = 65528
0x001654:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x001658:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x00165c:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x001660:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x001664: 000393c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x393c0
0x00166c:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x001670: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x001678:          fff9 0118 - NPU_SET_OFM_ZERO_POINT         65529, zero_point = 65529
0x00167c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x001680:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x001684:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001688: 0001d520 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1d520
0x001690: 00006db0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 28080
0x001698: 0001d340 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1d340
0x0016a0: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x0016a8:          fff9 0126 - NPU_SET_ACTIVATION_MIN         65529, clip_boundary = 65529
0x0016ac:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0016b0:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x0016b4:          0003 012f - NPU_SET_BLOCKDEP                   3, blockdep = 3
0x0016b8:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0016bc:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x64270, dest: Sram:FeatureMap|Staging, address: 0x2b470, sizes: (N/A), length: 57168
0x0016c0: 00064270 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x64270
0x0016c8: 0002b470 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2b470
0x0016d0: 0000df50 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xdf50
0x0016d8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0016dc: 00039630 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x39630
0x0016e4:          005f 0113 - NPU_SET_OFM_DEPTH_M1              95, depth_m1 = 95
0x0016e8: 0002b830 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2b830
0x0016f0: 0000db90 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 56208
0x0016f8: 0002b470 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2b470
0x001700: 000003c0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 960
0x001708:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x00170c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001710:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x721c0, dest: Sram:FeatureMap|Staging, address: 0x1d340, sizes: (N/A), length: 57648
0x001714: 000721c0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x721c0
0x00171c: 0001d340 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1d340
0x001724: 0000e130 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xe130
0x00172c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001730:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001734: 00039b10 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x39b10
0x00173c: 0001d700 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1d700
0x001744: 0000dd70 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 56688
0x00174c: 0001d340 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1d340
0x001754:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001758:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x802f0, dest: Sram:FeatureMap|Staging, address: 0x2b470, sizes: (N/A), length: 9712
0x00175c: 000802f0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x802f0
0x001764: 0002b470 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2b470
0x00176c: 000025f0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x25f0
0x001774:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001778:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x00177c: 00039ff0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x39ff0
0x001784:          000f 0113 - NPU_SET_OFM_DEPTH_M1              15, depth_m1 = 15
0x001788: 0002b510 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2b510
0x001790: 00002550 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 9552
0x001798: 0002b470 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2b470
0x0017a0: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x0017a8:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0017ac:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0017b0: 000393c0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x393c0
0x0017b8:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x0017bc: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x0017c4:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0017c8:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x0017cc:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x0017d0:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x0017d4:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x0017d8:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0017dc: 00000d00 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xd00
0x0017e4:          00ff 0113 - NPU_SET_OFM_DEPTH_M1             255, depth_m1 = 255
0x0017e8: 00001a00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1a00
0x0017f0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0017f4:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x0017f8:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x0017fc:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001800:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001804: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x00180c:          000a 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         10, height_m1 = 10
0x001810:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x001814:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001818:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x00181c:          0002 012f - NPU_SET_BLOCKDEP                   2, blockdep = 2
0x001820:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x828e0, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 18464
0x001824: 000828e0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x828e0
0x00182c: 00017c40 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x17c40
0x001834: 00004820 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x4820
0x00183c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001840: 6bacffb2 2029 4024 - NPU_SET_OFM_SCALE               8233, shift = 41, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1806499762
0x001848: 00015200 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15200
0x001850:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x001854: 00000340 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x340
0x00185c:          fff8 0109 - NPU_SET_IFM_ZERO_POINT         65528, zero_point = 65528
0x001860:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001864: 0001f060 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x1f060
0x00186c: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x001874:          fff9 0118 - NPU_SET_OFM_ZERO_POINT         65529, zero_point = 65529
0x001878: 00018640 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x18640
0x001880: 00003e20 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 15904
0x001888: 00017c40 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x17c40
0x001890: 00000a00 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 2560
0x001898:          fff9 0126 - NPU_SET_ACTIVATION_MIN         65529, clip_boundary = 65529
0x00189c:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0018a0:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x0018a4:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x0018a8:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x0018ac:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x0018b0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0018b4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0018b8: 0001f060 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x1f060
0x0018c0:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x0018c4: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x0018cc:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0018d0:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0018d4: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x0018dc: 00001a00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1a00
0x0018e4:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0018e8:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0018ec: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0018f4:          000a 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         10, height_m1 = 10
0x0018f8:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x0018fc:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001900:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001904:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x87100, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 29728
0x001908: 00087100 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x87100
0x001910: 00007420 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x7420
0x001918:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x00191c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001920: 64c0bae2 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1690352354
0x001928: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001930:          01ff 0104 - NPU_SET_IFM_DEPTH_M1             511, depth_m1 = 511
0x001934: 00001a00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1a00
0x00193c:          fff9 0109 - NPU_SET_IFM_ZERO_POINT         65529, zero_point = 65529
0x001940:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001944: 00015200 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x15200
0x00194c:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x001950: 00000340 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x340
0x001958:          ffdc 0118 - NPU_SET_OFM_ZERO_POINT         65500, zero_point = 65500
0x00195c: 00017ec0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x17ec0
0x001964: 000071a0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 29088
0x00196c: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x001974:          ffdc 0126 - NPU_SET_ACTIVATION_MIN         65500, clip_boundary = 65500
0x001978:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x00197c:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x001980:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x001984:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001988:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x8e520, dest: Sram:FeatureMap|Staging, address: 0x1f060, sizes: (N/A), length: 39072
0x00198c: 0008e520 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x8e520
0x001994: 0001f060 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1f060
0x00199c: 000098a0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x98a0
0x0019a4:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0019a8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0019ac: 550f800b 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1427079179
0x0019b4: 00015200 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15200
0x0019bc:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x0019c0: 00000340 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x340
0x0019c8:          ffdc 0109 - NPU_SET_IFM_ZERO_POINT         65500, zero_point = 65500
0x0019cc:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x0019d0:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x0019d4:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x0019d8:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x0019dc: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x0019e4: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x0019ec:          0056 0118 - NPU_SET_OFM_ZERO_POINT            86, zero_point = 86
0x0019f0:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x0019f4:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x0019f8:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0019fc: 0001f2e0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1f2e0
0x001a04: 00009620 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 38432
0x001a0c: 0001f060 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1f060
0x001a14:          0056 0126 - NPU_SET_ACTIVATION_MIN            86, clip_boundary = 86
0x001a18:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001a1c:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001a20:          0003 012f - NPU_SET_BLOCKDEP                   3, blockdep = 3
0x001a24:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001a28:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x97dc0, dest: Sram:FeatureMap|Staging, address: 0x28910, sizes: (N/A), length: 78528
0x001a2c: 00097dc0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x97dc0
0x001a34: 00028910 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x28910
0x001a3c: 000132c0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x132c0
0x001a44:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001a48: 00000340 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x340
0x001a50:          007f 0113 - NPU_SET_OFM_DEPTH_M1             127, depth_m1 = 127
0x001a54: 00028e10 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x28e10
0x001a5c: 00012dc0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 77248
0x001a64: 00028910 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x28910
0x001a6c: 00000500 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1280
0x001a74:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x001a78:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001a7c:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0xab080, dest: Sram:FeatureMap|Staging, address: 0x1f060, sizes: (N/A), length: 39088
0x001a80: 000ab080 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xab080
0x001a88: 0001f060 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1f060
0x001a90: 000098b0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x98b0
0x001a98:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001a9c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001aa0: 000009c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x9c0
0x001aa8:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x001aac: 0001f2e0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1f2e0
0x001ab4: 00009630 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 38448
0x001abc: 0001f060 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1f060
0x001ac4: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x001acc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001ad0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001ad4: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001adc:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x001ae0: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x001ae8:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001aec:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x001af0:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x001af4:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x001af8:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x001afc:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001b00: 00018940 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x18940
0x001b08:          00ff 0113 - NPU_SET_OFM_DEPTH_M1             255, depth_m1 = 255
0x001b0c: 00001a00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1a00
0x001b14:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001b18:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x001b1c:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x001b20:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001b24:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001b28: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x001b30:          000a 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         10, height_m1 = 10
0x001b34:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x001b38:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001b3c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001b40:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0xb4930, dest: Sram:FeatureMap|Staging, address: 0xa900, sizes: (N/A), length: 18800
0x001b44: 000b4930 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xb4930
0x001b4c: 0000a900 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xa900
0x001b54: 00004970 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x4970
0x001b5c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001b60: 7946a2a4 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 2034672292
0x001b68: 00015200 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15200
0x001b70:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x001b74: 00000340 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x340
0x001b7c:          ffdc 0109 - NPU_SET_IFM_ZERO_POINT         65500, zero_point = 65500
0x001b80:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001b84: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x001b8c: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x001b94:          0056 0118 - NPU_SET_OFM_ZERO_POINT            86, zero_point = 86
0x001b98: 0000b300 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xb300
0x001ba0: 00003f70 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 16240
0x001ba8: 0000a900 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xa900
0x001bb0: 00000a00 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 2560
0x001bb8:          0056 0126 - NPU_SET_ACTIVATION_MIN            86, clip_boundary = 86
0x001bbc:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001bc0:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x001bc4:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001bc8:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x001bcc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001bd0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001bd4: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001bdc:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x001be0: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x001be8:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001bec:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001bf0: 00017c40 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x17c40
0x001bf8: 00001a00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1a00
0x001c00:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001c04:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001c08: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x001c10:          000a 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         10, height_m1 = 10
0x001c14:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x001c18:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001c1c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001c20:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0xb92a0, dest: Sram:FeatureMap|Staging, address: 0xa900, sizes: (N/A), length: 2576
0x001c24: 000b92a0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xb92a0
0x001c2c: 00000a10 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xa10
0x001c34:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001c38:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001c3c: 73ffadc8 2024 4024 - NPU_SET_OFM_SCALE               8228, shift = 36, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1946136008
0x001c44: 00017c40 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x17c40
0x001c4c:          01ff 0104 - NPU_SET_IFM_DEPTH_M1             511, depth_m1 = 511
0x001c50: 00001a00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1a00
0x001c58:          0056 0109 - NPU_SET_IFM_ZERO_POINT            86, zero_point = 86
0x001c5c:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001c60: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x001c68:          0003 0113 - NPU_SET_OFM_DEPTH_M1               3, depth_m1 = 3
0x001c6c: 000000d0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd0
0x001c74:          ffc9 0118 - NPU_SET_OFM_ZERO_POINT         65481, zero_point = 65481
0x001c78: 0000a9a0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xa9a0
0x001c80: 00000970 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 2416
0x001c88: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x001c90:          ffc9 0126 - NPU_SET_ACTIVATION_MIN         65481, clip_boundary = 65481
0x001c94:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001c98:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x001c9c:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001ca0:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x001ca4:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001ca8:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// AvgPool , subOps: -, size=13,13 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 16], IFM Block=[1, 8, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001cac: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001cb4:          0003 0104 - NPU_SET_IFM_DEPTH_M1               3, depth_m1 = 3
0x001cb8: 000000d0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd0
0x001cc0:          ffc9 0109 - NPU_SET_IFM_ZERO_POINT         65481, zero_point = 65481
0x001cc4:          0101 0114 - NPU_SET_OFM_PRECISION            257, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001cc8: 00000a90 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xa90
0x001cd0:          0000 0112 - NPU_SET_OFM_HEIGHT_M1              0, height_m1 = 0
0x001cd4:          0000 0111 - NPU_SET_OFM_WIDTH_M1               0, width_m1 = 0
0x001cd8:          0000 011b - NPU_SET_OFM_HEIGHT0_M1             0, height_m1 = 0
0x001cdc:          0000 011c - NPU_SET_OFM_HEIGHT1_M1             0, height_m1 = 0
0x001ce0:          0000 011a - NPU_SET_OFM_WIDTH0_M1              0, width_m1 = 0
0x001ce4: 00000004 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x4
0x001cec: 00000004 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x4
0x001cf4: 00000001 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x1
0x001cfc:          000c 0121 - NPU_SET_KERNEL_HEIGHT_M1          12, height_m1 = 12
0x001d00:          000c 0120 - NPU_SET_KERNEL_WIDTH_M1           12, width_m1 = 12
0x001d04:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001d08: 60f25dec 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1626496492
0x001d10:          0000 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          0, height_m1 = 0
0x001d14:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x001d18:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001d1c:          0000 012f - NPU_SET_BLOCKDEP                   0, blockdep = 0
0x001d20:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
0x001d24:          ffff 0000 - NPU_OP_STOP                    65535, mask = 65535

Configuration files:
   original = ['/workspace/executorch-venv/lib/python3.12/site-packages/ethosu/config_files/Arm/vela.ini']
   used = ['/workspace/executorch-venv/lib/python3.12/site-packages/ethosu/config_files/Arm/vela.ini']
System Configuration (Ethos_U85_SRAM_MRAM):
   core_clock = 400000000.0
   axi0_port = Sram
   axi1_port = OffChipFlash
   Sram_clock_scales = 1.0
   Sram_burst_length = 32
   Sram_read_latency = 2
   Sram_write_latency = 2
   Dram_clock_scales = 1.0
   Dram_burst_length = 1
   Dram_read_latency = 0
   Dram_write_latency = 0
   OnChipFlash_clock_scales = 1.0
   OnChipFlash_burst_length = 1
   OnChipFlash_read_latency = 0
   OnChipFlash_write_latency = 0
   OffChipFlash_clock_scales = 0.121
   OffChipFlash_burst_length = 256
   OffChipFlash_read_latency = 15
   OffChipFlash_write_latency = 0
Memory Mode (Shared_Sram):
   const_mem_area = Axi1
   arena_mem_area = Axi0
   cache_mem_area = Axi0
   arena_cache_size = 1099511627776 from Default
Architecture Settings:
   permanent_storage_mem_area = OffChipFlash
   feature_map_storage_mem_area = Sram
   fast_storage_mem_area = Sram

################################################################################
Performance for NPU Grap /tmp/tmpmdc1wgqi/output/out
Original Operator    NNG Operator         Target Staging Usage  Peak% (Staging)  Op Cycles Network% (cycles)        NPU    SRAM AC    DRAM AC OnFlash AC OffFlash AC  MAC Count Network% (MAC)  Util% (MAC) Name                 
-------------------- -------------------- ------ ------------- ---------------- ---------- ----------------- ---------- ---------- ---------- ---------- ----------- ---------- -------------- ------------ -------------------- 
Transpose            Transpose            NPU           953344            96.95    2140919             42.12    2140919      90944          0          0           0     150528           0.06         0.03 tosa_transpose_default 
Slice                MemoryCopy           NPU           952672            96.88     399751              7.86     399751      35586          0          0           0     149856           0.06         0.15 aten_slice_copy_tensor 
Conv2D               Conv2D               NPU           941648            95.76     296216              5.83     296216      64430          0          0       64793   21290688           7.95        28.08 aten_clamp_default   
Clamp                Clamp                NPU           941648            95.76     394784              7.77     394784     151650          0          0           0     788544           0.29         0.78 aten_clamp_default   
MaxPool              MaxPool              NPU           982144            99.88      48859              0.96      48859      34850          0          0           0    1742400           0.65        13.93 aten_max_pool2d_default 
Conv2D               Conv2D               NPU           243168            24.73      12760              0.25      12760       8554          0          0        2314    3097600           1.16        94.83 aten_clamp_default_1 
Clamp                Clamp                NPU           243168            24.73      25080              0.49      25080       3052          0          0           0      48400           0.02         0.75 aten_clamp_default_1 
Conv2D               Conv2D               NPU           252368            25.67     145754              2.87     145754      25762          0          0        9256   27878400          10.41        74.71 aten_clamp_default_2 
Clamp                Clamp                NPU           252368            25.67      97312              1.91      97312      17250          0          0           0     193600           0.07         0.78 aten_clamp_default_2 
Concat               MemoryCopy           NPU           629200            63.99      97167              1.91      97167      16354          0          0           0     193600           0.07         0.78 aten_cat_default     
Conv2D               Conv2D               NPU           631344            64.21      48912              0.96      48912      11489          0          0       16198    3097600           1.16        24.74 aten_clamp_default_3 
Clamp                Clamp                NPU           631344            64.21      97312              1.91      97312      12322          0          0           0     193600           0.07         0.78 aten_clamp_default_3 
Concat               MemoryCopy           NPU           580800            59.07      97167              1.91      97167      16354          0          0           0     193600           0.07         0.78 aten_cat_default     
Conv2D               Conv2D               NPU           437648            44.51      25080              0.49      25080      15484          0          0        2314    6195200           2.31        96.49 aten_clamp_default_4 
Clamp                Clamp                NPU           437648            44.51      25080              0.49      25080       3052          0          0           0      48400           0.02         0.75 aten_clamp_default_4 
Conv2D               Conv2D               NPU           252208            25.65     145754              2.87     145754      25622          0          0        9256   27878400          10.41        74.71 aten_clamp_default_5 
Clamp                Clamp                NPU           252208            25.65      97312              1.91      97312      17250          0          0           0     193600           0.07         0.78 aten_clamp_default_5 
Concat               MemoryCopy           NPU           629200            63.99      97167              1.91      97167      16354          0          0           0     193600           0.07         0.78 aten_cat_default_1   
Conv2D               Conv2D               NPU           631376            64.21      48912              0.96      48912      11538          0          0       16198    3097600           1.16        24.74 aten_clamp_default_6 
Clamp                Clamp                NPU           631376            64.21      97312              1.91      97312      12322          0          0           0     193600           0.07         0.78 aten_clamp_default_6 
Concat               MemoryCopy           NPU           580800            59.07      97167              1.91      97167      16354          0          0           0     193600           0.07         0.78 aten_cat_default_1   
MaxPool              MaxPool              NPU           480512            48.87      17316              0.34      12697      17316          0          0           0     839808           0.31        18.94 aten_max_pool2d_default_1 
Conv2D               Conv2D               NPU           120640            12.27      12528              0.25      12528       5363          0          0        2314    2985984           1.12        93.10 aten_clamp_default_7 
Clamp                Clamp                NPU           120640            12.27      12528              0.25      12528       1485          0          0           0      23328           0.01         0.73 aten_clamp_default_7 
Conv2D               Conv2D               NPU           155264            15.79     145966              2.87     145966      20594          0          0       11861   26873856          10.04        71.92 aten_clamp_default_8 
Clamp                Clamp                NPU           155264            15.79      48816              0.96      48816       7452          0          0           0      93312           0.03         0.75 aten_clamp_default_8 
Concat               MemoryCopy           NPU           303264            30.84      46960              0.92      46960       7172          0          0           0      93312           0.03         0.78 aten_cat_default_2   
Conv2D               Conv2D               NPU           310528            31.58      48816              0.96      48816       8558          0          0        9256    2985984           1.12        23.89 aten_clamp_default_9 
Clamp                Clamp                NPU           310528            31.58      48816              0.96      48816       5940          0          0           0      93312           0.03         0.75 aten_clamp_default_9 
Concat               MemoryCopy           NPU           279936            28.47      46960              0.92      46960       7172          0          0           0      93312           0.03         0.78 aten_cat_default_2   
Conv2D               Conv2D               NPU           217472            22.12      24789              0.49      24789       9927          0          0        2314    5971968           2.23        94.11 aten_clamp_default_10 
Clamp                Clamp                NPU           217472            22.12      12528              0.25      12528       1485          0          0           0      23328           0.01         0.73 aten_clamp_default_10 
Conv2D               Conv2D               NPU           155360            15.80     153453              3.02     153453      20178          0          0        4628   26873856          10.04        68.41 aten_clamp_default_11 
Clamp                Clamp                NPU           155360            15.80      48816              0.96      48816       7452          0          0           0      93312           0.03         0.75 aten_clamp_default_11 
Concat               MemoryCopy           NPU           303264            30.84      46960              0.92      46960       7172          0          0           0      93312           0.03         0.78 aten_cat_default_3   
Conv2D               Conv2D               NPU           310496            31.58      48816              0.96      48816       8544          0          0        9256    2985984           1.12        23.89 aten_clamp_default_12 
Clamp                Clamp                NPU           310496            31.58      48816              0.96      48816       5940          0          0           0      93312           0.03         0.75 aten_clamp_default_12 
Concat               MemoryCopy           NPU           279936            28.47      46960              0.92      46960       7172          0          0           0      93312           0.03         0.78 aten_cat_default_3   
MaxPool              MaxPool              NPU           229888            23.38       7184              0.14       4377       7184          0          0           0     389376           0.15        21.17 aten_max_pool2d_default_2 
Conv2D               Conv2D               NPU            62752             6.38       9173              0.18       9173       4206          0          0        3611    2076672           0.78        88.43 aten_clamp_default_13 
Clamp                Clamp                NPU            62752             6.38       4680              0.09       4680        526          0          0           0       8112           0.00         0.68 aten_clamp_default_13 
Conv2D               Conv2D               NPU            83408             8.48      79766              1.57      79766      13319          0          0       41597   14017536           5.24        68.65 aten_clamp_default_14 
Clamp                Clamp                NPU            83408             8.48      17888              0.35      17888       2574          0          0           0      32448           0.01         0.71 aten_clamp_default_14 
Concat               MemoryCopy           NPU           105456            10.72      16303              0.32      16303       2694          0          0           0      32448           0.01         0.78 aten_cat_default_4   
Conv2D               Conv2D               NPU           117760            11.98       8496              0.17       8496       4616          0          0        3966    1557504           0.58        71.61 aten_clamp_default_15 
Clamp                Clamp                NPU           117760            11.98      16736              0.33      16736       2550          0          0           0      32448           0.01         0.76 aten_clamp_default_15 
Concat               MemoryCopy           NPU            97344             9.90      16303              0.32      16303       2694          0          0           0      32448           0.01         0.78 aten_cat_default_4   
Conv2D               Conv2D               NPU            89536             9.11      13709              0.27      13709       5948          0          0        1735    3115008           1.16        88.76 aten_clamp_default_16 
Clamp                Clamp                NPU            89536             9.11       4680              0.09       4680        526          0          0           0       8112           0.00         0.68 aten_clamp_default_16 
Conv2D               Conv2D               NPU           105552            10.73      86332              1.70      86332      12966          0          0       34766   14017536           5.24        63.42 aten_clamp_default_17 
Clamp                Clamp                NPU           105552            10.73      17888              0.35      17888       2574          0          0           0      32448           0.01         0.71 aten_clamp_default_17 
Concat               MemoryCopy           NPU           105456            10.72      16303              0.32      16303       2694          0          0           0      32448           0.01         0.78 aten_cat_default_5   
Conv2D               Conv2D               NPU           117792            11.98       8496              0.17       8496       4620          0          0        3966    1557504           0.58        71.61 aten_clamp_default_18 
Clamp                Clamp                NPU           117792            11.98      16736              0.33      16736       2550          0          0           0      32448           0.01         0.76 aten_clamp_default_18 
Concat               MemoryCopy           NPU            97344             9.90      16303              0.32      16303       2694          0          0           0      32448           0.01         0.78 aten_cat_default_5   
Conv2D               Conv2D               NPU            97984             9.96      18288              0.36      18288       8034          0          0        1322    4153344           1.55        88.71 aten_clamp_default_19 
Clamp                Clamp                NPU            97984             9.96       6240              0.12       6240        754          0          0           0      10816           0.00         0.68 aten_clamp_default_19 
Conv2D               Conv2D               NPU           168896            17.18     149781              2.95     149781      22902          0          0       65818   24920064           9.31        64.99 aten_clamp_default_20 
Clamp                Clamp                NPU           168896            17.18      23712              0.47      23712       3432          0          0           0      43264           0.02         0.71 aten_clamp_default_20 
Concat               MemoryCopy           NPU           140608            14.30      21808              0.43      21808       3816          0          0           0      43264           0.02         0.77 aten_cat_default_6   
Conv2D               Conv2D               NPU           159072            16.18      12064              0.24      12064       9002          0          0        2644    2768896           1.03        89.66 aten_clamp_default_21 
Clamp                Clamp                NPU           159072            16.18      23712              0.47      23712       3016          0          0           0      43264           0.02         0.71 aten_clamp_default_21 
Concat               MemoryCopy           NPU           129792            13.20      21808              0.43      21808       3816          0          0           0      43264           0.02         0.77 aten_cat_default_6   
Conv2D               Conv2D               NPU           127072            12.92      24336              0.48      24336      10630          0          0        1322    5537792           2.07        88.89 aten_clamp_default_22 
Clamp                Clamp                NPU           127072            12.92       6240              0.12       6240        754          0          0           0      10816           0.00         0.68 aten_clamp_default_22 
Conv2D               Conv2D               NPU           171696            17.46     155210              3.05     155210      22917          0          0       62353   24920064           9.31        62.72 aten_clamp_default_23 
Clamp                Clamp                NPU           171696            17.46      23712              0.47      23712       3432          0          0           0      43264           0.02         0.71 aten_clamp_default_23 
Concat               MemoryCopy           NPU           140608            14.30      21808              0.43      21808       3816          0          0           0      43264           0.02         0.77 aten_cat_default_7   
Conv2D               Conv2D               NPU           159408            16.21      12064              0.24      12064       9023          0          0        2644    2768896           1.03        89.66 aten_clamp_default_24 
Clamp                Clamp                NPU           159408            16.21      23712              0.47      23712       3016          0          0           0      43264           0.02         0.71 aten_clamp_default_24 
Concat               MemoryCopy           NPU           129792            13.20      21808              0.43      21808       3816          0          0           0      43264           0.02         0.77 aten_cat_default_7   
Conv2D               Conv2D               NPU            91808             9.34       3563              0.07       3442       3563          0          0         165     346112           0.13        37.95 aten_clamp_default_25 
Clamp                Clamp                NPU            91808             9.34        873              0.02        873        188          0          0           0        676           0.00         0.30 aten_clamp_default_25 
AvgPool              AvgPool              NPU             2720             0.28        858              0.02        858        160          0          0           0        676           0.00         0.31 aten_view_copy_default 

Network summary for out
Accelerator configuration               Ethos_U85_256
System configuration              Ethos_U85_SRAM_MRAM
Memory mode                               Shared_Sram
Accelerator clock                                 400 MHz
Design peak SRAM bandwidth                      11.92 GB/s
Design peak Off-chip Flash bandwidth             0.72 GB/s

Total SRAM used                                960.27 KiB
Total Off-chip Flash used                      743.17 KiB

CPU operators = 0 (0.0%)
NPU operators = 48 (100.0%)

Average SRAM bandwidth                           1.67 GB/s
Input   SRAM bandwidth                          11.68 MB/batch
Weight  SRAM bandwidth                           4.17 MB/batch
Output  SRAM bandwidth                           5.06 MB/batch
Total   SRAM bandwidth                          21.25 MB/batch
Total   SRAM bandwidth            per input     21.25 MB/inference (batch size 1)

Average Off-chip Flash bandwidth                 0.06 GB/s
Input   Off-chip Flash bandwidth                 0.00 MB/batch
Weight  Off-chip Flash bandwidth                 0.73 MB/batch
Output  Off-chip Flash bandwidth                 0.00 MB/batch
Total   Off-chip Flash bandwidth                 0.73 MB/batch
Total   Off-chip Flash bandwidth  per input      0.73 MB/inference (batch size 1)

Original Weights Size                          704.69 KiB
NPU Encoded Weights Size                       714.27 KiB

Neural network macs                         267693188 MACs/batch

Info: The numbers below are internal compiler estimates.
For performance numbers the compiled network should be run on an FVP Model or FPGA.

Network Tops/s                                   0.04 Tops/s

NPU cycles                                    5075326 cycles/batch
SRAM Access cycles                             673972 cycles/batch
DRAM Access cycles                                  0 cycles/batch
On-chip Flash Access cycles                         0 cycles/batch
Off-chip Flash Access cycles                   385864 cycles/batch
Total cycles                                  5082873 cycles/batch

Batch Inference time                12.71 ms,   78.70 inferences/s (batch size 1)

class GraphModule(torch.nn.Module):
    def forward(self, x):
        x: "f32[1, 3, 224, 224]"; 
    
        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
        # No stacktrace found for following nodes
        alloc: "i8[1, 3, 224, 224]" = executorch_exir_memory_alloc(((1, 3, 224, 224), torch.int8))
        quantized_decomposed_quantize_per_tensor_default: "i8[1, 3, 224, 224]" = torch.ops.quantized_decomposed.quantize_per_tensor.out(x, 0.03172215074300766, 8, -128, 127, torch.int8, out = alloc);  x = alloc = None
        lowered_module_0 = self.lowered_module_0
        executorch_call_delegate = torch.ops.higher_order.executorch_call_delegate(lowered_module_0, quantized_decomposed_quantize_per_tensor_default);  lowered_module_0 = quantized_decomposed_quantize_per_tensor_default = None
        getitem: "i8[1, 4]" = executorch_call_delegate[0];  executorch_call_delegate = None
        alloc_1: "f32[1, 4]" = executorch_exir_memory_alloc(((1, 4), torch.float32))
        quantized_decomposed_dequantize_per_tensor_default: "f32[1, 4]" = torch.ops.quantized_decomposed.dequantize_per_tensor.out(getitem, 0.18244418501853943, -55, -128, 127, torch.int8, out = alloc_1);  getitem = alloc_1 = None
        return pytree.tree_unflatten((quantized_decomposed_dequantize_per_tensor_default,), self._out_spec)
        
