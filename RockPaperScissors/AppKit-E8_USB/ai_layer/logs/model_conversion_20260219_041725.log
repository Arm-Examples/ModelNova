Loading weights from: /workspace2/model/alif_sensor_25epochs.pth
class GraphModule(torch.nn.Module):
    def forward(self, x):
        x: "f32[1, 3, 224, 224]"; 
    
        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
        # No stacktrace found for following nodes
        features_0_weight: "f32[64, 3, 3, 3]" = getattr(self.features, "0").weight
        features_0_bias: "f32[64]" = getattr(self.features, "0").bias
        features_3_squeeze_weight: "f32[16, 64, 1, 1]" = getattr(self.features, "3").squeeze.weight
        features_3_squeeze_bias: "f32[16]" = getattr(self.features, "3").squeeze.bias
        features_3_expand1x1_weight: "f32[64, 16, 1, 1]" = getattr(self.features, "3").expand1x1.weight
        features_3_expand1x1_bias: "f32[64]" = getattr(self.features, "3").expand1x1.bias
        features_3_expand3x3_weight: "f32[64, 16, 3, 3]" = getattr(self.features, "3").expand3x3.weight
        features_3_expand3x3_bias: "f32[64]" = getattr(self.features, "3").expand3x3.bias
        features_4_squeeze_weight: "f32[16, 128, 1, 1]" = getattr(self.features, "4").squeeze.weight
        features_4_squeeze_bias: "f32[16]" = getattr(self.features, "4").squeeze.bias
        features_4_expand1x1_weight: "f32[64, 16, 1, 1]" = getattr(self.features, "4").expand1x1.weight
        features_4_expand1x1_bias: "f32[64]" = getattr(self.features, "4").expand1x1.bias
        features_4_expand3x3_weight: "f32[64, 16, 3, 3]" = getattr(self.features, "4").expand3x3.weight
        features_4_expand3x3_bias: "f32[64]" = getattr(self.features, "4").expand3x3.bias
        features_6_squeeze_weight: "f32[32, 128, 1, 1]" = getattr(self.features, "6").squeeze.weight
        features_6_squeeze_bias: "f32[32]" = getattr(self.features, "6").squeeze.bias
        features_6_expand1x1_weight: "f32[128, 32, 1, 1]" = getattr(self.features, "6").expand1x1.weight
        features_6_expand1x1_bias: "f32[128]" = getattr(self.features, "6").expand1x1.bias
        features_6_expand3x3_weight: "f32[128, 32, 3, 3]" = getattr(self.features, "6").expand3x3.weight
        features_6_expand3x3_bias: "f32[128]" = getattr(self.features, "6").expand3x3.bias
        features_7_squeeze_weight: "f32[32, 256, 1, 1]" = getattr(self.features, "7").squeeze.weight
        features_7_squeeze_bias: "f32[32]" = getattr(self.features, "7").squeeze.bias
        features_7_expand1x1_weight: "f32[128, 32, 1, 1]" = getattr(self.features, "7").expand1x1.weight
        features_7_expand1x1_bias: "f32[128]" = getattr(self.features, "7").expand1x1.bias
        features_7_expand3x3_weight: "f32[128, 32, 3, 3]" = getattr(self.features, "7").expand3x3.weight
        features_7_expand3x3_bias: "f32[128]" = getattr(self.features, "7").expand3x3.bias
        features_9_squeeze_weight: "f32[48, 256, 1, 1]" = getattr(self.features, "9").squeeze.weight
        features_9_squeeze_bias: "f32[48]" = getattr(self.features, "9").squeeze.bias
        features_9_expand1x1_weight: "f32[192, 48, 1, 1]" = getattr(self.features, "9").expand1x1.weight
        features_9_expand1x1_bias: "f32[192]" = getattr(self.features, "9").expand1x1.bias
        features_9_expand3x3_weight: "f32[192, 48, 3, 3]" = getattr(self.features, "9").expand3x3.weight
        features_9_expand3x3_bias: "f32[192]" = getattr(self.features, "9").expand3x3.bias
        features_10_squeeze_weight: "f32[48, 384, 1, 1]" = getattr(self.features, "10").squeeze.weight
        features_10_squeeze_bias: "f32[48]" = getattr(self.features, "10").squeeze.bias
        features_10_expand1x1_weight: "f32[192, 48, 1, 1]" = getattr(self.features, "10").expand1x1.weight
        features_10_expand1x1_bias: "f32[192]" = getattr(self.features, "10").expand1x1.bias
        features_10_expand3x3_weight: "f32[192, 48, 3, 3]" = getattr(self.features, "10").expand3x3.weight
        features_10_expand3x3_bias: "f32[192]" = getattr(self.features, "10").expand3x3.bias
        features_11_squeeze_weight: "f32[64, 384, 1, 1]" = getattr(self.features, "11").squeeze.weight
        features_11_squeeze_bias: "f32[64]" = getattr(self.features, "11").squeeze.bias
        features_11_expand1x1_weight: "f32[256, 64, 1, 1]" = getattr(self.features, "11").expand1x1.weight
        features_11_expand1x1_bias: "f32[256]" = getattr(self.features, "11").expand1x1.bias
        features_11_expand3x3_weight: "f32[256, 64, 3, 3]" = getattr(self.features, "11").expand3x3.weight
        features_11_expand3x3_bias: "f32[256]" = getattr(self.features, "11").expand3x3.bias
        features_12_squeeze_weight: "f32[64, 512, 1, 1]" = getattr(self.features, "12").squeeze.weight
        features_12_squeeze_bias: "f32[64]" = getattr(self.features, "12").squeeze.bias
        features_12_expand1x1_weight: "f32[256, 64, 1, 1]" = getattr(self.features, "12").expand1x1.weight
        features_12_expand1x1_bias: "f32[256]" = getattr(self.features, "12").expand1x1.bias
        features_12_expand3x3_weight: "f32[256, 64, 3, 3]" = getattr(self.features, "12").expand3x3.weight
        features_12_expand3x3_bias: "f32[256]" = getattr(self.features, "12").expand3x3.bias
        classifier_1_weight: "f32[4, 512, 1, 1]" = getattr(self.classifier, "1").weight
        classifier_1_bias: "f32[4]" = getattr(self.classifier, "1").bias
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d: "f32[1, 64, 111, 111]" = torch.ops.aten.conv2d.default(x, features_0_weight, features_0_bias, [2, 2]);  x = features_0_weight = features_0_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu_: "f32[1, 64, 111, 111]" = torch.ops.aten.relu_.default(conv2d);  conv2d = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        max_pool2d: "f32[1, 64, 55, 55]" = torch.ops.aten.max_pool2d.default(relu_, [3, 3], [2, 2], [0, 0], [1, 1], True);  relu_ = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_1: "f32[1, 16, 55, 55]" = torch.ops.aten.conv2d.default(max_pool2d, features_3_squeeze_weight, features_3_squeeze_bias);  max_pool2d = features_3_squeeze_weight = features_3_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__1: "f32[1, 16, 55, 55]" = torch.ops.aten.relu_.default(conv2d_1);  conv2d_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_2: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(relu__1, features_3_expand1x1_weight, features_3_expand1x1_bias);  features_3_expand1x1_weight = features_3_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__2: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(conv2d_2);  conv2d_2 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_3: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(relu__1, features_3_expand3x3_weight, features_3_expand3x3_bias, [1, 1], [1, 1]);  relu__1 = features_3_expand3x3_weight = features_3_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__3: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(conv2d_3);  conv2d_3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat: "f32[1, 128, 55, 55]" = torch.ops.aten.cat.default([relu__2, relu__3], 1);  relu__2 = relu__3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_4: "f32[1, 16, 55, 55]" = torch.ops.aten.conv2d.default(cat, features_4_squeeze_weight, features_4_squeeze_bias);  cat = features_4_squeeze_weight = features_4_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__4: "f32[1, 16, 55, 55]" = torch.ops.aten.relu_.default(conv2d_4);  conv2d_4 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_5: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(relu__4, features_4_expand1x1_weight, features_4_expand1x1_bias);  features_4_expand1x1_weight = features_4_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__5: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(conv2d_5);  conv2d_5 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_6: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(relu__4, features_4_expand3x3_weight, features_4_expand3x3_bias, [1, 1], [1, 1]);  relu__4 = features_4_expand3x3_weight = features_4_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__6: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(conv2d_6);  conv2d_6 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_1: "f32[1, 128, 55, 55]" = torch.ops.aten.cat.default([relu__5, relu__6], 1);  relu__5 = relu__6 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        max_pool2d_1: "f32[1, 128, 27, 27]" = torch.ops.aten.max_pool2d.default(cat_1, [3, 3], [2, 2], [0, 0], [1, 1], True);  cat_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_7: "f32[1, 32, 27, 27]" = torch.ops.aten.conv2d.default(max_pool2d_1, features_6_squeeze_weight, features_6_squeeze_bias);  max_pool2d_1 = features_6_squeeze_weight = features_6_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__7: "f32[1, 32, 27, 27]" = torch.ops.aten.relu_.default(conv2d_7);  conv2d_7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_8: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(relu__7, features_6_expand1x1_weight, features_6_expand1x1_bias);  features_6_expand1x1_weight = features_6_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__8: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(conv2d_8);  conv2d_8 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_9: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(relu__7, features_6_expand3x3_weight, features_6_expand3x3_bias, [1, 1], [1, 1]);  relu__7 = features_6_expand3x3_weight = features_6_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__9: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(conv2d_9);  conv2d_9 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_2: "f32[1, 256, 27, 27]" = torch.ops.aten.cat.default([relu__8, relu__9], 1);  relu__8 = relu__9 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_10: "f32[1, 32, 27, 27]" = torch.ops.aten.conv2d.default(cat_2, features_7_squeeze_weight, features_7_squeeze_bias);  cat_2 = features_7_squeeze_weight = features_7_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__10: "f32[1, 32, 27, 27]" = torch.ops.aten.relu_.default(conv2d_10);  conv2d_10 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_11: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(relu__10, features_7_expand1x1_weight, features_7_expand1x1_bias);  features_7_expand1x1_weight = features_7_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__11: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(conv2d_11);  conv2d_11 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_12: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(relu__10, features_7_expand3x3_weight, features_7_expand3x3_bias, [1, 1], [1, 1]);  relu__10 = features_7_expand3x3_weight = features_7_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__12: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(conv2d_12);  conv2d_12 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_3: "f32[1, 256, 27, 27]" = torch.ops.aten.cat.default([relu__11, relu__12], 1);  relu__11 = relu__12 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        max_pool2d_2: "f32[1, 256, 13, 13]" = torch.ops.aten.max_pool2d.default(cat_3, [3, 3], [2, 2], [0, 0], [1, 1], True);  cat_3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_13: "f32[1, 48, 13, 13]" = torch.ops.aten.conv2d.default(max_pool2d_2, features_9_squeeze_weight, features_9_squeeze_bias);  max_pool2d_2 = features_9_squeeze_weight = features_9_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__13: "f32[1, 48, 13, 13]" = torch.ops.aten.relu_.default(conv2d_13);  conv2d_13 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_14: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(relu__13, features_9_expand1x1_weight, features_9_expand1x1_bias);  features_9_expand1x1_weight = features_9_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__14: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(conv2d_14);  conv2d_14 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_15: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(relu__13, features_9_expand3x3_weight, features_9_expand3x3_bias, [1, 1], [1, 1]);  relu__13 = features_9_expand3x3_weight = features_9_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__15: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(conv2d_15);  conv2d_15 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_4: "f32[1, 384, 13, 13]" = torch.ops.aten.cat.default([relu__14, relu__15], 1);  relu__14 = relu__15 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_16: "f32[1, 48, 13, 13]" = torch.ops.aten.conv2d.default(cat_4, features_10_squeeze_weight, features_10_squeeze_bias);  cat_4 = features_10_squeeze_weight = features_10_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__16: "f32[1, 48, 13, 13]" = torch.ops.aten.relu_.default(conv2d_16);  conv2d_16 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_17: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(relu__16, features_10_expand1x1_weight, features_10_expand1x1_bias);  features_10_expand1x1_weight = features_10_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__17: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(conv2d_17);  conv2d_17 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_18: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(relu__16, features_10_expand3x3_weight, features_10_expand3x3_bias, [1, 1], [1, 1]);  relu__16 = features_10_expand3x3_weight = features_10_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__18: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(conv2d_18);  conv2d_18 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_5: "f32[1, 384, 13, 13]" = torch.ops.aten.cat.default([relu__17, relu__18], 1);  relu__17 = relu__18 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_19: "f32[1, 64, 13, 13]" = torch.ops.aten.conv2d.default(cat_5, features_11_squeeze_weight, features_11_squeeze_bias);  cat_5 = features_11_squeeze_weight = features_11_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__19: "f32[1, 64, 13, 13]" = torch.ops.aten.relu_.default(conv2d_19);  conv2d_19 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_20: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(relu__19, features_11_expand1x1_weight, features_11_expand1x1_bias);  features_11_expand1x1_weight = features_11_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__20: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(conv2d_20);  conv2d_20 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_21: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(relu__19, features_11_expand3x3_weight, features_11_expand3x3_bias, [1, 1], [1, 1]);  relu__19 = features_11_expand3x3_weight = features_11_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__21: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(conv2d_21);  conv2d_21 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_6: "f32[1, 512, 13, 13]" = torch.ops.aten.cat.default([relu__20, relu__21], 1);  relu__20 = relu__21 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_22: "f32[1, 64, 13, 13]" = torch.ops.aten.conv2d.default(cat_6, features_12_squeeze_weight, features_12_squeeze_bias);  cat_6 = features_12_squeeze_weight = features_12_squeeze_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__22: "f32[1, 64, 13, 13]" = torch.ops.aten.relu_.default(conv2d_22);  conv2d_22 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_23: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(relu__22, features_12_expand1x1_weight, features_12_expand1x1_bias);  features_12_expand1x1_weight = features_12_expand1x1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__23: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(conv2d_23);  conv2d_23 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_24: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(relu__22, features_12_expand3x3_weight, features_12_expand3x3_bias, [1, 1], [1, 1]);  relu__22 = features_12_expand3x3_weight = features_12_expand3x3_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__24: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(conv2d_24);  conv2d_24 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_7: "f32[1, 512, 13, 13]" = torch.ops.aten.cat.default([relu__23, relu__24], 1);  relu__23 = relu__24 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
        dropout: "f32[1, 512, 13, 13]" = torch.ops.aten.dropout.default(cat_7, 0.5, False);  cat_7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        conv2d_25: "f32[1, 4, 13, 13]" = torch.ops.aten.conv2d.default(dropout, classifier_1_weight, classifier_1_bias);  dropout = classifier_1_weight = classifier_1_bias = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        relu__25: "f32[1, 4, 13, 13]" = torch.ops.aten.relu_.default(conv2d_25);  conv2d_25 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:1500 in forward, code: return F.adaptive_avg_pool2d(input, self.output_size)
        adaptive_avg_pool2d: "f32[1, 4, 1, 1]" = torch.ops.aten.adaptive_avg_pool2d.default(relu__25, [1, 1]);  relu__25 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:97 in forward, code: return torch.flatten(x, 1)
        flatten: "f32[1, 4]" = torch.ops.aten.flatten.using_ints(adaptive_avg_pool2d, 1);  adaptive_avg_pool2d = None
        return pytree.tree_unflatten((flatten,), self._out_spec)
        Skipping import of cpp extensions due to incompatible torch version 2.9.0+cpu for torchao version 0.14.0+cpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
/workspace/executorch-venv/lib/python3.12/site-packages/executorch/backends/arm/quantizer/quantization_config.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(act_scale * weight_scale).to(

class GraphModule(torch.nn.Module):
    def forward(self, x):
        x: "f32[1, 3, 224, 224]"; 
    
        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
        # No stacktrace found for following nodes
        quantize_per_tensor_default = torch.ops.quantized_decomposed.quantize_per_tensor.default(x, 0.03026646561920643, -1, -128, 127, torch.int8);  x = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default, 0.03026646561920643, -1, -128, 127, torch.int8);  quantize_per_tensor_default = None
        
        # No stacktrace found for following nodes
        _scale_0 = self._scale_0
        _zero_point_0 = self._zero_point_0
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default = self._frozen_param0
        dequantize_per_channel_default = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default, _scale_0, _zero_point_0, 0, -127, 127, torch.int8);  quantize_per_channel_default = _scale_0 = _zero_point_0 = None
        
        # No stacktrace found for following nodes
        _scale_1 = self._scale_1
        _zero_point_1 = self._zero_point_1
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_1 = self._frozen_param1
        dequantize_per_channel_default_1 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_1, _scale_1, _zero_point_1, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_1 = _scale_1 = _zero_point_1 = None
        conv2d: "f32[1, 64, 111, 111]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default, dequantize_per_channel_default, dequantize_per_channel_default_1, [2, 2]);  dequantize_per_tensor_default = dequantize_per_channel_default = dequantize_per_channel_default_1 = None
        quantize_per_tensor_default_1 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d, 0.06461931765079498, -19, -128, 127, torch.int8);  conv2d = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_1 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_1, 0.06461931765079498, -19, -128, 127, torch.int8);  quantize_per_tensor_default_1 = None
        relu_: "f32[1, 64, 111, 111]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_1);  dequantize_per_tensor_default_1 = None
        quantize_per_tensor_default_2 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu_, 0.06461931765079498, -19, -128, 127, torch.int8);  relu_ = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        dequantize_per_tensor_default_2 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_2, 0.06461931765079498, -19, -128, 127, torch.int8);  quantize_per_tensor_default_2 = None
        max_pool2d: "f32[1, 64, 55, 55]" = torch.ops.aten.max_pool2d.default(dequantize_per_tensor_default_2, [3, 3], [2, 2], [0, 0], [1, 1], True);  dequantize_per_tensor_default_2 = None
        quantize_per_tensor_default_3 = torch.ops.quantized_decomposed.quantize_per_tensor.default(max_pool2d, 0.06461931765079498, -19, -128, 127, torch.int8);  max_pool2d = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_3 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_3, 0.06461931765079498, -19, -128, 127, torch.int8);  quantize_per_tensor_default_3 = None
        
        # No stacktrace found for following nodes
        _scale_2 = self._scale_2
        _zero_point_2 = self._zero_point_2
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_2 = self._frozen_param2
        dequantize_per_channel_default_2 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_2, _scale_2, _zero_point_2, 0, -127, 127, torch.int8);  quantize_per_channel_default_2 = _scale_2 = _zero_point_2 = None
        
        # No stacktrace found for following nodes
        _scale_3 = self._scale_3
        _zero_point_3 = self._zero_point_3
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_3 = self._frozen_param3
        dequantize_per_channel_default_3 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_3, _scale_3, _zero_point_3, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_3 = _scale_3 = _zero_point_3 = None
        conv2d_1: "f32[1, 16, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_3, dequantize_per_channel_default_2, dequantize_per_channel_default_3);  dequantize_per_tensor_default_3 = dequantize_per_channel_default_2 = dequantize_per_channel_default_3 = None
        quantize_per_tensor_default_4 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_1, 0.17291930317878723, 6, -128, 127, torch.int8);  conv2d_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_4 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_4, 0.17291930317878723, 6, -128, 127, torch.int8);  quantize_per_tensor_default_4 = None
        relu__1: "f32[1, 16, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_4);  dequantize_per_tensor_default_4 = None
        quantize_per_tensor_default_5 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__1, 0.17291930317878723, 6, -128, 127, torch.int8);  relu__1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_68 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_5, 0.17291930317878723, 6, -128, 127, torch.int8)
        dequantize_per_tensor_default_67 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_5, 0.17291930317878723, 6, -128, 127, torch.int8);  quantize_per_tensor_default_5 = None
        
        # No stacktrace found for following nodes
        _scale_4 = self._scale_4
        _zero_point_4 = self._zero_point_4
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_4 = self._frozen_param4
        dequantize_per_channel_default_4 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_4, _scale_4, _zero_point_4, 0, -127, 127, torch.int8);  quantize_per_channel_default_4 = _scale_4 = _zero_point_4 = None
        
        # No stacktrace found for following nodes
        _scale_5 = self._scale_5
        _zero_point_5 = self._zero_point_5
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_5 = self._frozen_param5
        dequantize_per_channel_default_5 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_5, _scale_5, _zero_point_5, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_5 = _scale_5 = _zero_point_5 = None
        conv2d_2: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_67, dequantize_per_channel_default_4, dequantize_per_channel_default_5);  dequantize_per_tensor_default_67 = dequantize_per_channel_default_4 = dequantize_per_channel_default_5 = None
        quantize_per_tensor_default_6 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_2, 0.21439144015312195, 41, -128, 127, torch.int8);  conv2d_2 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_6 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_6, 0.21439144015312195, 41, -128, 127, torch.int8);  quantize_per_tensor_default_6 = None
        relu__2: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_6);  dequantize_per_tensor_default_6 = None
        quantize_per_tensor_default_7 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__2, 0.21439144015312195, 41, -128, 127, torch.int8);  relu__2 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_7 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_7, 0.21439144015312195, 41, -128, 127, torch.int8);  quantize_per_tensor_default_7 = None
        _scale_6 = self._scale_6
        _zero_point_6 = self._zero_point_6
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_6 = self._frozen_param6
        dequantize_per_channel_default_6 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_6, _scale_6, _zero_point_6, 0, -127, 127, torch.int8);  quantize_per_channel_default_6 = _scale_6 = _zero_point_6 = None
        
        # No stacktrace found for following nodes
        _scale_7 = self._scale_7
        _zero_point_7 = self._zero_point_7
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_7 = self._frozen_param7
        dequantize_per_channel_default_7 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_7, _scale_7, _zero_point_7, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_7 = _scale_7 = _zero_point_7 = None
        conv2d_3: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_68, dequantize_per_channel_default_6, dequantize_per_channel_default_7, [1, 1], [1, 1]);  dequantize_per_tensor_default_68 = dequantize_per_channel_default_6 = dequantize_per_channel_default_7 = None
        quantize_per_tensor_default_8 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_3, 0.21439144015312195, 41, -128, 127, torch.int8);  conv2d_3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_8 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_8, 0.21439144015312195, 41, -128, 127, torch.int8);  quantize_per_tensor_default_8 = None
        relu__3: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_8);  dequantize_per_tensor_default_8 = None
        quantize_per_tensor_default_9 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__3, 0.21439144015312195, 41, -128, 127, torch.int8);  relu__3 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_9 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_9, 0.21439144015312195, 41, -128, 127, torch.int8);  quantize_per_tensor_default_9 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat: "f32[1, 128, 55, 55]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_7, dequantize_per_tensor_default_9], 1);  dequantize_per_tensor_default_7 = dequantize_per_tensor_default_9 = None
        quantize_per_tensor_default_10 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat, 0.21439144015312195, 41, -128, 127, torch.int8);  cat = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_10 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_10, 0.21439144015312195, 41, -128, 127, torch.int8);  quantize_per_tensor_default_10 = None
        
        # No stacktrace found for following nodes
        _scale_8 = self._scale_8
        _zero_point_8 = self._zero_point_8
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_8 = self._frozen_param8
        dequantize_per_channel_default_8 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_8, _scale_8, _zero_point_8, 0, -127, 127, torch.int8);  quantize_per_channel_default_8 = _scale_8 = _zero_point_8 = None
        
        # No stacktrace found for following nodes
        _scale_9 = self._scale_9
        _zero_point_9 = self._zero_point_9
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_9 = self._frozen_param9
        dequantize_per_channel_default_9 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_9, _scale_9, _zero_point_9, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_9 = _scale_9 = _zero_point_9 = None
        conv2d_4: "f32[1, 16, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_10, dequantize_per_channel_default_8, dequantize_per_channel_default_9);  dequantize_per_tensor_default_10 = dequantize_per_channel_default_8 = dequantize_per_channel_default_9 = None
        quantize_per_tensor_default_11 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_4, 0.20491719245910645, -24, -128, 127, torch.int8);  conv2d_4 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_11 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_11, 0.20491719245910645, -24, -128, 127, torch.int8);  quantize_per_tensor_default_11 = None
        relu__4: "f32[1, 16, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_11);  dequantize_per_tensor_default_11 = None
        quantize_per_tensor_default_12 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__4, 0.20491719245910645, -24, -128, 127, torch.int8);  relu__4 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_70 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_12, 0.20491719245910645, -24, -128, 127, torch.int8)
        dequantize_per_tensor_default_69 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_12, 0.20491719245910645, -24, -128, 127, torch.int8);  quantize_per_tensor_default_12 = None
        
        # No stacktrace found for following nodes
        _scale_10 = self._scale_10
        _zero_point_10 = self._zero_point_10
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_10 = self._frozen_param10
        dequantize_per_channel_default_10 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_10, _scale_10, _zero_point_10, 0, -127, 127, torch.int8);  quantize_per_channel_default_10 = _scale_10 = _zero_point_10 = None
        
        # No stacktrace found for following nodes
        _scale_11 = self._scale_11
        _zero_point_11 = self._zero_point_11
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_11 = self._frozen_param11
        dequantize_per_channel_default_11 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_11, _scale_11, _zero_point_11, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_11 = _scale_11 = _zero_point_11 = None
        conv2d_5: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_69, dequantize_per_channel_default_10, dequantize_per_channel_default_11);  dequantize_per_tensor_default_69 = dequantize_per_channel_default_10 = dequantize_per_channel_default_11 = None
        quantize_per_tensor_default_13 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_5, 0.27028554677963257, 19, -128, 127, torch.int8);  conv2d_5 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_13 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_13, 0.27028554677963257, 19, -128, 127, torch.int8);  quantize_per_tensor_default_13 = None
        relu__5: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_13);  dequantize_per_tensor_default_13 = None
        quantize_per_tensor_default_14 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__5, 0.27028554677963257, 19, -128, 127, torch.int8);  relu__5 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_14 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_14, 0.27028554677963257, 19, -128, 127, torch.int8);  quantize_per_tensor_default_14 = None
        _scale_12 = self._scale_12
        _zero_point_12 = self._zero_point_12
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_12 = self._frozen_param12
        dequantize_per_channel_default_12 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_12, _scale_12, _zero_point_12, 0, -127, 127, torch.int8);  quantize_per_channel_default_12 = _scale_12 = _zero_point_12 = None
        
        # No stacktrace found for following nodes
        _scale_13 = self._scale_13
        _zero_point_13 = self._zero_point_13
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_13 = self._frozen_param13
        dequantize_per_channel_default_13 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_13, _scale_13, _zero_point_13, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_13 = _scale_13 = _zero_point_13 = None
        conv2d_6: "f32[1, 64, 55, 55]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_70, dequantize_per_channel_default_12, dequantize_per_channel_default_13, [1, 1], [1, 1]);  dequantize_per_tensor_default_70 = dequantize_per_channel_default_12 = dequantize_per_channel_default_13 = None
        quantize_per_tensor_default_15 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_6, 0.27028554677963257, 19, -128, 127, torch.int8);  conv2d_6 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_15 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_15, 0.27028554677963257, 19, -128, 127, torch.int8);  quantize_per_tensor_default_15 = None
        relu__6: "f32[1, 64, 55, 55]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_15);  dequantize_per_tensor_default_15 = None
        quantize_per_tensor_default_16 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__6, 0.27028554677963257, 19, -128, 127, torch.int8);  relu__6 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_16 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_16, 0.27028554677963257, 19, -128, 127, torch.int8);  quantize_per_tensor_default_16 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_1: "f32[1, 128, 55, 55]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_14, dequantize_per_tensor_default_16], 1);  dequantize_per_tensor_default_14 = dequantize_per_tensor_default_16 = None
        quantize_per_tensor_default_17 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_1, 0.27028554677963257, 19, -128, 127, torch.int8);  cat_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        dequantize_per_tensor_default_17 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_17, 0.27028554677963257, 19, -128, 127, torch.int8);  quantize_per_tensor_default_17 = None
        max_pool2d_1: "f32[1, 128, 27, 27]" = torch.ops.aten.max_pool2d.default(dequantize_per_tensor_default_17, [3, 3], [2, 2], [0, 0], [1, 1], True);  dequantize_per_tensor_default_17 = None
        quantize_per_tensor_default_18 = torch.ops.quantized_decomposed.quantize_per_tensor.default(max_pool2d_1, 0.27028554677963257, 19, -128, 127, torch.int8);  max_pool2d_1 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_18 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_18, 0.27028554677963257, 19, -128, 127, torch.int8);  quantize_per_tensor_default_18 = None
        
        # No stacktrace found for following nodes
        _scale_14 = self._scale_14
        _zero_point_14 = self._zero_point_14
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_14 = self._frozen_param14
        dequantize_per_channel_default_14 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_14, _scale_14, _zero_point_14, 0, -127, 127, torch.int8);  quantize_per_channel_default_14 = _scale_14 = _zero_point_14 = None
        
        # No stacktrace found for following nodes
        _scale_15 = self._scale_15
        _zero_point_15 = self._zero_point_15
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_15 = self._frozen_param15
        dequantize_per_channel_default_15 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_15, _scale_15, _zero_point_15, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_15 = _scale_15 = _zero_point_15 = None
        conv2d_7: "f32[1, 32, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_18, dequantize_per_channel_default_14, dequantize_per_channel_default_15);  dequantize_per_tensor_default_18 = dequantize_per_channel_default_14 = dequantize_per_channel_default_15 = None
        quantize_per_tensor_default_19 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_7, 0.6366238594055176, -37, -128, 127, torch.int8);  conv2d_7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_19 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_19, 0.6366238594055176, -37, -128, 127, torch.int8);  quantize_per_tensor_default_19 = None
        relu__7: "f32[1, 32, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_19);  dequantize_per_tensor_default_19 = None
        quantize_per_tensor_default_20 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__7, 0.6366238594055176, -37, -128, 127, torch.int8);  relu__7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_72 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_20, 0.6366238594055176, -37, -128, 127, torch.int8)
        dequantize_per_tensor_default_71 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_20, 0.6366238594055176, -37, -128, 127, torch.int8);  quantize_per_tensor_default_20 = None
        
        # No stacktrace found for following nodes
        _scale_16 = self._scale_16
        _zero_point_16 = self._zero_point_16
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_16 = self._frozen_param16
        dequantize_per_channel_default_16 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_16, _scale_16, _zero_point_16, 0, -127, 127, torch.int8);  quantize_per_channel_default_16 = _scale_16 = _zero_point_16 = None
        
        # No stacktrace found for following nodes
        _scale_17 = self._scale_17
        _zero_point_17 = self._zero_point_17
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_17 = self._frozen_param17
        dequantize_per_channel_default_17 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_17, _scale_17, _zero_point_17, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_17 = _scale_17 = _zero_point_17 = None
        conv2d_8: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_71, dequantize_per_channel_default_16, dequantize_per_channel_default_17);  dequantize_per_tensor_default_71 = dequantize_per_channel_default_16 = dequantize_per_channel_default_17 = None
        quantize_per_tensor_default_21 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_8, 0.8618559241294861, 16, -128, 127, torch.int8);  conv2d_8 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_21 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_21, 0.8618559241294861, 16, -128, 127, torch.int8);  quantize_per_tensor_default_21 = None
        relu__8: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_21);  dequantize_per_tensor_default_21 = None
        quantize_per_tensor_default_22 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__8, 0.8618559241294861, 16, -128, 127, torch.int8);  relu__8 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_22 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_22, 0.8618559241294861, 16, -128, 127, torch.int8);  quantize_per_tensor_default_22 = None
        _scale_18 = self._scale_18
        _zero_point_18 = self._zero_point_18
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_18 = self._frozen_param18
        dequantize_per_channel_default_18 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_18, _scale_18, _zero_point_18, 0, -127, 127, torch.int8);  quantize_per_channel_default_18 = _scale_18 = _zero_point_18 = None
        
        # No stacktrace found for following nodes
        _scale_19 = self._scale_19
        _zero_point_19 = self._zero_point_19
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_19 = self._frozen_param19
        dequantize_per_channel_default_19 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_19, _scale_19, _zero_point_19, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_19 = _scale_19 = _zero_point_19 = None
        conv2d_9: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_72, dequantize_per_channel_default_18, dequantize_per_channel_default_19, [1, 1], [1, 1]);  dequantize_per_tensor_default_72 = dequantize_per_channel_default_18 = dequantize_per_channel_default_19 = None
        quantize_per_tensor_default_23 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_9, 0.8618559241294861, 16, -128, 127, torch.int8);  conv2d_9 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_23 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_23, 0.8618559241294861, 16, -128, 127, torch.int8);  quantize_per_tensor_default_23 = None
        relu__9: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_23);  dequantize_per_tensor_default_23 = None
        quantize_per_tensor_default_24 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__9, 0.8618559241294861, 16, -128, 127, torch.int8);  relu__9 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_24 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_24, 0.8618559241294861, 16, -128, 127, torch.int8);  quantize_per_tensor_default_24 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_2: "f32[1, 256, 27, 27]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_22, dequantize_per_tensor_default_24], 1);  dequantize_per_tensor_default_22 = dequantize_per_tensor_default_24 = None
        quantize_per_tensor_default_25 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_2, 0.8618559241294861, 16, -128, 127, torch.int8);  cat_2 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_25 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_25, 0.8618559241294861, 16, -128, 127, torch.int8);  quantize_per_tensor_default_25 = None
        
        # No stacktrace found for following nodes
        _scale_20 = self._scale_20
        _zero_point_20 = self._zero_point_20
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_20 = self._frozen_param20
        dequantize_per_channel_default_20 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_20, _scale_20, _zero_point_20, 0, -127, 127, torch.int8);  quantize_per_channel_default_20 = _scale_20 = _zero_point_20 = None
        
        # No stacktrace found for following nodes
        _scale_21 = self._scale_21
        _zero_point_21 = self._zero_point_21
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_21 = self._frozen_param21
        dequantize_per_channel_default_21 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_21, _scale_21, _zero_point_21, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_21 = _scale_21 = _zero_point_21 = None
        conv2d_10: "f32[1, 32, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_25, dequantize_per_channel_default_20, dequantize_per_channel_default_21);  dequantize_per_tensor_default_25 = dequantize_per_channel_default_20 = dequantize_per_channel_default_21 = None
        quantize_per_tensor_default_26 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_10, 0.9502250552177429, -24, -128, 127, torch.int8);  conv2d_10 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_26 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_26, 0.9502250552177429, -24, -128, 127, torch.int8);  quantize_per_tensor_default_26 = None
        relu__10: "f32[1, 32, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_26);  dequantize_per_tensor_default_26 = None
        quantize_per_tensor_default_27 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__10, 0.9502250552177429, -24, -128, 127, torch.int8);  relu__10 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_74 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_27, 0.9502250552177429, -24, -128, 127, torch.int8)
        dequantize_per_tensor_default_73 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_27, 0.9502250552177429, -24, -128, 127, torch.int8);  quantize_per_tensor_default_27 = None
        
        # No stacktrace found for following nodes
        _scale_22 = self._scale_22
        _zero_point_22 = self._zero_point_22
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_22 = self._frozen_param22
        dequantize_per_channel_default_22 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_22, _scale_22, _zero_point_22, 0, -127, 127, torch.int8);  quantize_per_channel_default_22 = _scale_22 = _zero_point_22 = None
        
        # No stacktrace found for following nodes
        _scale_23 = self._scale_23
        _zero_point_23 = self._zero_point_23
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_23 = self._frozen_param23
        dequantize_per_channel_default_23 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_23, _scale_23, _zero_point_23, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_23 = _scale_23 = _zero_point_23 = None
        conv2d_11: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_73, dequantize_per_channel_default_22, dequantize_per_channel_default_23);  dequantize_per_tensor_default_73 = dequantize_per_channel_default_22 = dequantize_per_channel_default_23 = None
        quantize_per_tensor_default_28 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_11, 1.381868839263916, 30, -128, 127, torch.int8);  conv2d_11 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_28 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_28, 1.381868839263916, 30, -128, 127, torch.int8);  quantize_per_tensor_default_28 = None
        relu__11: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_28);  dequantize_per_tensor_default_28 = None
        quantize_per_tensor_default_29 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__11, 1.381868839263916, 30, -128, 127, torch.int8);  relu__11 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_29 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_29, 1.381868839263916, 30, -128, 127, torch.int8);  quantize_per_tensor_default_29 = None
        _scale_24 = self._scale_24
        _zero_point_24 = self._zero_point_24
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_24 = self._frozen_param24
        dequantize_per_channel_default_24 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_24, _scale_24, _zero_point_24, 0, -127, 127, torch.int8);  quantize_per_channel_default_24 = _scale_24 = _zero_point_24 = None
        
        # No stacktrace found for following nodes
        _scale_25 = self._scale_25
        _zero_point_25 = self._zero_point_25
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_25 = self._frozen_param25
        dequantize_per_channel_default_25 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_25, _scale_25, _zero_point_25, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_25 = _scale_25 = _zero_point_25 = None
        conv2d_12: "f32[1, 128, 27, 27]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_74, dequantize_per_channel_default_24, dequantize_per_channel_default_25, [1, 1], [1, 1]);  dequantize_per_tensor_default_74 = dequantize_per_channel_default_24 = dequantize_per_channel_default_25 = None
        quantize_per_tensor_default_30 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_12, 1.381868839263916, 30, -128, 127, torch.int8);  conv2d_12 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_30 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_30, 1.381868839263916, 30, -128, 127, torch.int8);  quantize_per_tensor_default_30 = None
        relu__12: "f32[1, 128, 27, 27]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_30);  dequantize_per_tensor_default_30 = None
        quantize_per_tensor_default_31 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__12, 1.381868839263916, 30, -128, 127, torch.int8);  relu__12 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_31 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_31, 1.381868839263916, 30, -128, 127, torch.int8);  quantize_per_tensor_default_31 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_3: "f32[1, 256, 27, 27]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_29, dequantize_per_tensor_default_31], 1);  dequantize_per_tensor_default_29 = dequantize_per_tensor_default_31 = None
        quantize_per_tensor_default_32 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_3, 1.381868839263916, 30, -128, 127, torch.int8);  cat_3 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(
        dequantize_per_tensor_default_32 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_32, 1.381868839263916, 30, -128, 127, torch.int8);  quantize_per_tensor_default_32 = None
        max_pool2d_2: "f32[1, 256, 13, 13]" = torch.ops.aten.max_pool2d.default(dequantize_per_tensor_default_32, [3, 3], [2, 2], [0, 0], [1, 1], True);  dequantize_per_tensor_default_32 = None
        quantize_per_tensor_default_33 = torch.ops.quantized_decomposed.quantize_per_tensor.default(max_pool2d_2, 1.381868839263916, 30, -128, 127, torch.int8);  max_pool2d_2 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_33 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_33, 1.381868839263916, 30, -128, 127, torch.int8);  quantize_per_tensor_default_33 = None
        
        # No stacktrace found for following nodes
        _scale_26 = self._scale_26
        _zero_point_26 = self._zero_point_26
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_26 = self._frozen_param26
        dequantize_per_channel_default_26 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_26, _scale_26, _zero_point_26, 0, -127, 127, torch.int8);  quantize_per_channel_default_26 = _scale_26 = _zero_point_26 = None
        
        # No stacktrace found for following nodes
        _scale_27 = self._scale_27
        _zero_point_27 = self._zero_point_27
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_27 = self._frozen_param27
        dequantize_per_channel_default_27 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_27, _scale_27, _zero_point_27, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_27 = _scale_27 = _zero_point_27 = None
        conv2d_13: "f32[1, 48, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_33, dequantize_per_channel_default_26, dequantize_per_channel_default_27);  dequantize_per_tensor_default_33 = dequantize_per_channel_default_26 = dequantize_per_channel_default_27 = None
        quantize_per_tensor_default_34 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_13, 1.2712727785110474, -14, -128, 127, torch.int8);  conv2d_13 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_34 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_34, 1.2712727785110474, -14, -128, 127, torch.int8);  quantize_per_tensor_default_34 = None
        relu__13: "f32[1, 48, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_34);  dequantize_per_tensor_default_34 = None
        quantize_per_tensor_default_35 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__13, 1.2712727785110474, -14, -128, 127, torch.int8);  relu__13 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_76 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_35, 1.2712727785110474, -14, -128, 127, torch.int8)
        dequantize_per_tensor_default_75 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_35, 1.2712727785110474, -14, -128, 127, torch.int8);  quantize_per_tensor_default_35 = None
        
        # No stacktrace found for following nodes
        _scale_28 = self._scale_28
        _zero_point_28 = self._zero_point_28
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_28 = self._frozen_param28
        dequantize_per_channel_default_28 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_28, _scale_28, _zero_point_28, 0, -127, 127, torch.int8);  quantize_per_channel_default_28 = _scale_28 = _zero_point_28 = None
        
        # No stacktrace found for following nodes
        _scale_29 = self._scale_29
        _zero_point_29 = self._zero_point_29
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_29 = self._frozen_param29
        dequantize_per_channel_default_29 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_29, _scale_29, _zero_point_29, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_29 = _scale_29 = _zero_point_29 = None
        conv2d_14: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_75, dequantize_per_channel_default_28, dequantize_per_channel_default_29);  dequantize_per_tensor_default_75 = dequantize_per_channel_default_28 = dequantize_per_channel_default_29 = None
        quantize_per_tensor_default_36 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_14, 1.8479576110839844, 6, -128, 127, torch.int8);  conv2d_14 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_36 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_36, 1.8479576110839844, 6, -128, 127, torch.int8);  quantize_per_tensor_default_36 = None
        relu__14: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_36);  dequantize_per_tensor_default_36 = None
        quantize_per_tensor_default_37 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__14, 1.8479576110839844, 6, -128, 127, torch.int8);  relu__14 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_37 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_37, 1.8479576110839844, 6, -128, 127, torch.int8);  quantize_per_tensor_default_37 = None
        _scale_30 = self._scale_30
        _zero_point_30 = self._zero_point_30
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_30 = self._frozen_param30
        dequantize_per_channel_default_30 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_30, _scale_30, _zero_point_30, 0, -127, 127, torch.int8);  quantize_per_channel_default_30 = _scale_30 = _zero_point_30 = None
        
        # No stacktrace found for following nodes
        _scale_31 = self._scale_31
        _zero_point_31 = self._zero_point_31
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_31 = self._frozen_param31
        dequantize_per_channel_default_31 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_31, _scale_31, _zero_point_31, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_31 = _scale_31 = _zero_point_31 = None
        conv2d_15: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_76, dequantize_per_channel_default_30, dequantize_per_channel_default_31, [1, 1], [1, 1]);  dequantize_per_tensor_default_76 = dequantize_per_channel_default_30 = dequantize_per_channel_default_31 = None
        quantize_per_tensor_default_38 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_15, 1.8479576110839844, 6, -128, 127, torch.int8);  conv2d_15 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_38 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_38, 1.8479576110839844, 6, -128, 127, torch.int8);  quantize_per_tensor_default_38 = None
        relu__15: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_38);  dequantize_per_tensor_default_38 = None
        quantize_per_tensor_default_39 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__15, 1.8479576110839844, 6, -128, 127, torch.int8);  relu__15 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_39 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_39, 1.8479576110839844, 6, -128, 127, torch.int8);  quantize_per_tensor_default_39 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_4: "f32[1, 384, 13, 13]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_37, dequantize_per_tensor_default_39], 1);  dequantize_per_tensor_default_37 = dequantize_per_tensor_default_39 = None
        quantize_per_tensor_default_40 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_4, 1.8479576110839844, 6, -128, 127, torch.int8);  cat_4 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_40 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_40, 1.8479576110839844, 6, -128, 127, torch.int8);  quantize_per_tensor_default_40 = None
        
        # No stacktrace found for following nodes
        _scale_32 = self._scale_32
        _zero_point_32 = self._zero_point_32
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_32 = self._frozen_param32
        dequantize_per_channel_default_32 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_32, _scale_32, _zero_point_32, 0, -127, 127, torch.int8);  quantize_per_channel_default_32 = _scale_32 = _zero_point_32 = None
        
        # No stacktrace found for following nodes
        _scale_33 = self._scale_33
        _zero_point_33 = self._zero_point_33
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_33 = self._frozen_param33
        dequantize_per_channel_default_33 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_33, _scale_33, _zero_point_33, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_33 = _scale_33 = _zero_point_33 = None
        conv2d_16: "f32[1, 48, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_40, dequantize_per_channel_default_32, dequantize_per_channel_default_33);  dequantize_per_tensor_default_40 = dequantize_per_channel_default_32 = dequantize_per_channel_default_33 = None
        quantize_per_tensor_default_41 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_16, 1.558590292930603, -20, -128, 127, torch.int8);  conv2d_16 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_41 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_41, 1.558590292930603, -20, -128, 127, torch.int8);  quantize_per_tensor_default_41 = None
        relu__16: "f32[1, 48, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_41);  dequantize_per_tensor_default_41 = None
        quantize_per_tensor_default_42 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__16, 1.558590292930603, -20, -128, 127, torch.int8);  relu__16 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_78 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_42, 1.558590292930603, -20, -128, 127, torch.int8)
        dequantize_per_tensor_default_77 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_42, 1.558590292930603, -20, -128, 127, torch.int8);  quantize_per_tensor_default_42 = None
        
        # No stacktrace found for following nodes
        _scale_34 = self._scale_34
        _zero_point_34 = self._zero_point_34
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_34 = self._frozen_param34
        dequantize_per_channel_default_34 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_34, _scale_34, _zero_point_34, 0, -127, 127, torch.int8);  quantize_per_channel_default_34 = _scale_34 = _zero_point_34 = None
        
        # No stacktrace found for following nodes
        _scale_35 = self._scale_35
        _zero_point_35 = self._zero_point_35
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_35 = self._frozen_param35
        dequantize_per_channel_default_35 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_35, _scale_35, _zero_point_35, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_35 = _scale_35 = _zero_point_35 = None
        conv2d_17: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_77, dequantize_per_channel_default_34, dequantize_per_channel_default_35);  dequantize_per_tensor_default_77 = dequantize_per_channel_default_34 = dequantize_per_channel_default_35 = None
        quantize_per_tensor_default_43 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_17, 2.101060152053833, 19, -128, 127, torch.int8);  conv2d_17 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_43 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_43, 2.101060152053833, 19, -128, 127, torch.int8);  quantize_per_tensor_default_43 = None
        relu__17: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_43);  dequantize_per_tensor_default_43 = None
        quantize_per_tensor_default_44 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__17, 2.101060152053833, 19, -128, 127, torch.int8);  relu__17 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_44 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_44, 2.101060152053833, 19, -128, 127, torch.int8);  quantize_per_tensor_default_44 = None
        _scale_36 = self._scale_36
        _zero_point_36 = self._zero_point_36
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_36 = self._frozen_param36
        dequantize_per_channel_default_36 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_36, _scale_36, _zero_point_36, 0, -127, 127, torch.int8);  quantize_per_channel_default_36 = _scale_36 = _zero_point_36 = None
        
        # No stacktrace found for following nodes
        _scale_37 = self._scale_37
        _zero_point_37 = self._zero_point_37
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_37 = self._frozen_param37
        dequantize_per_channel_default_37 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_37, _scale_37, _zero_point_37, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_37 = _scale_37 = _zero_point_37 = None
        conv2d_18: "f32[1, 192, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_78, dequantize_per_channel_default_36, dequantize_per_channel_default_37, [1, 1], [1, 1]);  dequantize_per_tensor_default_78 = dequantize_per_channel_default_36 = dequantize_per_channel_default_37 = None
        quantize_per_tensor_default_45 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_18, 2.101060152053833, 19, -128, 127, torch.int8);  conv2d_18 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_45 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_45, 2.101060152053833, 19, -128, 127, torch.int8);  quantize_per_tensor_default_45 = None
        relu__18: "f32[1, 192, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_45);  dequantize_per_tensor_default_45 = None
        quantize_per_tensor_default_46 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__18, 2.101060152053833, 19, -128, 127, torch.int8);  relu__18 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_46 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_46, 2.101060152053833, 19, -128, 127, torch.int8);  quantize_per_tensor_default_46 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_5: "f32[1, 384, 13, 13]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_44, dequantize_per_tensor_default_46], 1);  dequantize_per_tensor_default_44 = dequantize_per_tensor_default_46 = None
        quantize_per_tensor_default_47 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_5, 2.101060152053833, 19, -128, 127, torch.int8);  cat_5 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_47 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_47, 2.101060152053833, 19, -128, 127, torch.int8);  quantize_per_tensor_default_47 = None
        
        # No stacktrace found for following nodes
        _scale_38 = self._scale_38
        _zero_point_38 = self._zero_point_38
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_38 = self._frozen_param38
        dequantize_per_channel_default_38 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_38, _scale_38, _zero_point_38, 0, -127, 127, torch.int8);  quantize_per_channel_default_38 = _scale_38 = _zero_point_38 = None
        
        # No stacktrace found for following nodes
        _scale_39 = self._scale_39
        _zero_point_39 = self._zero_point_39
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_39 = self._frozen_param39
        dequantize_per_channel_default_39 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_39, _scale_39, _zero_point_39, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_39 = _scale_39 = _zero_point_39 = None
        conv2d_19: "f32[1, 64, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_47, dequantize_per_channel_default_38, dequantize_per_channel_default_39);  dequantize_per_tensor_default_47 = dequantize_per_channel_default_38 = dequantize_per_channel_default_39 = None
        quantize_per_tensor_default_48 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_19, 3.0575764179229736, -26, -128, 127, torch.int8);  conv2d_19 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_48 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_48, 3.0575764179229736, -26, -128, 127, torch.int8);  quantize_per_tensor_default_48 = None
        relu__19: "f32[1, 64, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_48);  dequantize_per_tensor_default_48 = None
        quantize_per_tensor_default_49 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__19, 3.0575764179229736, -26, -128, 127, torch.int8);  relu__19 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_80 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_49, 3.0575764179229736, -26, -128, 127, torch.int8)
        dequantize_per_tensor_default_79 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_49, 3.0575764179229736, -26, -128, 127, torch.int8);  quantize_per_tensor_default_49 = None
        
        # No stacktrace found for following nodes
        _scale_40 = self._scale_40
        _zero_point_40 = self._zero_point_40
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_40 = self._frozen_param40
        dequantize_per_channel_default_40 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_40, _scale_40, _zero_point_40, 0, -127, 127, torch.int8);  quantize_per_channel_default_40 = _scale_40 = _zero_point_40 = None
        
        # No stacktrace found for following nodes
        _scale_41 = self._scale_41
        _zero_point_41 = self._zero_point_41
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_41 = self._frozen_param41
        dequantize_per_channel_default_41 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_41, _scale_41, _zero_point_41, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_41 = _scale_41 = _zero_point_41 = None
        conv2d_20: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_79, dequantize_per_channel_default_40, dequantize_per_channel_default_41);  dequantize_per_tensor_default_79 = dequantize_per_channel_default_40 = dequantize_per_channel_default_41 = None
        quantize_per_tensor_default_50 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_20, 2.848783016204834, 15, -128, 127, torch.int8);  conv2d_20 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_50 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_50, 2.848783016204834, 15, -128, 127, torch.int8);  quantize_per_tensor_default_50 = None
        relu__20: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_50);  dequantize_per_tensor_default_50 = None
        quantize_per_tensor_default_51 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__20, 2.848783016204834, 15, -128, 127, torch.int8);  relu__20 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_51 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_51, 2.848783016204834, 15, -128, 127, torch.int8);  quantize_per_tensor_default_51 = None
        _scale_42 = self._scale_42
        _zero_point_42 = self._zero_point_42
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_42 = self._frozen_param42
        dequantize_per_channel_default_42 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_42, _scale_42, _zero_point_42, 0, -127, 127, torch.int8);  quantize_per_channel_default_42 = _scale_42 = _zero_point_42 = None
        
        # No stacktrace found for following nodes
        _scale_43 = self._scale_43
        _zero_point_43 = self._zero_point_43
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_43 = self._frozen_param43
        dequantize_per_channel_default_43 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_43, _scale_43, _zero_point_43, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_43 = _scale_43 = _zero_point_43 = None
        conv2d_21: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_80, dequantize_per_channel_default_42, dequantize_per_channel_default_43, [1, 1], [1, 1]);  dequantize_per_tensor_default_80 = dequantize_per_channel_default_42 = dequantize_per_channel_default_43 = None
        quantize_per_tensor_default_52 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_21, 2.848783016204834, 15, -128, 127, torch.int8);  conv2d_21 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_52 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_52, 2.848783016204834, 15, -128, 127, torch.int8);  quantize_per_tensor_default_52 = None
        relu__21: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_52);  dequantize_per_tensor_default_52 = None
        quantize_per_tensor_default_53 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__21, 2.848783016204834, 15, -128, 127, torch.int8);  relu__21 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_53 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_53, 2.848783016204834, 15, -128, 127, torch.int8);  quantize_per_tensor_default_53 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_6: "f32[1, 512, 13, 13]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_51, dequantize_per_tensor_default_53], 1);  dequantize_per_tensor_default_51 = dequantize_per_tensor_default_53 = None
        quantize_per_tensor_default_54 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_6, 2.848783016204834, 15, -128, 127, torch.int8);  cat_6 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_54 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_54, 2.848783016204834, 15, -128, 127, torch.int8);  quantize_per_tensor_default_54 = None
        
        # No stacktrace found for following nodes
        _scale_44 = self._scale_44
        _zero_point_44 = self._zero_point_44
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_44 = self._frozen_param44
        dequantize_per_channel_default_44 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_44, _scale_44, _zero_point_44, 0, -127, 127, torch.int8);  quantize_per_channel_default_44 = _scale_44 = _zero_point_44 = None
        
        # No stacktrace found for following nodes
        _scale_45 = self._scale_45
        _zero_point_45 = self._zero_point_45
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_45 = self._frozen_param45
        dequantize_per_channel_default_45 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_45, _scale_45, _zero_point_45, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_45 = _scale_45 = _zero_point_45 = None
        conv2d_22: "f32[1, 64, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_54, dequantize_per_channel_default_44, dequantize_per_channel_default_45);  dequantize_per_tensor_default_54 = dequantize_per_channel_default_44 = dequantize_per_channel_default_45 = None
        quantize_per_tensor_default_55 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_22, 4.477190971374512, -8, -128, 127, torch.int8);  conv2d_22 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_55 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_55, 4.477190971374512, -8, -128, 127, torch.int8);  quantize_per_tensor_default_55 = None
        relu__22: "f32[1, 64, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_55);  dequantize_per_tensor_default_55 = None
        quantize_per_tensor_default_56 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__22, 4.477190971374512, -8, -128, 127, torch.int8);  relu__22 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_82 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_56, 4.477190971374512, -8, -128, 127, torch.int8)
        dequantize_per_tensor_default_81 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_56, 4.477190971374512, -8, -128, 127, torch.int8);  quantize_per_tensor_default_56 = None
        
        # No stacktrace found for following nodes
        _scale_46 = self._scale_46
        _zero_point_46 = self._zero_point_46
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_46 = self._frozen_param46
        dequantize_per_channel_default_46 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_46, _scale_46, _zero_point_46, 0, -127, 127, torch.int8);  quantize_per_channel_default_46 = _scale_46 = _zero_point_46 = None
        
        # No stacktrace found for following nodes
        _scale_47 = self._scale_47
        _zero_point_47 = self._zero_point_47
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_47 = self._frozen_param47
        dequantize_per_channel_default_47 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_47, _scale_47, _zero_point_47, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_47 = _scale_47 = _zero_point_47 = None
        conv2d_23: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_81, dequantize_per_channel_default_46, dequantize_per_channel_default_47);  dequantize_per_tensor_default_81 = dequantize_per_channel_default_46 = dequantize_per_channel_default_47 = None
        quantize_per_tensor_default_57 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_23, 6.87384557723999, 59, -128, 127, torch.int8);  conv2d_23 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_57 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_57, 6.87384557723999, 59, -128, 127, torch.int8);  quantize_per_tensor_default_57 = None
        relu__23: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_57);  dequantize_per_tensor_default_57 = None
        quantize_per_tensor_default_58 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__23, 6.87384557723999, 59, -128, 127, torch.int8);  relu__23 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_58 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_58, 6.87384557723999, 59, -128, 127, torch.int8);  quantize_per_tensor_default_58 = None
        _scale_48 = self._scale_48
        _zero_point_48 = self._zero_point_48
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_48 = self._frozen_param48
        dequantize_per_channel_default_48 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_48, _scale_48, _zero_point_48, 0, -127, 127, torch.int8);  quantize_per_channel_default_48 = _scale_48 = _zero_point_48 = None
        
        # No stacktrace found for following nodes
        _scale_49 = self._scale_49
        _zero_point_49 = self._zero_point_49
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_49 = self._frozen_param49
        dequantize_per_channel_default_49 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_49, _scale_49, _zero_point_49, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_49 = _scale_49 = _zero_point_49 = None
        conv2d_24: "f32[1, 256, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_82, dequantize_per_channel_default_48, dequantize_per_channel_default_49, [1, 1], [1, 1]);  dequantize_per_tensor_default_82 = dequantize_per_channel_default_48 = dequantize_per_channel_default_49 = None
        quantize_per_tensor_default_59 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_24, 6.87384557723999, 59, -128, 127, torch.int8);  conv2d_24 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_59 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_59, 6.87384557723999, 59, -128, 127, torch.int8);  quantize_per_tensor_default_59 = None
        relu__24: "f32[1, 256, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_59);  dequantize_per_tensor_default_59 = None
        quantize_per_tensor_default_60 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__24, 6.87384557723999, 59, -128, 127, torch.int8);  relu__24 = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_60 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_60, 6.87384557723999, 59, -128, 127, torch.int8);  quantize_per_tensor_default_60 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:31 in forward, code: return torch.cat(
        cat_7: "f32[1, 512, 13, 13]" = torch.ops.aten.cat.default([dequantize_per_tensor_default_58, dequantize_per_tensor_default_60], 1);  dequantize_per_tensor_default_58 = dequantize_per_tensor_default_60 = None
        quantize_per_tensor_default_61 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_7, 6.87384557723999, 59, -128, 127, torch.int8);  cat_7 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
        dequantize_per_tensor_default_61 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_61, 6.87384557723999, 59, -128, 127, torch.int8);  quantize_per_tensor_default_61 = None
        dropout: "f32[1, 512, 13, 13]" = torch.ops.aten.dropout.default(dequantize_per_tensor_default_61, 0.5, False);  dequantize_per_tensor_default_61 = None
        quantize_per_tensor_default_62 = torch.ops.quantized_decomposed.quantize_per_tensor.default(dropout, 6.87384557723999, 59, -128, 127, torch.int8);  dropout = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        dequantize_per_tensor_default_62 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_62, 6.87384557723999, 59, -128, 127, torch.int8);  quantize_per_tensor_default_62 = None
        
        # No stacktrace found for following nodes
        _scale_50 = self._scale_50
        _zero_point_50 = self._zero_point_50
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_50 = self._frozen_param50
        dequantize_per_channel_default_50 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_50, _scale_50, _zero_point_50, 0, -127, 127, torch.int8);  quantize_per_channel_default_50 = _scale_50 = _zero_point_50 = None
        
        # No stacktrace found for following nodes
        _scale_51 = self._scale_51
        _zero_point_51 = self._zero_point_51
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
        quantize_per_channel_default_51 = self._frozen_param51
        dequantize_per_channel_default_51 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_51, _scale_51, _zero_point_51, 0, -2147483648, 2147483646, torch.int32);  quantize_per_channel_default_51 = _scale_51 = _zero_point_51 = None
        conv2d_25: "f32[1, 4, 13, 13]" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_62, dequantize_per_channel_default_50, dequantize_per_channel_default_51);  dequantize_per_tensor_default_62 = dequantize_per_channel_default_50 = dequantize_per_channel_default_51 = None
        quantize_per_tensor_default_63 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_25, 1.39414381980896, -26, -128, 127, torch.int8);  conv2d_25 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)
        dequantize_per_tensor_default_63 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_63, 1.39414381980896, -26, -128, 127, torch.int8);  quantize_per_tensor_default_63 = None
        relu__25: "f32[1, 4, 13, 13]" = torch.ops.aten.relu_.default(dequantize_per_tensor_default_63);  dequantize_per_tensor_default_63 = None
        quantize_per_tensor_default_64 = torch.ops.quantized_decomposed.quantize_per_tensor.default(relu__25, 1.39414381980896, -26, -128, 127, torch.int8);  relu__25 = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:1500 in forward, code: return F.adaptive_avg_pool2d(input, self.output_size)
        dequantize_per_tensor_default_64 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_64, 1.39414381980896, -26, -128, 127, torch.int8);  quantize_per_tensor_default_64 = None
        adaptive_avg_pool2d: "f32[1, 4, 1, 1]" = torch.ops.aten.adaptive_avg_pool2d.default(dequantize_per_tensor_default_64, [1, 1]);  dequantize_per_tensor_default_64 = None
        quantize_per_tensor_default_65 = torch.ops.quantized_decomposed.quantize_per_tensor.default(adaptive_avg_pool2d, 1.39414381980896, -26, -128, 127, torch.int8);  adaptive_avg_pool2d = None
        
         # File: /workspace/executorch-venv/lib/python3.12/site-packages/torchvision/models/squeezenet.py:97 in forward, code: return torch.flatten(x, 1)
        dequantize_per_tensor_default_65 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_65, 1.39414381980896, -26, -128, 127, torch.int8);  quantize_per_tensor_default_65 = None
        flatten: "f32[1, 4]" = torch.ops.aten.flatten.using_ints(dequantize_per_tensor_default_65, 1);  dequantize_per_tensor_default_65 = None
        quantize_per_tensor_default_66 = torch.ops.quantized_decomposed.quantize_per_tensor.default(flatten, 1.39414381980896, -26, -128, 127, torch.int8);  flatten = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_66 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_66, 1.39414381980896, -26, -128, 127, torch.int8);  quantize_per_tensor_default_66 = None
        return pytree.tree_unflatten((dequantize_per_tensor_default_66,), self._out_spec)
        WARNING:executorch.backends.arm._passes.to_tosa_memory_format_pass:Ignoring dim_order kwarg '[0, 1, 2, 3]' for 'dim_order_ops__clone_dim_order_default'.

[ Before Graph Optimisation ]
0     Const                const-1                       
1     Const                const-0                       
2     Const                aten_convolution_default_25_output_zp
3     Const                aten_convolution_default_25_input_zp
4     Const                aten_convolution_default_25_shifts
5     Const                aten_convolution_default_25_multipliers
6     Const                layer-26_weight_zp            
7     Const                layer-26_input_zp             
8     Const                b__frozen_param51             
9     Const                b__frozen_param50             
10    Const                aten_convolution_default_23_output_zp
11    Const                aten_convolution_default_23_input_zp
12    Const                aten_convolution_default_23_shifts
13    Const                aten_convolution_default_23_multipliers
14    Const                layer-24_weight_zp            
15    Const                layer-24_input_zp             
16    Const                b__frozen_param49             
17    Const                b__frozen_param48             
18    Const                aten_convolution_default_22_output_zp
19    Const                aten_convolution_default_22_input_zp
20    Const                aten_convolution_default_22_shifts
21    Const                aten_convolution_default_22_multipliers
22    Const                layer-23_weight_zp            
23    Const                layer-23_input_zp             
24    Const                b__frozen_param45             
25    Const                b__frozen_param44             
26    Const                aten_convolution_default_20_output_zp
27    Const                aten_convolution_default_20_input_zp
28    Const                aten_convolution_default_20_shifts
29    Const                aten_convolution_default_20_multipliers
30    Const                layer-21_weight_zp            
31    Const                layer-21_input_zp             
32    Const                b__frozen_param43             
33    Const                b__frozen_param42             
34    Const                aten_convolution_default_19_output_zp
35    Const                aten_convolution_default_19_input_zp
36    Const                aten_convolution_default_19_shifts
37    Const                aten_convolution_default_19_multipliers
38    Const                layer-20_weight_zp            
39    Const                layer-20_input_zp             
40    Const                b__frozen_param39             
41    Const                b__frozen_param38             
42    Const                aten_convolution_default_17_output_zp
43    Const                aten_convolution_default_17_input_zp
44    Const                aten_convolution_default_17_shifts
45    Const                aten_convolution_default_17_multipliers
46    Const                layer-18_weight_zp            
47    Const                layer-18_input_zp             
48    Const                b__frozen_param37             
49    Const                b__frozen_param36             
50    Const                aten_convolution_default_16_output_zp
51    Const                aten_convolution_default_16_input_zp
52    Const                aten_convolution_default_16_shifts
53    Const                aten_convolution_default_16_multipliers
54    Const                layer-17_weight_zp            
55    Const                layer-17_input_zp             
56    Const                b__frozen_param33             
57    Const                b__frozen_param32             
58    Const                aten_convolution_default_14_output_zp
59    Const                aten_convolution_default_14_input_zp
60    Const                aten_convolution_default_14_shifts
61    Const                aten_convolution_default_14_multipliers
62    Const                layer-15_weight_zp            
63    Const                layer-15_input_zp             
64    Const                b__frozen_param31             
65    Const                b__frozen_param30             
66    Const                aten_convolution_default_13_output_zp
67    Const                aten_convolution_default_13_input_zp
68    Const                aten_convolution_default_13_shifts
69    Const                aten_convolution_default_13_multipliers
70    Const                layer-14_weight_zp            
71    Const                layer-14_input_zp             
72    Const                b__frozen_param27             
73    Const                b__frozen_param26             
74    Const                aten_convolution_default_11_output_zp
75    Const                aten_convolution_default_11_input_zp
76    Const                aten_convolution_default_11_shifts
77    Const                aten_convolution_default_11_multipliers
78    Const                layer-12_weight_zp            
79    Const                layer-12_input_zp             
80    Const                b__frozen_param25             
81    Const                b__frozen_param24             
82    Const                aten_convolution_default_10_output_zp
83    Const                aten_convolution_default_10_input_zp
84    Const                aten_convolution_default_10_shifts
85    Const                aten_convolution_default_10_multipliers
86    Const                layer-11_weight_zp            
87    Const                layer-11_input_zp             
88    Const                b__frozen_param21             
89    Const                b__frozen_param20             
90    Const                aten_convolution_default_8_output_zp
91    Const                aten_convolution_default_8_input_zp
92    Const                aten_convolution_default_8_shifts
93    Const                aten_convolution_default_8_multipliers
94    Const                layer-9_weight_zp             
95    Const                layer-9_input_zp              
96    Const                b__frozen_param19             
97    Const                b__frozen_param18             
98    Const                aten_convolution_default_7_output_zp
99    Const                aten_convolution_default_7_input_zp
100   Const                aten_convolution_default_7_shifts
101   Const                aten_convolution_default_7_multipliers
102   Const                layer-8_weight_zp             
103   Const                layer-8_input_zp              
104   Const                b__frozen_param15             
105   Const                b__frozen_param14             
106   Const                aten_convolution_default_5_output_zp
107   Const                aten_convolution_default_5_input_zp
108   Const                aten_convolution_default_5_shifts
109   Const                aten_convolution_default_5_multipliers
110   Const                layer-6_weight_zp             
111   Const                layer-6_input_zp              
112   Const                b__frozen_param13             
113   Const                b__frozen_param12             
114   Const                aten_convolution_default_4_output_zp
115   Const                aten_convolution_default_4_input_zp
116   Const                aten_convolution_default_4_shifts
117   Const                aten_convolution_default_4_multipliers
118   Const                layer-5_weight_zp             
119   Const                layer-5_input_zp              
120   Const                b__frozen_param9              
121   Const                b__frozen_param8              
122   Const                aten_convolution_default_2_output_zp
123   Const                aten_convolution_default_2_input_zp
124   Const                aten_convolution_default_2_shifts
125   Const                aten_convolution_default_2_multipliers
126   Const                layer-3_weight_zp             
127   Const                layer-3_input_zp              
128   Const                b__frozen_param7              
129   Const                b__frozen_param6              
130   Const                aten_convolution_default_1_output_zp
131   Const                aten_convolution_default_1_input_zp
132   Const                aten_convolution_default_1_shifts
133   Const                aten_convolution_default_1_multipliers
134   Const                layer-2_weight_zp             
135   Const                layer-2_input_zp              
136   Const                b__frozen_param3              
137   Const                b__frozen_param2              
138   Const                aten_convolution_default_output_zp
139   Const                aten_convolution_default_input_zp
140   Const                aten_convolution_default_shifts
141   Const                aten_convolution_default_multipliers
142   Const                layer-1_weight_zp             
143   Const                layer-1_input_zp              
144   Const                b__frozen_param1              
145   Const                b__frozen_param0              
146   Transpose            tosa_transpose_default        
147   Slice                aten_slice_copy_tensor        
148   Slice                aten_slice_copy_tensor_1      
149   Conv2D               layer-1                       
150   Rescale              aten_convolution_default      
151   Clamp                aten_clamp_default            
152   MaxPool              aten_max_pool2d_default       
153   Conv2D               layer-2                       
154   Rescale              aten_convolution_default_1    
155   Clamp                aten_clamp_default_1          
156   Conv2D               layer-3                       
157   Rescale              aten_convolution_default_2    
158   Clamp                aten_clamp_default_2          
159   Const                aten_convolution_default_3_output_zp
160   Const                aten_convolution_default_3_input_zp
161   Const                aten_convolution_default_3_shifts
162   Const                aten_convolution_default_3_multipliers
163   Const                layer-4_weight_zp             
164   Const                layer-4_input_zp              
165   Const                b__frozen_param5              
166   Const                b__frozen_param4              
167   Conv2D               layer-4                       
168   Rescale              aten_convolution_default_3    
169   Clamp                aten_clamp_default_3          
170   Concat               aten_cat_default              
171   Conv2D               layer-5                       
172   Rescale              aten_convolution_default_4    
173   Clamp                aten_clamp_default_4          
174   Conv2D               layer-6                       
175   Rescale              aten_convolution_default_5    
176   Clamp                aten_clamp_default_5          
177   Const                aten_convolution_default_6_output_zp
178   Const                aten_convolution_default_6_input_zp
179   Const                aten_convolution_default_6_shifts
180   Const                aten_convolution_default_6_multipliers
181   Const                layer-7_weight_zp             
182   Const                layer-7_input_zp              
183   Const                b__frozen_param11             
184   Const                b__frozen_param10             
185   Conv2D               layer-7                       
186   Rescale              aten_convolution_default_6    
187   Clamp                aten_clamp_default_6          
188   Concat               aten_cat_default_1            
189   MaxPool              aten_max_pool2d_default_1     
190   Conv2D               layer-8                       
191   Rescale              aten_convolution_default_7    
192   Clamp                aten_clamp_default_7          
193   Conv2D               layer-9                       
194   Rescale              aten_convolution_default_8    
195   Clamp                aten_clamp_default_8          
196   Const                aten_convolution_default_9_output_zp
197   Const                aten_convolution_default_9_input_zp
198   Const                aten_convolution_default_9_shifts
199   Const                aten_convolution_default_9_multipliers
200   Const                layer-10_weight_zp            
201   Const                layer-10_input_zp             
202   Const                b__frozen_param17             
203   Const                b__frozen_param16             
204   Conv2D               layer-10                      
205   Rescale              aten_convolution_default_9    
206   Clamp                aten_clamp_default_9          
207   Concat               aten_cat_default_2            
208   Conv2D               layer-11                      
209   Rescale              aten_convolution_default_10   
210   Clamp                aten_clamp_default_10         
211   Conv2D               layer-12                      
212   Rescale              aten_convolution_default_11   
213   Clamp                aten_clamp_default_11         
214   Const                aten_convolution_default_12_output_zp
215   Const                aten_convolution_default_12_input_zp
216   Const                aten_convolution_default_12_shifts
217   Const                aten_convolution_default_12_multipliers
218   Const                layer-13_weight_zp            
219   Const                layer-13_input_zp             
220   Const                b__frozen_param23             
221   Const                b__frozen_param22             
222   Conv2D               layer-13                      
223   Rescale              aten_convolution_default_12   
224   Clamp                aten_clamp_default_12         
225   Concat               aten_cat_default_3            
226   MaxPool              aten_max_pool2d_default_2     
227   Conv2D               layer-14                      
228   Rescale              aten_convolution_default_13   
229   Clamp                aten_clamp_default_13         
230   Conv2D               layer-15                      
231   Rescale              aten_convolution_default_14   
232   Clamp                aten_clamp_default_14         
233   Const                aten_convolution_default_15_output_zp
234   Const                aten_convolution_default_15_input_zp
235   Const                aten_convolution_default_15_shifts
236   Const                aten_convolution_default_15_multipliers
237   Const                layer-16_weight_zp            
238   Const                layer-16_input_zp             
239   Const                b__frozen_param29             
240   Const                b__frozen_param28             
241   Conv2D               layer-16                      
242   Rescale              aten_convolution_default_15   
243   Clamp                aten_clamp_default_15         
244   Concat               aten_cat_default_4            
245   Conv2D               layer-17                      
246   Rescale              aten_convolution_default_16   
247   Clamp                aten_clamp_default_16         
248   Conv2D               layer-18                      
249   Rescale              aten_convolution_default_17   
250   Clamp                aten_clamp_default_17         
251   Const                aten_convolution_default_18_output_zp
252   Const                aten_convolution_default_18_input_zp
253   Const                aten_convolution_default_18_shifts
254   Const                aten_convolution_default_18_multipliers
255   Const                layer-19_weight_zp            
256   Const                layer-19_input_zp             
257   Const                b__frozen_param35             
258   Const                b__frozen_param34             
259   Conv2D               layer-19                      
260   Rescale              aten_convolution_default_18   
261   Clamp                aten_clamp_default_18         
262   Concat               aten_cat_default_5            
263   Conv2D               layer-20                      
264   Rescale              aten_convolution_default_19   
265   Clamp                aten_clamp_default_19         
266   Conv2D               layer-21                      
267   Rescale              aten_convolution_default_20   
268   Clamp                aten_clamp_default_20         
269   Const                aten_convolution_default_21_output_zp
270   Const                aten_convolution_default_21_input_zp
271   Const                aten_convolution_default_21_shifts
272   Const                aten_convolution_default_21_multipliers
273   Const                layer-22_weight_zp            
274   Const                layer-22_input_zp             
275   Const                b__frozen_param41             
276   Const                b__frozen_param40             
277   Conv2D               layer-22                      
278   Rescale              aten_convolution_default_21   
279   Clamp                aten_clamp_default_21         
280   Concat               aten_cat_default_6            
281   Conv2D               layer-23                      
282   Rescale              aten_convolution_default_22   
283   Clamp                aten_clamp_default_22         
284   Conv2D               layer-24                      
285   Rescale              aten_convolution_default_23   
286   Clamp                aten_clamp_default_23         
287   Const                aten_convolution_default_24_output_zp
288   Const                aten_convolution_default_24_input_zp
289   Const                aten_convolution_default_24_shifts
290   Const                aten_convolution_default_24_multipliers
291   Const                layer-25_weight_zp            
292   Const                layer-25_input_zp             
293   Const                b__frozen_param47             
294   Const                b__frozen_param46             
295   Conv2D               layer-25                      
296   Rescale              aten_convolution_default_24   
297   Clamp                aten_clamp_default_24         
298   Concat               aten_cat_default_7            
299   Conv2D               layer-26                      
300   Rescale              aten_convolution_default_25   
301   Clamp                aten_clamp_default_25         
302   AvgPool              aten_avg_pool2d_default       
303   Reshape              aten_view_copy_default        


[ After Graph Optimization ]
0     Const                const-1                       
1     Const                const-0                       
2     Const                aten_convolution_default_25_output_zp
3     Const                aten_convolution_default_25_input_zp
4     Const                aten_convolution_default_25_shifts
5     Const                aten_convolution_default_25_multipliers
6     Const                layer-26_weight_zp            
7     Const                layer-26_input_zp             
8     Const                b__frozen_param51             
9     Const                b__frozen_param50             
10    Const                aten_convolution_default_23_output_zp
11    Const                aten_convolution_default_23_input_zp
12    Const                aten_convolution_default_23_shifts
13    Const                aten_convolution_default_23_multipliers
14    Const                layer-24_weight_zp            
15    Const                layer-24_input_zp             
16    Const                b__frozen_param49             
17    Const                b__frozen_param48             
18    Const                aten_convolution_default_22_output_zp
19    Const                aten_convolution_default_22_input_zp
20    Const                aten_convolution_default_22_shifts
21    Const                aten_convolution_default_22_multipliers
22    Const                layer-23_weight_zp            
23    Const                layer-23_input_zp             
24    Const                b__frozen_param45             
25    Const                b__frozen_param44             
26    Const                aten_convolution_default_20_output_zp
27    Const                aten_convolution_default_20_input_zp
28    Const                aten_convolution_default_20_shifts
29    Const                aten_convolution_default_20_multipliers
30    Const                layer-21_weight_zp            
31    Const                layer-21_input_zp             
32    Const                b__frozen_param43             
33    Const                b__frozen_param42             
34    Const                aten_convolution_default_19_output_zp
35    Const                aten_convolution_default_19_input_zp
36    Const                aten_convolution_default_19_shifts
37    Const                aten_convolution_default_19_multipliers
38    Const                layer-20_weight_zp            
39    Const                layer-20_input_zp             
40    Const                b__frozen_param39             
41    Const                b__frozen_param38             
42    Const                aten_convolution_default_17_output_zp
43    Const                aten_convolution_default_17_input_zp
44    Const                aten_convolution_default_17_shifts
45    Const                aten_convolution_default_17_multipliers
46    Const                layer-18_weight_zp            
47    Const                layer-18_input_zp             
48    Const                b__frozen_param37             
49    Const                b__frozen_param36             
50    Const                aten_convolution_default_16_output_zp
51    Const                aten_convolution_default_16_input_zp
52    Const                aten_convolution_default_16_shifts
53    Const                aten_convolution_default_16_multipliers
54    Const                layer-17_weight_zp            
55    Const                layer-17_input_zp             
56    Const                b__frozen_param33             
57    Const                b__frozen_param32             
58    Const                aten_convolution_default_14_output_zp
59    Const                aten_convolution_default_14_input_zp
60    Const                aten_convolution_default_14_shifts
61    Const                aten_convolution_default_14_multipliers
62    Const                layer-15_weight_zp            
63    Const                layer-15_input_zp             
64    Const                b__frozen_param31             
65    Const                b__frozen_param30             
66    Const                aten_convolution_default_13_output_zp
67    Const                aten_convolution_default_13_input_zp
68    Const                aten_convolution_default_13_shifts
69    Const                aten_convolution_default_13_multipliers
70    Const                layer-14_weight_zp            
71    Const                layer-14_input_zp             
72    Const                b__frozen_param27             
73    Const                b__frozen_param26             
74    Const                aten_convolution_default_11_output_zp
75    Const                aten_convolution_default_11_input_zp
76    Const                aten_convolution_default_11_shifts
77    Const                aten_convolution_default_11_multipliers
78    Const                layer-12_weight_zp            
79    Const                layer-12_input_zp             
80    Const                b__frozen_param25             
81    Const                b__frozen_param24             
82    Const                aten_convolution_default_10_output_zp
83    Const                aten_convolution_default_10_input_zp
84    Const                aten_convolution_default_10_shifts
85    Const                aten_convolution_default_10_multipliers
86    Const                layer-11_weight_zp            
87    Const                layer-11_input_zp             
88    Const                b__frozen_param21             
89    Const                b__frozen_param20             
90    Const                aten_convolution_default_8_output_zp
91    Const                aten_convolution_default_8_input_zp
92    Const                aten_convolution_default_8_shifts
93    Const                aten_convolution_default_8_multipliers
94    Const                layer-9_weight_zp             
95    Const                layer-9_input_zp              
96    Const                b__frozen_param19             
97    Const                b__frozen_param18             
98    Const                aten_convolution_default_7_output_zp
99    Const                aten_convolution_default_7_input_zp
100   Const                aten_convolution_default_7_shifts
101   Const                aten_convolution_default_7_multipliers
102   Const                layer-8_weight_zp             
103   Const                layer-8_input_zp              
104   Const                b__frozen_param15             
105   Const                b__frozen_param14             
106   Const                aten_convolution_default_5_output_zp
107   Const                aten_convolution_default_5_input_zp
108   Const                aten_convolution_default_5_shifts
109   Const                aten_convolution_default_5_multipliers
110   Const                layer-6_weight_zp             
111   Const                layer-6_input_zp              
112   Const                b__frozen_param13             
113   Const                b__frozen_param12             
114   Const                aten_convolution_default_4_output_zp
115   Const                aten_convolution_default_4_input_zp
116   Const                aten_convolution_default_4_shifts
117   Const                aten_convolution_default_4_multipliers
118   Const                layer-5_weight_zp             
119   Const                layer-5_input_zp              
120   Const                b__frozen_param9              
121   Const                b__frozen_param8              
122   Const                aten_convolution_default_2_output_zp
123   Const                aten_convolution_default_2_input_zp
124   Const                aten_convolution_default_2_shifts
125   Const                aten_convolution_default_2_multipliers
126   Const                layer-3_weight_zp             
127   Const                layer-3_input_zp              
128   Const                b__frozen_param7              
129   Const                b__frozen_param6              
130   Const                aten_convolution_default_1_output_zp
131   Const                aten_convolution_default_1_input_zp
132   Const                aten_convolution_default_1_shifts
133   Const                aten_convolution_default_1_multipliers
134   Const                layer-2_weight_zp             
135   Const                layer-2_input_zp              
136   Const                b__frozen_param3              
137   Const                b__frozen_param2              
138   Const                aten_convolution_default_output_zp
139   Const                aten_convolution_default_input_zp
140   Const                aten_convolution_default_shifts
141   Const                aten_convolution_default_multipliers
142   Const                layer-1_weight_zp             
143   Const                layer-1_input_zp              
144   Const                b__frozen_param1              
145   Const                b__frozen_param0              
146   Transpose            tosa_transpose_default        
147   Slice                aten_slice_copy_tensor        
148   Slice                aten_slice_copy_tensor_1      
149   Conv2D               layer-1                       
150   Rescale              aten_convolution_default      
151   Clamp                aten_clamp_default            
152   MaxPool              aten_max_pool2d_default       
153   Conv2D               layer-2                       
154   Rescale              aten_convolution_default_1    
155   Clamp                aten_clamp_default_1          
156   Conv2D               layer-3                       
157   Rescale              aten_convolution_default_2    
158   Clamp                aten_clamp_default_2          
159   Const                aten_convolution_default_3_output_zp
160   Const                aten_convolution_default_3_input_zp
161   Const                aten_convolution_default_3_shifts
162   Const                aten_convolution_default_3_multipliers
163   Const                layer-4_weight_zp             
164   Const                layer-4_input_zp              
165   Const                b__frozen_param5              
166   Const                b__frozen_param4              
167   Conv2D               layer-4                       
168   Rescale              aten_convolution_default_3    
169   Clamp                aten_clamp_default_3          
170   Concat               aten_cat_default              
171   Conv2D               layer-5                       
172   Rescale              aten_convolution_default_4    
173   Clamp                aten_clamp_default_4          
174   Conv2D               layer-6                       
175   Rescale              aten_convolution_default_5    
176   Clamp                aten_clamp_default_5          
177   Const                aten_convolution_default_6_output_zp
178   Const                aten_convolution_default_6_input_zp
179   Const                aten_convolution_default_6_shifts
180   Const                aten_convolution_default_6_multipliers
181   Const                layer-7_weight_zp             
182   Const                layer-7_input_zp              
183   Const                b__frozen_param11             
184   Const                b__frozen_param10             
185   Conv2D               layer-7                       
186   Rescale              aten_convolution_default_6    
187   Clamp                aten_clamp_default_6          
188   Concat               aten_cat_default_1            
189   MaxPool              aten_max_pool2d_default_1     
190   Conv2D               layer-8                       
191   Rescale              aten_convolution_default_7    
192   Clamp                aten_clamp_default_7          
193   Conv2D               layer-9                       
194   Rescale              aten_convolution_default_8    
195   Clamp                aten_clamp_default_8          
196   Const                aten_convolution_default_9_output_zp
197   Const                aten_convolution_default_9_input_zp
198   Const                aten_convolution_default_9_shifts
199   Const                aten_convolution_default_9_multipliers
200   Const                layer-10_weight_zp            
201   Const                layer-10_input_zp             
202   Const                b__frozen_param17             
203   Const                b__frozen_param16             
204   Conv2D               layer-10                      
205   Rescale              aten_convolution_default_9    
206   Clamp                aten_clamp_default_9          
207   Concat               aten_cat_default_2            
208   Conv2D               layer-11                      
209   Rescale              aten_convolution_default_10   
210   Clamp                aten_clamp_default_10         
211   Conv2D               layer-12                      
212   Rescale              aten_convolution_default_11   
213   Clamp                aten_clamp_default_11         
214   Const                aten_convolution_default_12_output_zp
215   Const                aten_convolution_default_12_input_zp
216   Const                aten_convolution_default_12_shifts
217   Const                aten_convolution_default_12_multipliers
218   Const                layer-13_weight_zp            
219   Const                layer-13_input_zp             
220   Const                b__frozen_param23             
221   Const                b__frozen_param22             
222   Conv2D               layer-13                      
223   Rescale              aten_convolution_default_12   
224   Clamp                aten_clamp_default_12         
225   Concat               aten_cat_default_3            
226   MaxPool              aten_max_pool2d_default_2     
227   Conv2D               layer-14                      
228   Rescale              aten_convolution_default_13   
229   Clamp                aten_clamp_default_13         
230   Conv2D               layer-15                      
231   Rescale              aten_convolution_default_14   
232   Clamp                aten_clamp_default_14         
233   Const                aten_convolution_default_15_output_zp
234   Const                aten_convolution_default_15_input_zp
235   Const                aten_convolution_default_15_shifts
236   Const                aten_convolution_default_15_multipliers
237   Const                layer-16_weight_zp            
238   Const                layer-16_input_zp             
239   Const                b__frozen_param29             
240   Const                b__frozen_param28             
241   Conv2D               layer-16                      
242   Rescale              aten_convolution_default_15   
243   Clamp                aten_clamp_default_15         
244   Concat               aten_cat_default_4            
245   Conv2D               layer-17                      
246   Rescale              aten_convolution_default_16   
247   Clamp                aten_clamp_default_16         
248   Conv2D               layer-18                      
249   Rescale              aten_convolution_default_17   
250   Clamp                aten_clamp_default_17         
251   Const                aten_convolution_default_18_output_zp
252   Const                aten_convolution_default_18_input_zp
253   Const                aten_convolution_default_18_shifts
254   Const                aten_convolution_default_18_multipliers
255   Const                layer-19_weight_zp            
256   Const                layer-19_input_zp             
257   Const                b__frozen_param35             
258   Const                b__frozen_param34             
259   Conv2D               layer-19                      
260   Rescale              aten_convolution_default_18   
261   Clamp                aten_clamp_default_18         
262   Concat               aten_cat_default_5            
263   Conv2D               layer-20                      
264   Rescale              aten_convolution_default_19   
265   Clamp                aten_clamp_default_19         
266   Conv2D               layer-21                      
267   Rescale              aten_convolution_default_20   
268   Clamp                aten_clamp_default_20         
269   Const                aten_convolution_default_21_output_zp
270   Const                aten_convolution_default_21_input_zp
271   Const                aten_convolution_default_21_shifts
272   Const                aten_convolution_default_21_multipliers
273   Const                layer-22_weight_zp            
274   Const                layer-22_input_zp             
275   Const                b__frozen_param41             
276   Const                b__frozen_param40             
277   Conv2D               layer-22                      
278   Rescale              aten_convolution_default_21   
279   Clamp                aten_clamp_default_21         
280   Concat               aten_cat_default_6            
281   Conv2D               layer-23                      
282   Rescale              aten_convolution_default_22   
283   Clamp                aten_clamp_default_22         
284   Conv2D               layer-24                      
285   Rescale              aten_convolution_default_23   
286   Clamp                aten_clamp_default_23         
287   Const                aten_convolution_default_24_output_zp
288   Const                aten_convolution_default_24_input_zp
289   Const                aten_convolution_default_24_shifts
290   Const                aten_convolution_default_24_multipliers
291   Const                layer-25_weight_zp            
292   Const                layer-25_input_zp             
293   Const                b__frozen_param47             
294   Const                b__frozen_param46             
295   Conv2D               layer-25                      
296   Rescale              aten_convolution_default_24   
297   Clamp                aten_clamp_default_24         
298   Concat               aten_cat_default_7            
299   Conv2D               layer-26                      
300   Rescale              aten_convolution_default_25   
301   Clamp                aten_clamp_default_25         
302   AvgPool              aten_avg_pool2d_default       
303   Reshape              aten_view_copy_default        


[ Graph With Tensor Quantization ]
0 Const const-1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const-1
1 Const const-0
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const-0
2 Const aten_convolution_default_25_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_output_zp
3 Const aten_convolution_default_25_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_input_zp
4 Const aten_convolution_default_25_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_shifts
5 Const aten_convolution_default_25_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_multipliers
6 Const layer-26_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26_weight_zp
7 Const layer-26_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26_input_zp
8 Const b__frozen_param51
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param51
9 Const b__frozen_param50
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param50
10 Const aten_convolution_default_23_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_output_zp
11 Const aten_convolution_default_23_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_input_zp
12 Const aten_convolution_default_23_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_shifts
13 Const aten_convolution_default_23_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_multipliers
14 Const layer-24_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24_weight_zp
15 Const layer-24_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24_input_zp
16 Const b__frozen_param49
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param49
17 Const b__frozen_param48
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param48
18 Const aten_convolution_default_22_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_output_zp
19 Const aten_convolution_default_22_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_input_zp
20 Const aten_convolution_default_22_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_shifts
21 Const aten_convolution_default_22_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_multipliers
22 Const layer-23_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23_weight_zp
23 Const layer-23_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23_input_zp
24 Const b__frozen_param45
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param45
25 Const b__frozen_param44
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param44
26 Const aten_convolution_default_20_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_output_zp
27 Const aten_convolution_default_20_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_input_zp
28 Const aten_convolution_default_20_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_shifts
29 Const aten_convolution_default_20_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_multipliers
30 Const layer-21_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21_weight_zp
31 Const layer-21_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21_input_zp
32 Const b__frozen_param43
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param43
33 Const b__frozen_param42
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param42
34 Const aten_convolution_default_19_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_output_zp
35 Const aten_convolution_default_19_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_input_zp
36 Const aten_convolution_default_19_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_shifts
37 Const aten_convolution_default_19_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_multipliers
38 Const layer-20_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20_weight_zp
39 Const layer-20_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20_input_zp
40 Const b__frozen_param39
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param39
41 Const b__frozen_param38
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param38
42 Const aten_convolution_default_17_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_output_zp
43 Const aten_convolution_default_17_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_input_zp
44 Const aten_convolution_default_17_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_shifts
45 Const aten_convolution_default_17_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_multipliers
46 Const layer-18_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18_weight_zp
47 Const layer-18_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18_input_zp
48 Const b__frozen_param37
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param37
49 Const b__frozen_param36
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param36
50 Const aten_convolution_default_16_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_output_zp
51 Const aten_convolution_default_16_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_input_zp
52 Const aten_convolution_default_16_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_shifts
53 Const aten_convolution_default_16_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_multipliers
54 Const layer-17_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17_weight_zp
55 Const layer-17_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17_input_zp
56 Const b__frozen_param33
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param33
57 Const b__frozen_param32
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param32
58 Const aten_convolution_default_14_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_output_zp
59 Const aten_convolution_default_14_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_input_zp
60 Const aten_convolution_default_14_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_shifts
61 Const aten_convolution_default_14_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_multipliers
62 Const layer-15_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15_weight_zp
63 Const layer-15_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15_input_zp
64 Const b__frozen_param31
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param31
65 Const b__frozen_param30
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param30
66 Const aten_convolution_default_13_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_output_zp
67 Const aten_convolution_default_13_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_input_zp
68 Const aten_convolution_default_13_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_shifts
69 Const aten_convolution_default_13_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_multipliers
70 Const layer-14_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14_weight_zp
71 Const layer-14_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14_input_zp
72 Const b__frozen_param27
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param27
73 Const b__frozen_param26
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param26
74 Const aten_convolution_default_11_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_output_zp
75 Const aten_convolution_default_11_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_input_zp
76 Const aten_convolution_default_11_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_shifts
77 Const aten_convolution_default_11_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_multipliers
78 Const layer-12_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12_weight_zp
79 Const layer-12_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12_input_zp
80 Const b__frozen_param25
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param25
81 Const b__frozen_param24
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param24
82 Const aten_convolution_default_10_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_output_zp
83 Const aten_convolution_default_10_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_input_zp
84 Const aten_convolution_default_10_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_shifts
85 Const aten_convolution_default_10_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_multipliers
86 Const layer-11_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11_weight_zp
87 Const layer-11_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11_input_zp
88 Const b__frozen_param21
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param21
89 Const b__frozen_param20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param20
90 Const aten_convolution_default_8_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_output_zp
91 Const aten_convolution_default_8_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_input_zp
92 Const aten_convolution_default_8_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_shifts
93 Const aten_convolution_default_8_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_multipliers
94 Const layer-9_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9_weight_zp
95 Const layer-9_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9_input_zp
96 Const b__frozen_param19
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param19
97 Const b__frozen_param18
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param18
98 Const aten_convolution_default_7_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_output_zp
99 Const aten_convolution_default_7_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_input_zp
100 Const aten_convolution_default_7_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_shifts
101 Const aten_convolution_default_7_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_multipliers
102 Const layer-8_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8_weight_zp
103 Const layer-8_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8_input_zp
104 Const b__frozen_param15
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param15
105 Const b__frozen_param14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param14
106 Const aten_convolution_default_5_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_output_zp
107 Const aten_convolution_default_5_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_input_zp
108 Const aten_convolution_default_5_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_shifts
109 Const aten_convolution_default_5_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_multipliers
110 Const layer-6_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6_weight_zp
111 Const layer-6_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6_input_zp
112 Const b__frozen_param13
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param13
113 Const b__frozen_param12
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param12
114 Const aten_convolution_default_4_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_output_zp
115 Const aten_convolution_default_4_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_input_zp
116 Const aten_convolution_default_4_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_shifts
117 Const aten_convolution_default_4_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_multipliers
118 Const layer-5_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_weight_zp
119 Const layer-5_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_input_zp
120 Const b__frozen_param9
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param9
121 Const b__frozen_param8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param8
122 Const aten_convolution_default_2_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_output_zp
123 Const aten_convolution_default_2_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_input_zp
124 Const aten_convolution_default_2_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_shifts
125 Const aten_convolution_default_2_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_multipliers
126 Const layer-3_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3_weight_zp
127 Const layer-3_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3_input_zp
128 Const b__frozen_param7
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param7
129 Const b__frozen_param6
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param6
130 Const aten_convolution_default_1_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_output_zp
131 Const aten_convolution_default_1_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_input_zp
132 Const aten_convolution_default_1_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_shifts
133 Const aten_convolution_default_1_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_multipliers
134 Const layer-2_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_weight_zp
135 Const layer-2_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_input_zp
136 Const b__frozen_param3
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param3
137 Const b__frozen_param2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param2
138 Const aten_convolution_default_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_output_zp
139 Const aten_convolution_default_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_input_zp
140 Const aten_convolution_default_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_shifts
141 Const aten_convolution_default_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_multipliers
142 Const layer-1_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_weight_zp
143 Const layer-1_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_input_zp
144 Const b__frozen_param1
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param1
145 Const b__frozen_param0
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param0
146 Transpose tosa_transpose_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 quantized_decomposed_quantize_per_tensor_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
147 Slice aten_slice_copy_tensor
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
    Input 01 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_start_shape
    Input 02 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_sizes_shape
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor
148 Slice aten_slice_copy_tensor_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor
    Input 01 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_1_start_shape
    Input 02 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_1_sizes_shape
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_1
149 Conv2D layer-1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-1], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param0
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param1
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1
150 Rescale aten_convolution_default
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default
151 Clamp aten_clamp_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default
152 MaxPool aten_max_pool2d_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default
153 Conv2D layer-2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-19], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param2
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param3
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2
154 Rescale aten_convolution_default_1
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1
155 Clamp aten_clamp_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
156 Conv2D layer-3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param6
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param7
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3
157 Rescale aten_convolution_default_2
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [41], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2
158 Clamp aten_clamp_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_2
159 Const aten_convolution_default_3_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_output_zp
160 Const aten_convolution_default_3_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_input_zp
161 Const aten_convolution_default_3_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_shifts
162 Const aten_convolution_default_3_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_multipliers
163 Const layer-4_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_weight_zp
164 Const layer-4_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_input_zp
165 Const b__frozen_param5
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param5
166 Const b__frozen_param4
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param4
167 Conv2D layer-4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param4
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param5
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4
168 Rescale aten_convolution_default_3
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [41], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3
169 Clamp aten_clamp_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_3
170 Concat aten_cat_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_3
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
171 Conv2D layer-5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [41], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param8
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param9
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5
172 Rescale aten_convolution_default_4
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4
173 Clamp aten_clamp_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
174 Conv2D layer-6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param12
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param13
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6
175 Rescale aten_convolution_default_5
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5
176 Clamp aten_clamp_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_5
177 Const aten_convolution_default_6_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_output_zp
178 Const aten_convolution_default_6_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_input_zp
179 Const aten_convolution_default_6_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_shifts
180 Const aten_convolution_default_6_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_multipliers
181 Const layer-7_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7_weight_zp
182 Const layer-7_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7_input_zp
183 Const b__frozen_param11
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param11
184 Const b__frozen_param10
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param10
185 Conv2D layer-7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param10
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param11
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7
186 Rescale aten_convolution_default_6
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-7
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6
187 Clamp aten_clamp_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_6
188 Concat aten_cat_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_6
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_5
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
189 MaxPool aten_max_pool2d_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_1
190 Conv2D layer-8
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param14
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param15
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8
191 Rescale aten_convolution_default_7
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-8
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7
192 Clamp aten_clamp_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
193 Conv2D layer-9
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param18
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param19
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9
194 Rescale aten_convolution_default_8
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-9
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [16], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8
195 Clamp aten_clamp_default_8
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_8
196 Const aten_convolution_default_9_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_output_zp
197 Const aten_convolution_default_9_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_input_zp
198 Const aten_convolution_default_9_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_shifts
199 Const aten_convolution_default_9_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_multipliers
200 Const layer-10_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10_weight_zp
201 Const layer-10_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10_input_zp
202 Const b__frozen_param17
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param17
203 Const b__frozen_param16
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param16
204 Conv2D layer-10
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param16
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param17
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10
205 Rescale aten_convolution_default_9
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-10
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [16], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9
206 Clamp aten_clamp_default_9
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_9
207 Concat aten_cat_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_9
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
208 Conv2D layer-11
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [16], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param20
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param21
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11
209 Rescale aten_convolution_default_10
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-11
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10
210 Clamp aten_clamp_default_10
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
211 Conv2D layer-12
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param24
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param25
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12
212 Rescale aten_convolution_default_11
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-12
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [30], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11
213 Clamp aten_clamp_default_11
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_11
214 Const aten_convolution_default_12_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_output_zp
215 Const aten_convolution_default_12_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_input_zp
216 Const aten_convolution_default_12_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_shifts
217 Const aten_convolution_default_12_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_multipliers
218 Const layer-13_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13_weight_zp
219 Const layer-13_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13_input_zp
220 Const b__frozen_param23
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param23
221 Const b__frozen_param22
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param22
222 Conv2D layer-13
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param22
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param23
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13
223 Rescale aten_convolution_default_12
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-13
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [30], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12
224 Clamp aten_clamp_default_12
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_12
225 Concat aten_cat_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_12
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_11
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
226 MaxPool aten_max_pool2d_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_2
227 Conv2D layer-14
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [30], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_2
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param26
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param27
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14
228 Rescale aten_convolution_default_13
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-14
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-14], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13
229 Clamp aten_clamp_default_13
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
230 Conv2D layer-15
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-14], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param30
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param31
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15
231 Rescale aten_convolution_default_14
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-15
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14
232 Clamp aten_clamp_default_14
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_14
233 Const aten_convolution_default_15_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_output_zp
234 Const aten_convolution_default_15_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_input_zp
235 Const aten_convolution_default_15_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_shifts
236 Const aten_convolution_default_15_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_multipliers
237 Const layer-16_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16_weight_zp
238 Const layer-16_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16_input_zp
239 Const b__frozen_param29
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param29
240 Const b__frozen_param28
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param28
241 Conv2D layer-16
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-14], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param28
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param29
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16
242 Rescale aten_convolution_default_15
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-16
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15
243 Clamp aten_clamp_default_15
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_15
244 Concat aten_cat_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_15
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
245 Conv2D layer-17
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param32
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param33
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17
246 Rescale aten_convolution_default_16
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-17
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-20], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16
247 Clamp aten_clamp_default_16
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
248 Conv2D layer-18
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-20], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param36
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param37
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18
249 Rescale aten_convolution_default_17
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-18
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17
250 Clamp aten_clamp_default_17
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_17
251 Const aten_convolution_default_18_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_output_zp
252 Const aten_convolution_default_18_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_input_zp
253 Const aten_convolution_default_18_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_shifts
254 Const aten_convolution_default_18_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_multipliers
255 Const layer-19_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19_weight_zp
256 Const layer-19_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19_input_zp
257 Const b__frozen_param35
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param35
258 Const b__frozen_param34
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param34
259 Conv2D layer-19
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-20], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param34
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param35
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19
260 Rescale aten_convolution_default_18
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-19
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18
261 Clamp aten_clamp_default_18
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_18
262 Concat aten_cat_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_18
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_17
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
263 Conv2D layer-20
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param38
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param39
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20
264 Rescale aten_convolution_default_19
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-20
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19
265 Clamp aten_clamp_default_19
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
266 Conv2D layer-21
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param42
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param43
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21
267 Rescale aten_convolution_default_20
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-21
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [15], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20
268 Clamp aten_clamp_default_20
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_20
269 Const aten_convolution_default_21_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_output_zp
270 Const aten_convolution_default_21_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_input_zp
271 Const aten_convolution_default_21_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_shifts
272 Const aten_convolution_default_21_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_multipliers
273 Const layer-22_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22_weight_zp
274 Const layer-22_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22_input_zp
275 Const b__frozen_param41
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param41
276 Const b__frozen_param40
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param40
277 Conv2D layer-22
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param40
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param41
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22
278 Rescale aten_convolution_default_21
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-22
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [15], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21
279 Clamp aten_clamp_default_21
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_21
280 Concat aten_cat_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_21
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
281 Conv2D layer-23
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [15], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param44
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param45
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23
282 Rescale aten_convolution_default_22
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-23
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22
283 Clamp aten_clamp_default_22
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
284 Conv2D layer-24
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param48
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param49
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24
285 Rescale aten_convolution_default_23
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-24
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [59], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23
286 Clamp aten_clamp_default_23
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_23
287 Const aten_convolution_default_24_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_output_zp
288 Const aten_convolution_default_24_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_input_zp
289 Const aten_convolution_default_24_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_shifts
290 Const aten_convolution_default_24_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_multipliers
291 Const layer-25_weight_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25_weight_zp
292 Const layer-25_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25_input_zp
293 Const b__frozen_param47
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param47
294 Const b__frozen_param46
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param46
295 Conv2D layer-25
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param46
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param47
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25
296 Rescale aten_convolution_default_24
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-25
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [59], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24
297 Clamp aten_clamp_default_24
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_24
298 Concat aten_cat_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_24
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_23
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
299 Conv2D layer-26
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [59], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param50
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param51
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26_weight_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26
300 Rescale aten_convolution_default_25
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-26
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25
301 Clamp aten_clamp_default_25
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_25
302 AvgPool aten_avg_pool2d_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_25
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const-0
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const-1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_avg_pool2d_default
303 Reshape aten_view_copy_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_avg_pool2d_default
    Input 01 Int64 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_view_copy_default_shape
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_view_copy_default


[ Before Graph Optimisation ]
0     Const                const-1                       
1     Const                const-0                       
2     Const                aten_convolution_default_25_output_zp
3     Const                aten_convolution_default_25_input_zp
4     Const                aten_convolution_default_25_shifts
5     Const                aten_convolution_default_25_multipliers
6     Const                layer-26_weight_zp            
7     Const                layer-26_input_zp             
8     Const                b__frozen_param51             
9     Const                b__frozen_param50             
10    Const                aten_convolution_default_23_output_zp
11    Const                aten_convolution_default_23_input_zp
12    Const                aten_convolution_default_23_shifts
13    Const                aten_convolution_default_23_multipliers
14    Const                layer-24_weight_zp            
15    Const                layer-24_input_zp             
16    Const                b__frozen_param49             
17    Const                b__frozen_param48             
18    Const                aten_convolution_default_22_output_zp
19    Const                aten_convolution_default_22_input_zp
20    Const                aten_convolution_default_22_shifts
21    Const                aten_convolution_default_22_multipliers
22    Const                layer-23_weight_zp            
23    Const                layer-23_input_zp             
24    Const                b__frozen_param45             
25    Const                b__frozen_param44             
26    Const                aten_convolution_default_20_output_zp
27    Const                aten_convolution_default_20_input_zp
28    Const                aten_convolution_default_20_shifts
29    Const                aten_convolution_default_20_multipliers
30    Const                layer-21_weight_zp            
31    Const                layer-21_input_zp             
32    Const                b__frozen_param43             
33    Const                b__frozen_param42             
34    Const                aten_convolution_default_19_output_zp
35    Const                aten_convolution_default_19_input_zp
36    Const                aten_convolution_default_19_shifts
37    Const                aten_convolution_default_19_multipliers
38    Const                layer-20_weight_zp            
39    Const                layer-20_input_zp             
40    Const                b__frozen_param39             
41    Const                b__frozen_param38             
42    Const                aten_convolution_default_17_output_zp
43    Const                aten_convolution_default_17_input_zp
44    Const                aten_convolution_default_17_shifts
45    Const                aten_convolution_default_17_multipliers
46    Const                layer-18_weight_zp            
47    Const                layer-18_input_zp             
48    Const                b__frozen_param37             
49    Const                b__frozen_param36             
50    Const                aten_convolution_default_16_output_zp
51    Const                aten_convolution_default_16_input_zp
52    Const                aten_convolution_default_16_shifts
53    Const                aten_convolution_default_16_multipliers
54    Const                layer-17_weight_zp            
55    Const                layer-17_input_zp             
56    Const                b__frozen_param33             
57    Const                b__frozen_param32             
58    Const                aten_convolution_default_14_output_zp
59    Const                aten_convolution_default_14_input_zp
60    Const                aten_convolution_default_14_shifts
61    Const                aten_convolution_default_14_multipliers
62    Const                layer-15_weight_zp            
63    Const                layer-15_input_zp             
64    Const                b__frozen_param31             
65    Const                b__frozen_param30             
66    Const                aten_convolution_default_13_output_zp
67    Const                aten_convolution_default_13_input_zp
68    Const                aten_convolution_default_13_shifts
69    Const                aten_convolution_default_13_multipliers
70    Const                layer-14_weight_zp            
71    Const                layer-14_input_zp             
72    Const                b__frozen_param27             
73    Const                b__frozen_param26             
74    Const                aten_convolution_default_11_output_zp
75    Const                aten_convolution_default_11_input_zp
76    Const                aten_convolution_default_11_shifts
77    Const                aten_convolution_default_11_multipliers
78    Const                layer-12_weight_zp            
79    Const                layer-12_input_zp             
80    Const                b__frozen_param25             
81    Const                b__frozen_param24             
82    Const                aten_convolution_default_10_output_zp
83    Const                aten_convolution_default_10_input_zp
84    Const                aten_convolution_default_10_shifts
85    Const                aten_convolution_default_10_multipliers
86    Const                layer-11_weight_zp            
87    Const                layer-11_input_zp             
88    Const                b__frozen_param21             
89    Const                b__frozen_param20             
90    Const                aten_convolution_default_8_output_zp
91    Const                aten_convolution_default_8_input_zp
92    Const                aten_convolution_default_8_shifts
93    Const                aten_convolution_default_8_multipliers
94    Const                layer-9_weight_zp             
95    Const                layer-9_input_zp              
96    Const                b__frozen_param19             
97    Const                b__frozen_param18             
98    Const                aten_convolution_default_7_output_zp
99    Const                aten_convolution_default_7_input_zp
100   Const                aten_convolution_default_7_shifts
101   Const                aten_convolution_default_7_multipliers
102   Const                layer-8_weight_zp             
103   Const                layer-8_input_zp              
104   Const                b__frozen_param15             
105   Const                b__frozen_param14             
106   Const                aten_convolution_default_5_output_zp
107   Const                aten_convolution_default_5_input_zp
108   Const                aten_convolution_default_5_shifts
109   Const                aten_convolution_default_5_multipliers
110   Const                layer-6_weight_zp             
111   Const                layer-6_input_zp              
112   Const                b__frozen_param13             
113   Const                b__frozen_param12             
114   Const                aten_convolution_default_4_output_zp
115   Const                aten_convolution_default_4_input_zp
116   Const                aten_convolution_default_4_shifts
117   Const                aten_convolution_default_4_multipliers
118   Const                layer-5_weight_zp             
119   Const                layer-5_input_zp              
120   Const                b__frozen_param9              
121   Const                b__frozen_param8              
122   Const                aten_convolution_default_2_output_zp
123   Const                aten_convolution_default_2_input_zp
124   Const                aten_convolution_default_2_shifts
125   Const                aten_convolution_default_2_multipliers
126   Const                layer-3_weight_zp             
127   Const                layer-3_input_zp              
128   Const                b__frozen_param7              
129   Const                b__frozen_param6              
130   Const                aten_convolution_default_1_output_zp
131   Const                aten_convolution_default_1_input_zp
132   Const                aten_convolution_default_1_shifts
133   Const                aten_convolution_default_1_multipliers
134   Const                layer-2_weight_zp             
135   Const                layer-2_input_zp              
136   Const                b__frozen_param3              
137   Const                b__frozen_param2              
138   Const                aten_convolution_default_output_zp
139   Const                aten_convolution_default_input_zp
140   Const                aten_convolution_default_shifts
141   Const                aten_convolution_default_multipliers
142   Const                layer-1_weight_zp             
143   Const                layer-1_input_zp              
144   Const                b__frozen_param1              
145   Const                b__frozen_param0              
146   Transpose            tosa_transpose_default        
147   Slice                aten_slice_copy_tensor        
148   Slice                aten_slice_copy_tensor_1      
149   Conv2D               layer-1                       
150   Rescale              aten_convolution_default      
151   Clamp                aten_clamp_default            
152   MaxPool              aten_max_pool2d_default       
153   Conv2D               layer-2                       
154   Rescale              aten_convolution_default_1    
155   Clamp                aten_clamp_default_1          
156   Conv2D               layer-3                       
157   Rescale              aten_convolution_default_2    
158   Clamp                aten_clamp_default_2          
159   Const                aten_convolution_default_3_output_zp
160   Const                aten_convolution_default_3_input_zp
161   Const                aten_convolution_default_3_shifts
162   Const                aten_convolution_default_3_multipliers
163   Const                layer-4_weight_zp             
164   Const                layer-4_input_zp              
165   Const                b__frozen_param5              
166   Const                b__frozen_param4              
167   Conv2D               layer-4                       
168   Rescale              aten_convolution_default_3    
169   Clamp                aten_clamp_default_3          
170   Concat               aten_cat_default              
171   Conv2D               layer-5                       
172   Rescale              aten_convolution_default_4    
173   Clamp                aten_clamp_default_4          
174   Conv2D               layer-6                       
175   Rescale              aten_convolution_default_5    
176   Clamp                aten_clamp_default_5          
177   Const                aten_convolution_default_6_output_zp
178   Const                aten_convolution_default_6_input_zp
179   Const                aten_convolution_default_6_shifts
180   Const                aten_convolution_default_6_multipliers
181   Const                layer-7_weight_zp             
182   Const                layer-7_input_zp              
183   Const                b__frozen_param11             
184   Const                b__frozen_param10             
185   Conv2D               layer-7                       
186   Rescale              aten_convolution_default_6    
187   Clamp                aten_clamp_default_6          
188   Concat               aten_cat_default_1            
189   MaxPool              aten_max_pool2d_default_1     
190   Conv2D               layer-8                       
191   Rescale              aten_convolution_default_7    
192   Clamp                aten_clamp_default_7          
193   Conv2D               layer-9                       
194   Rescale              aten_convolution_default_8    
195   Clamp                aten_clamp_default_8          
196   Const                aten_convolution_default_9_output_zp
197   Const                aten_convolution_default_9_input_zp
198   Const                aten_convolution_default_9_shifts
199   Const                aten_convolution_default_9_multipliers
200   Const                layer-10_weight_zp            
201   Const                layer-10_input_zp             
202   Const                b__frozen_param17             
203   Const                b__frozen_param16             
204   Conv2D               layer-10                      
205   Rescale              aten_convolution_default_9    
206   Clamp                aten_clamp_default_9          
207   Concat               aten_cat_default_2            
208   Conv2D               layer-11                      
209   Rescale              aten_convolution_default_10   
210   Clamp                aten_clamp_default_10         
211   Conv2D               layer-12                      
212   Rescale              aten_convolution_default_11   
213   Clamp                aten_clamp_default_11         
214   Const                aten_convolution_default_12_output_zp
215   Const                aten_convolution_default_12_input_zp
216   Const                aten_convolution_default_12_shifts
217   Const                aten_convolution_default_12_multipliers
218   Const                layer-13_weight_zp            
219   Const                layer-13_input_zp             
220   Const                b__frozen_param23             
221   Const                b__frozen_param22             
222   Conv2D               layer-13                      
223   Rescale              aten_convolution_default_12   
224   Clamp                aten_clamp_default_12         
225   Concat               aten_cat_default_3            
226   MaxPool              aten_max_pool2d_default_2     
227   Conv2D               layer-14                      
228   Rescale              aten_convolution_default_13   
229   Clamp                aten_clamp_default_13         
230   Conv2D               layer-15                      
231   Rescale              aten_convolution_default_14   
232   Clamp                aten_clamp_default_14         
233   Const                aten_convolution_default_15_output_zp
234   Const                aten_convolution_default_15_input_zp
235   Const                aten_convolution_default_15_shifts
236   Const                aten_convolution_default_15_multipliers
237   Const                layer-16_weight_zp            
238   Const                layer-16_input_zp             
239   Const                b__frozen_param29             
240   Const                b__frozen_param28             
241   Conv2D               layer-16                      
242   Rescale              aten_convolution_default_15   
243   Clamp                aten_clamp_default_15         
244   Concat               aten_cat_default_4            
245   Conv2D               layer-17                      
246   Rescale              aten_convolution_default_16   
247   Clamp                aten_clamp_default_16         
248   Conv2D               layer-18                      
249   Rescale              aten_convolution_default_17   
250   Clamp                aten_clamp_default_17         
251   Const                aten_convolution_default_18_output_zp
252   Const                aten_convolution_default_18_input_zp
253   Const                aten_convolution_default_18_shifts
254   Const                aten_convolution_default_18_multipliers
255   Const                layer-19_weight_zp            
256   Const                layer-19_input_zp             
257   Const                b__frozen_param35             
258   Const                b__frozen_param34             
259   Conv2D               layer-19                      
260   Rescale              aten_convolution_default_18   
261   Clamp                aten_clamp_default_18         
262   Concat               aten_cat_default_5            
263   Conv2D               layer-20                      
264   Rescale              aten_convolution_default_19   
265   Clamp                aten_clamp_default_19         
266   Conv2D               layer-21                      
267   Rescale              aten_convolution_default_20   
268   Clamp                aten_clamp_default_20         
269   Const                aten_convolution_default_21_output_zp
270   Const                aten_convolution_default_21_input_zp
271   Const                aten_convolution_default_21_shifts
272   Const                aten_convolution_default_21_multipliers
273   Const                layer-22_weight_zp            
274   Const                layer-22_input_zp             
275   Const                b__frozen_param41             
276   Const                b__frozen_param40             
277   Conv2D               layer-22                      
278   Rescale              aten_convolution_default_21   
279   Clamp                aten_clamp_default_21         
280   Concat               aten_cat_default_6            
281   Conv2D               layer-23                      
282   Rescale              aten_convolution_default_22   
283   Clamp                aten_clamp_default_22         
284   Conv2D               layer-24                      
285   Rescale              aten_convolution_default_23   
286   Clamp                aten_clamp_default_23         
287   Const                aten_convolution_default_24_output_zp
288   Const                aten_convolution_default_24_input_zp
289   Const                aten_convolution_default_24_shifts
290   Const                aten_convolution_default_24_multipliers
291   Const                layer-25_weight_zp            
292   Const                layer-25_input_zp             
293   Const                b__frozen_param47             
294   Const                b__frozen_param46             
295   Conv2D               layer-25                      
296   Rescale              aten_convolution_default_24   
297   Clamp                aten_clamp_default_24         
298   Concat               aten_cat_default_7            
299   Conv2D               layer-26                      
300   Rescale              aten_convolution_default_25   
301   Clamp                aten_clamp_default_25         
302   AvgPool              aten_avg_pool2d_default       
303   Reshape              aten_view_copy_default        


[ After Graph Optimization ]
0     Transpose            tosa_transpose_default        
1     MemoryCopy           aten_slice_copy_tensor        
2     Conv2D               aten_convolution_default      
3     Clamp                aten_clamp_default            
4     MaxPool              aten_max_pool2d_default       
5     Conv2D               aten_convolution_default_1    
6     Clamp                aten_clamp_default_1          
7     Conv2D               aten_convolution_default_2    
8     Clamp                aten_clamp_default_2          
9     MemoryCopy           aten_cat_default              
10    Conv2D               aten_convolution_default_3    
11    Clamp                aten_clamp_default_3          
12    MemoryCopy           aten_cat_default              
13    Conv2D               aten_convolution_default_4    
14    Clamp                aten_clamp_default_4          
15    Conv2D               aten_convolution_default_5    
16    Clamp                aten_clamp_default_5          
17    MemoryCopy           aten_cat_default_1            
18    Conv2D               aten_convolution_default_6    
19    Clamp                aten_clamp_default_6          
20    MemoryCopy           aten_cat_default_1            
21    MaxPool              aten_max_pool2d_default_1     
22    Conv2D               aten_convolution_default_7    
23    Clamp                aten_clamp_default_7          
24    Conv2D               aten_convolution_default_8    
25    Clamp                aten_clamp_default_8          
26    MemoryCopy           aten_cat_default_2            
27    Conv2D               aten_convolution_default_9    
28    Clamp                aten_clamp_default_9          
29    MemoryCopy           aten_cat_default_2            
30    Conv2D               aten_convolution_default_10   
31    Clamp                aten_clamp_default_10         
32    Conv2D               aten_convolution_default_11   
33    Clamp                aten_clamp_default_11         
34    MemoryCopy           aten_cat_default_3            
35    Conv2D               aten_convolution_default_12   
36    Clamp                aten_clamp_default_12         
37    MemoryCopy           aten_cat_default_3            
38    MaxPool              aten_max_pool2d_default_2     
39    Conv2D               aten_convolution_default_13   
40    Clamp                aten_clamp_default_13         
41    Conv2D               aten_convolution_default_14   
42    Clamp                aten_clamp_default_14         
43    MemoryCopy           aten_cat_default_4            
44    Conv2D               aten_convolution_default_15   
45    Clamp                aten_clamp_default_15         
46    MemoryCopy           aten_cat_default_4            
47    Conv2D               aten_convolution_default_16   
48    Clamp                aten_clamp_default_16         
49    Conv2D               aten_convolution_default_17   
50    Clamp                aten_clamp_default_17         
51    MemoryCopy           aten_cat_default_5            
52    Conv2D               aten_convolution_default_18   
53    Clamp                aten_clamp_default_18         
54    MemoryCopy           aten_cat_default_5            
55    Conv2D               aten_convolution_default_19   
56    Clamp                aten_clamp_default_19         
57    Conv2D               aten_convolution_default_20   
58    Clamp                aten_clamp_default_20         
59    MemoryCopy           aten_cat_default_6            
60    Conv2D               aten_convolution_default_21   
61    Clamp                aten_clamp_default_21         
62    MemoryCopy           aten_cat_default_6            
63    Conv2D               aten_convolution_default_22   
64    Clamp                aten_clamp_default_22         
65    Conv2D               aten_convolution_default_23   
66    Clamp                aten_clamp_default_23         
67    MemoryCopy           aten_cat_default_7            
68    Conv2D               aten_convolution_default_24   
69    Clamp                aten_clamp_default_24         
70    MemoryCopy           aten_cat_default_7            
71    Conv2D               aten_convolution_default_25   
72    Clamp                aten_clamp_default_25         
73    AvgPool              aten_view_copy_default        


[ Graph With Tensor Quantization ]
0 Transpose tosa_transpose_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 quantized_decomposed_quantize_per_tensor_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
1 MemoryCopy aten_slice_copy_tensor
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor
2 Conv2D aten_convolution_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-1], quantMin: [], quantMax: [], dimension: 0 aten_slice_copy_tensor
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1743041885, shift:40), (scale:1910871053, shift:40), (scale:1809976044, shift:39), (scale:1310748782, shift:39), (scale:1160601846, shift:39), (scale:1280836543, shift:40), (scale:1410239818, shift:39), (scale:1154759928, shift:40), (scale:1807366659, shift:39), (scale:1120393557, shift:39), (scale:1631946869, shift:39), (scale:1410889346, shift:41), (scale:1371421113, shift:40), (scale:2056718895, shift:40), (scale:1360222414, shift:39), (scale:1608999584, shift:39), (scale:1956541401, shift:41), (scale:1554976975, shift:39), (scale:1266159623, shift:39), (scale:1161805458, shift:39), (scale:1371134060, shift:39), (scale:1899753770, shift:40), (scale:1505585478, shift:39), (scale:1129383955, shift:39), (scale:1255190903, shift:40), (scale:1406137369, shift:39), (scale:2104961217, shift:41), (scale:1888711907, shift:39), (scale:1442770675, shift:39), (scale:1608696223, shift:39), (scale:1583384400, shift:41), (scale:1484524547, shift:39), (scale:1518870774, shift:39), (scale:1466868814, shift:39), (scale:1960604400, shift:40), (scale:2114759899, shift:40), (scale:2098383198, shift:40), (scale:1198127489, shift:40), (scale:1825925281, shift:39), (scale:2005272337, shift:41), (scale:1272910786, shift:39), (scale:1512838685, shift:39), (scale:1536078061, shift:40), (scale:1465513881, shift:40), (scale:1165437637, shift:40), (scale:1484408358, shift:40), (scale:1192602001, shift:40), (scale:1241190850, shift:39), (scale:1395234836, shift:40), (scale:1267606044, shift:39), (scale:2045951856, shift:40), (scale:1849271133, shift:40), (scale:1446045056, shift:41), (scale:1135410768, shift:39), (scale:1721950018, shift:40), (scale:1129043423, shift:39), (scale:1517472915, shift:40), (scale:1572664483, shift:40), (scale:2082281081, shift:40), (scale:1418716541, shift:40), (scale:1270744452, shift:39), (scale:1214494358, shift:39), (scale:1661257303, shift:41), (scale:1160983626, shift:40)], zero_point: [-19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default
3 Clamp aten_clamp_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-19], quantMax: [127], dimension: 0 aten_clamp_default
4 MaxPool aten_max_pool2d_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default
5 Conv2D aten_convolution_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-19], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1181452645, shift:38), (scale:1344568138, shift:39), (scale:1720366593, shift:39), (scale:2115104005, shift:40), (scale:1442768802, shift:39), (scale:1943231317, shift:39), (scale:2058663773, shift:40), (scale:1976676629, shift:39), (scale:1905189624, shift:39), (scale:1576406263, shift:39), (scale:1113223746, shift:38), (scale:1536290907, shift:39), (scale:1219798366, shift:39), (scale:1246374744, shift:39), (scale:1380059656, shift:39), (scale:1076818042, shift:38)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1
6 Clamp aten_clamp_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [6], quantMax: [127], dimension: 0 aten_clamp_default_1
7 Conv2D aten_convolution_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1236309437, shift:39), (scale:1975283516, shift:39), (scale:1797069587, shift:40), (scale:2105703532, shift:40), (scale:1415101508, shift:39), (scale:1166008172, shift:40), (scale:1538817975, shift:39), (scale:1475473506, shift:39), (scale:1641417624, shift:40), (scale:1444973142, shift:39), (scale:1678694836, shift:39), (scale:1153228048, shift:39), (scale:2074220019, shift:39), (scale:1548347087, shift:40), (scale:1787865370, shift:40), (scale:1539912727, shift:39), (scale:2108976845, shift:39), (scale:1424996088, shift:39), (scale:1207724993, shift:39), (scale:1107986835, shift:38), (scale:1940562411, shift:38), (scale:2050057221, shift:39), (scale:2064775667, shift:39), (scale:1599775547, shift:39), (scale:1444540053, shift:39), (scale:1329922911, shift:39), (scale:1100210937, shift:38), (scale:1196960518, shift:38), (scale:1529035101, shift:39), (scale:1997013165, shift:40), (scale:1516264785, shift:39), (scale:1316096041, shift:39), (scale:1442916714, shift:39), (scale:1079283976, shift:39), (scale:1169991669, shift:39), (scale:1196674647, shift:38), (scale:1563944721, shift:39), (scale:2143010994, shift:39), (scale:2118491606, shift:40), (scale:1937805090, shift:40), (scale:1770953909, shift:39), (scale:1317377243, shift:39), (scale:1276205836, shift:39), (scale:1689015278, shift:40), (scale:2058825560, shift:40), (scale:1961715364, shift:40), (scale:2020419011, shift:40), (scale:1823323605, shift:39), (scale:1604508047, shift:40), (scale:1679437025, shift:40), (scale:1562799279, shift:39), (scale:1849644935, shift:40), (scale:1960247298, shift:39), (scale:1813954824, shift:39), (scale:1942230349, shift:40), (scale:1315709409, shift:39), (scale:1363138473, shift:39), (scale:1912769711, shift:40), (scale:1300294199, shift:39), (scale:2097050408, shift:40), (scale:1911133778, shift:39), (scale:1530811853, shift:39), (scale:1373081059, shift:40), (scale:1136924358, shift:39)], zero_point: [41], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2
8 Clamp aten_clamp_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [41], quantMax: [127], dimension: 0 aten_clamp_default_2
9 MemoryCopy aten_cat_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_2
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
10 Conv2D aten_convolution_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1542696375, shift:39), (scale:1321590448, shift:39), (scale:2024456090, shift:40), (scale:2014390029, shift:40), (scale:1647642244, shift:39), (scale:1380916836, shift:39), (scale:1169811206, shift:38), (scale:1968577076, shift:39), (scale:1467277939, shift:39), (scale:1129099732, shift:38), (scale:1298639682, shift:39), (scale:1144651940, shift:38), (scale:2097091497, shift:39), (scale:1427943576, shift:39), (scale:1964612678, shift:39), (scale:1770665871, shift:38), (scale:1752397018, shift:39), (scale:1486624304, shift:39), (scale:1343112070, shift:39), (scale:1179384608, shift:38), (scale:1723260242, shift:39), (scale:1552563183, shift:39), (scale:1229334988, shift:39), (scale:1590947742, shift:39), (scale:1245209407, shift:39), (scale:1295041269, shift:39), (scale:1219356888, shift:39), (scale:1427263021, shift:39), (scale:1308324376, shift:39), (scale:1511999340, shift:39), (scale:2136302283, shift:39), (scale:1179026367, shift:39), (scale:1262756101, shift:38), (scale:1502138623, shift:39), (scale:1847264645, shift:39), (scale:1100151575, shift:38), (scale:1634730180, shift:39), (scale:1951534502, shift:38), (scale:1632658163, shift:38), (scale:1264350016, shift:38), (scale:1732071839, shift:43), (scale:1110676844, shift:38), (scale:1586801230, shift:39), (scale:1898801816, shift:39), (scale:1209890856, shift:39), (scale:1293898613, shift:38), (scale:2016824417, shift:39), (scale:1438336183, shift:39), (scale:1183635392, shift:38), (scale:1838279709, shift:39), (scale:1979107508, shift:39), (scale:1150217893, shift:39), (scale:1127959864, shift:38), (scale:1485234287, shift:38), (scale:2037849766, shift:38), (scale:1722567298, shift:39), (scale:1698059887, shift:38), (scale:2025033405, shift:39), (scale:1804267446, shift:38), (scale:1194549668, shift:38), (scale:1765160107, shift:39), (scale:1769436701, shift:39), (scale:1578325161, shift:39), (scale:1570460271, shift:38)], zero_point: [41], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3
11 Clamp aten_clamp_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [41], quantMax: [127], dimension: 0 aten_clamp_default_3
12 MemoryCopy aten_cat_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
13 Conv2D aten_convolution_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [41], quantMin: [], quantMax: [], dimension: 0 aten_cat_default
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1534003179, shift:38), (scale:1348228087, shift:38), (scale:1074333733, shift:38), (scale:1558320147, shift:38), (scale:1711133596, shift:38), (scale:1722069207, shift:38), (scale:1333913456, shift:38), (scale:1412249336, shift:38), (scale:1207984049, shift:38), (scale:1434696407, shift:38), (scale:1121212338, shift:37), (scale:1750904435, shift:38), (scale:1771902111, shift:38), (scale:1084767888, shift:37), (scale:1414563841, shift:38), (scale:1683850076, shift:38)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4
14 Clamp aten_clamp_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_4
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-24], quantMax: [127], dimension: 0 aten_clamp_default_4
15 Conv2D aten_convolution_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1547903862, shift:39), (scale:1164245419, shift:39), (scale:1227210409, shift:39), (scale:1245711617, shift:39), (scale:1500725681, shift:39), (scale:1563417106, shift:39), (scale:1206114458, shift:39), (scale:1242991299, shift:39), (scale:2021049520, shift:40), (scale:1607508437, shift:39), (scale:1974288431, shift:40), (scale:1829038100, shift:40), (scale:1259846652, shift:39), (scale:1381462186, shift:39), (scale:1775707374, shift:40), (scale:1375304692, shift:39), (scale:1773313121, shift:40), (scale:1625536753, shift:39), (scale:1163266155, shift:39), (scale:2034306217, shift:40), (scale:1528781868, shift:39), (scale:1279759549, shift:39), (scale:1455174333, shift:39), (scale:1283261453, shift:39), (scale:1488853889, shift:39), (scale:1146819548, shift:39), (scale:1119022951, shift:39), (scale:1615181554, shift:40), (scale:1429212437, shift:39), (scale:1533902843, shift:39), (scale:1086438626, shift:39), (scale:1997857725, shift:40), (scale:2105378981, shift:39), (scale:1932288674, shift:40), (scale:1294093426, shift:39), (scale:1772637700, shift:40), (scale:1401537242, shift:39), (scale:1394913260, shift:39), (scale:1176722275, shift:39), (scale:1134991231, shift:38), (scale:1099266003, shift:39), (scale:1960395907, shift:40), (scale:1140571900, shift:39), (scale:1970861250, shift:40), (scale:1273947723, shift:39), (scale:1169419573, shift:39), (scale:2120355479, shift:40), (scale:1937445943, shift:40), (scale:1444267831, shift:39), (scale:1333539585, shift:39), (scale:1097432079, shift:39), (scale:1114364970, shift:39), (scale:1190337547, shift:39), (scale:1143908443, shift:39), (scale:1980814787, shift:40), (scale:1348285897, shift:39), (scale:1211670672, shift:39), (scale:1257670554, shift:39), (scale:1301546351, shift:39), (scale:1502956318, shift:39), (scale:1816702926, shift:39), (scale:1089251911, shift:39), (scale:1125438775, shift:39), (scale:1799660765, shift:40)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5
16 Clamp aten_clamp_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_5
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [19], quantMax: [127], dimension: 0 aten_clamp_default_5
17 MemoryCopy aten_cat_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_5
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
18 Conv2D aten_convolution_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:2034062832, shift:39), (scale:1799134984, shift:39), (scale:1899333741, shift:39), (scale:1791897301, shift:39), (scale:1411497086, shift:38), (scale:1079341268, shift:39), (scale:1683662777, shift:39), (scale:1875630528, shift:38), (scale:2115952430, shift:39), (scale:1743478518, shift:40), (scale:1832754276, shift:39), (scale:1339918824, shift:39), (scale:1180615749, shift:38), (scale:1271884970, shift:39), (scale:1242178756, shift:38), (scale:1151859686, shift:39), (scale:1750822561, shift:38), (scale:1539012658, shift:42), (scale:1845035299, shift:39), (scale:1348928615, shift:39), (scale:1238964973, shift:39), (scale:1505112911, shift:39), (scale:1314631767, shift:38), (scale:2036316178, shift:39), (scale:1439387233, shift:39), (scale:1796852720, shift:39), (scale:1941547575, shift:39), (scale:1337464793, shift:38), (scale:1266495186, shift:39), (scale:2062372885, shift:39), (scale:1220131586, shift:38), (scale:1834479512, shift:39), (scale:1229587194, shift:39), (scale:1307776628, shift:38), (scale:1416369436, shift:39), (scale:2135665224, shift:39), (scale:1958050953, shift:39), (scale:1226157101, shift:39), (scale:1754897991, shift:38), (scale:1855112081, shift:38), (scale:1160690140, shift:39), (scale:1264505410, shift:39), (scale:1190540950, shift:38), (scale:1424000242, shift:38), (scale:1319408723, shift:38), (scale:1396257989, shift:40), (scale:1087056889, shift:39), (scale:1311541811, shift:39), (scale:1083279964, shift:39), (scale:1340610742, shift:39), (scale:1343781729, shift:39), (scale:1595424024, shift:39), (scale:1729907789, shift:39), (scale:1857051782, shift:39), (scale:1126876471, shift:39), (scale:1202057369, shift:38), (scale:1954738283, shift:39), (scale:1313152341, shift:39), (scale:1686865205, shift:39), (scale:1444466964, shift:39), (scale:2042932590, shift:40), (scale:1480112811, shift:40), (scale:1432004955, shift:39), (scale:1108013195, shift:39)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6
19 Clamp aten_clamp_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_6
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [19], quantMax: [127], dimension: 0 aten_clamp_default_6
20 MemoryCopy aten_cat_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_6
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
21 MaxPool aten_max_pool2d_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_1
22 Conv2D aten_convolution_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1372429351, shift:39), (scale:1160812588, shift:39), (scale:1103123400, shift:39), (scale:1264579002, shift:39), (scale:2115379644, shift:40), (scale:1530749425, shift:39), (scale:1227694897, shift:39), (scale:1620834574, shift:40), (scale:1633121595, shift:40), (scale:1779782645, shift:40), (scale:1578428923, shift:40), (scale:1656166077, shift:40), (scale:1167042777, shift:39), (scale:1901769660, shift:40), (scale:1295999817, shift:39), (scale:2138602700, shift:40), (scale:1243446552, shift:39), (scale:1435285220, shift:39), (scale:1320287685, shift:39), (scale:1673571630, shift:39), (scale:1605635704, shift:39), (scale:1105219874, shift:39), (scale:1254536162, shift:39), (scale:2089195070, shift:39), (scale:1932200875, shift:40), (scale:1438660404, shift:39), (scale:1211038853, shift:38), (scale:1376521981, shift:39), (scale:1122339362, shift:39), (scale:1237691545, shift:39), (scale:1205360036, shift:39), (scale:1771747047, shift:39)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7
23 Clamp aten_clamp_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_7
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-37], quantMax: [127], dimension: 0 aten_clamp_default_7
24 Conv2D aten_convolution_default_8
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1269507840, shift:39), (scale:1475195063, shift:40), (scale:1189839579, shift:39), (scale:1144533646, shift:40), (scale:1088699367, shift:38), (scale:1747626512, shift:40), (scale:1708553456, shift:40), (scale:1138760751, shift:39), (scale:1649291671, shift:39), (scale:1931252363, shift:40), (scale:1788417528, shift:40), (scale:1729635486, shift:40), (scale:1096434538, shift:40), (scale:1711773804, shift:40), (scale:1199408155, shift:40), (scale:2056192046, shift:40), (scale:1467409876, shift:39), (scale:1820132003, shift:40), (scale:1079657620, shift:40), (scale:1252140083, shift:39), (scale:1637229453, shift:40), (scale:1125264315, shift:39), (scale:1210646660, shift:40), (scale:1239907487, shift:40), (scale:1598020057, shift:39), (scale:1146102691, shift:40), (scale:1312941224, shift:40), (scale:1213336209, shift:39), (scale:1521865437, shift:40), (scale:1148561918, shift:39), (scale:1934419386, shift:41), (scale:1410517780, shift:40), (scale:1143382036, shift:39), (scale:1253566170, shift:39), (scale:1264410215, shift:40), (scale:1185311331, shift:40), (scale:1151465526, shift:39), (scale:1202088627, shift:39), (scale:1222751710, shift:40), (scale:1404279229, shift:40), (scale:2064845763, shift:41), (scale:1194868939, shift:39), (scale:1449733132, shift:39), (scale:1470104719, shift:40), (scale:1442463144, shift:40), (scale:1300678278, shift:40), (scale:1189564725, shift:39), (scale:1673413272, shift:40), (scale:1225020892, shift:40), (scale:1952687819, shift:41), (scale:1676243132, shift:40), (scale:1169667302, shift:39), (scale:1261817674, shift:39), (scale:1753779400, shift:40), (scale:1347627950, shift:40), (scale:1420685136, shift:40), (scale:1119107266, shift:40), (scale:1271619976, shift:40), (scale:1208481009, shift:39), (scale:2144948667, shift:40), (scale:1135052339, shift:40), (scale:1115344677, shift:39), (scale:1338574006, shift:40), (scale:1389161935, shift:40), (scale:1244916802, shift:40), (scale:1466405290, shift:39), (scale:1821601487, shift:39), (scale:1257631411, shift:40), (scale:1499677936, shift:40), (scale:1680487260, shift:40), (scale:1610888024, shift:40), (scale:2024284880, shift:41), (scale:1471260395, shift:40), (scale:1170461516, shift:40), (scale:1138987386, shift:40), (scale:1413982727, shift:40), (scale:2139180972, shift:40), (scale:1736862455, shift:40), (scale:1857654450, shift:40), (scale:1314523318, shift:39), (scale:1529417844, shift:40), (scale:2142564134, shift:41), (scale:1345412093, shift:40), (scale:1854444313, shift:40), (scale:1452282842, shift:40), (scale:1369487554, shift:40), (scale:1736535692, shift:40), (scale:1334157704, shift:39), (scale:1234722499, shift:39), (scale:1793540586, shift:40), (scale:1534248083, shift:40), (scale:1278784164, shift:40), (scale:1482306681, shift:40), (scale:1200792640, shift:39), (scale:1441816521, shift:40), (scale:1465264836, shift:40), (scale:2041227921, shift:40), (scale:1074308684, shift:40), (scale:1848010613, shift:40), (scale:1782791657, shift:40), (scale:1985927184, shift:40), (scale:1419053310, shift:40), (scale:1374924894, shift:40), (scale:1175474897, shift:40), (scale:1281203112, shift:40), (scale:2146139042, shift:39), (scale:1355303650, shift:40), (scale:1090006038, shift:40), (scale:1537457464, shift:40), (scale:1795205977, shift:41), (scale:1176337943, shift:40), (scale:1833292127, shift:40), (scale:1420040026, shift:40), (scale:1252060000, shift:39), (scale:1385301582, shift:40), (scale:1379018783, shift:40), (scale:1197037143, shift:40), (scale:1194594084, shift:40), (scale:2107029112, shift:41), (scale:2101252530, shift:40), (scale:2083563495, shift:40), (scale:1224509854, shift:40), (scale:1556563130, shift:39), (scale:1504490116, shift:40), (scale:1074009719, shift:40), (scale:1367996606, shift:40), (scale:1982613421, shift:40), (scale:1811975047, shift:40)], zero_point: [16], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8
25 Clamp aten_clamp_default_8
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [16], quantMax: [127], dimension: 0 aten_clamp_default_8
26 MemoryCopy aten_cat_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_8
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
27 Conv2D aten_convolution_default_9
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-37], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1903094265, shift:39), (scale:1835419297, shift:40), (scale:1909779844, shift:39), (scale:1077098171, shift:39), (scale:1164985127, shift:39), (scale:1826234402, shift:40), (scale:1247006908, shift:39), (scale:1647132733, shift:40), (scale:1423051232, shift:39), (scale:1324362210, shift:40), (scale:1380233741, shift:39), (scale:1706737543, shift:40), (scale:1139031540, shift:39), (scale:1948954635, shift:40), (scale:2085329297, shift:42), (scale:1276661060, shift:39), (scale:1672600526, shift:40), (scale:1111136762, shift:38), (scale:1724433574, shift:40), (scale:1177860753, shift:39), (scale:1378803589, shift:39), (scale:1099796238, shift:39), (scale:1904459178, shift:40), (scale:1341667564, shift:39), (scale:1908963128, shift:40), (scale:1370157152, shift:40), (scale:1967641165, shift:40), (scale:1672983640, shift:40), (scale:1403523213, shift:39), (scale:1398614026, shift:39), (scale:1141772618, shift:39), (scale:1741433345, shift:40), (scale:2062218050, shift:40), (scale:1079392977, shift:39), (scale:1853637808, shift:40), (scale:1294334400, shift:41), (scale:1240625305, shift:39), (scale:1479768979, shift:40), (scale:2042825426, shift:41), (scale:1200007976, shift:39), (scale:1262781510, shift:39), (scale:1168343896, shift:39), (scale:1107946669, shift:39), (scale:2121409490, shift:40), (scale:1098452598, shift:39), (scale:2030751105, shift:39), (scale:1091253711, shift:39), (scale:1187719028, shift:39), (scale:1537441012, shift:40), (scale:1508564717, shift:40), (scale:1595122500, shift:39), (scale:1362465095, shift:39), (scale:1856773251, shift:40), (scale:1838436364, shift:40), (scale:1214773547, shift:39), (scale:1771879157, shift:40), (scale:1932025020, shift:40), (scale:1167175645, shift:39), (scale:1806065907, shift:40), (scale:1174002197, shift:39), (scale:1666572254, shift:40), (scale:1079808804, shift:39), (scale:1082573614, shift:39), (scale:1718112199, shift:40), (scale:2111790046, shift:40), (scale:1586273633, shift:43), (scale:1503774189, shift:39), (scale:1349866404, shift:39), (scale:1907082164, shift:40), (scale:1878796236, shift:40), (scale:1124694750, shift:39), (scale:1386271658, shift:39), (scale:1822204144, shift:40), (scale:1110495812, shift:39), (scale:1519495748, shift:41), (scale:1984481337, shift:40), (scale:1934781510, shift:40), (scale:1226137519, shift:38), (scale:1639565576, shift:40), (scale:1819086288, shift:40), (scale:1774477183, shift:40), (scale:1798256326, shift:40), (scale:2129578926, shift:40), (scale:1488977227, shift:39), (scale:2053977891, shift:40), (scale:1510624663, shift:39), (scale:1190838682, shift:39), (scale:1917320527, shift:40), (scale:1113702451, shift:39), (scale:1486692917, shift:39), (scale:1410417841, shift:40), (scale:1211379039, shift:39), (scale:1410834614, shift:39), (scale:1288911715, shift:38), (scale:1104277590, shift:39), (scale:1158445532, shift:39), (scale:1813938836, shift:40), (scale:1742064745, shift:40), (scale:1249838376, shift:39), (scale:1452005718, shift:39), (scale:1429083948, shift:39), (scale:1299509365, shift:39), (scale:1387648390, shift:39), (scale:1123129486, shift:39), (scale:1629929871, shift:39), (scale:1109679474, shift:39), (scale:2059001294, shift:40), (scale:1561139977, shift:40), (scale:1644044754, shift:39), (scale:1110571073, shift:39), (scale:2020918737, shift:40), (scale:1733493097, shift:40), (scale:1253629991, shift:39), (scale:1378930190, shift:39), (scale:1606052585, shift:40), (scale:1774605959, shift:40), (scale:2132430910, shift:40), (scale:1727249441, shift:39), (scale:1755617817, shift:39), (scale:1598437398, shift:39), (scale:2113185405, shift:39), (scale:1604238752, shift:40), (scale:1690066047, shift:40), (scale:1223685289, shift:39), (scale:1213483611, shift:41), (scale:1891238164, shift:40), (scale:1599564236, shift:42), (scale:1638089851, shift:40)], zero_point: [16], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9
28 Clamp aten_clamp_default_9
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_9
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [16], quantMax: [127], dimension: 0 aten_clamp_default_9
29 MemoryCopy aten_cat_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_9
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
30 Conv2D aten_convolution_default_10
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [16], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_2
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1766628161, shift:39), (scale:1647893864, shift:39), (scale:1379412015, shift:38), (scale:1744999199, shift:39), (scale:1428393136, shift:39), (scale:1894575043, shift:39), (scale:1121708426, shift:38), (scale:1210137657, shift:38), (scale:1870717846, shift:39), (scale:1950768179, shift:39), (scale:1130102881, shift:38), (scale:2112057670, shift:39), (scale:1646445447, shift:39), (scale:1740242155, shift:39), (scale:1925648202, shift:39), (scale:1797896710, shift:38), (scale:1616974996, shift:39), (scale:1794297610, shift:39), (scale:1485196822, shift:38), (scale:1342067685, shift:38), (scale:1102927537, shift:38), (scale:2001186687, shift:39), (scale:1721572834, shift:39), (scale:1686902549, shift:39), (scale:1959869427, shift:39), (scale:1153948005, shift:38), (scale:1298683216, shift:38), (scale:1756786914, shift:39), (scale:1300607628, shift:38), (scale:1754900931, shift:39), (scale:1239434776, shift:38), (scale:1131203474, shift:38)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10
31 Clamp aten_clamp_default_10
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_10
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-24], quantMax: [127], dimension: 0 aten_clamp_default_10
32 Conv2D aten_convolution_default_11
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1761806285, shift:40), (scale:2006271705, shift:41), (scale:1322157919, shift:40), (scale:1437840621, shift:39), (scale:1255700830, shift:40), (scale:1364841271, shift:40), (scale:2117430742, shift:40), (scale:1128266455, shift:39), (scale:1501921762, shift:40), (scale:1279279607, shift:40), (scale:1711923883, shift:40), (scale:1896550524, shift:40), (scale:1207957432, shift:40), (scale:1266745809, shift:39), (scale:1115576514, shift:40), (scale:1488493090, shift:40), (scale:1415140877, shift:40), (scale:1299847036, shift:39), (scale:1444784243, shift:40), (scale:1593149385, shift:39), (scale:1253030552, shift:39), (scale:1400600718, shift:40), (scale:1442942299, shift:40), (scale:1796645067, shift:40), (scale:1158258459, shift:40), (scale:1448478167, shift:40), (scale:1681275795, shift:39), (scale:1100193147, shift:40), (scale:1323608449, shift:40), (scale:1916541262, shift:40), (scale:1181010486, shift:40), (scale:1759924468, shift:39), (scale:1505951385, shift:40), (scale:1357533608, shift:40), (scale:1111673197, shift:40), (scale:1410139100, shift:40), (scale:1271393228, shift:40), (scale:2054139208, shift:41), (scale:1684169639, shift:40), (scale:1310847214, shift:40), (scale:1605683622, shift:40), (scale:1120299980, shift:39), (scale:1811400165, shift:40), (scale:1304730782, shift:40), (scale:1233757685, shift:40), (scale:1387925741, shift:40), (scale:1983789892, shift:40), (scale:1668226655, shift:40), (scale:2067652201, shift:40), (scale:1307892023, shift:40), (scale:1446477878, shift:40), (scale:1638758708, shift:39), (scale:1206960633, shift:39), (scale:1777730961, shift:40), (scale:1093127444, shift:39), (scale:1496813395, shift:40), (scale:2122016108, shift:40), (scale:1301854190, shift:40), (scale:1445168440, shift:40), (scale:1493693523, shift:40), (scale:1391772199, shift:40), (scale:1682161429, shift:40), (scale:1161561144, shift:40), (scale:1707991432, shift:40), (scale:1844595307, shift:40), (scale:1645061826, shift:40), (scale:1746777276, shift:41), (scale:1699081584, shift:41), (scale:1274021522, shift:40), (scale:1919273681, shift:40), (scale:1554257567, shift:39), (scale:1731556917, shift:40), (scale:1126115920, shift:40), (scale:1290027966, shift:40), (scale:2117490946, shift:41), (scale:1129986319, shift:40), (scale:1465953361, shift:40), (scale:1933759445, shift:40), (scale:1553758155, shift:40), (scale:1614787637, shift:40), (scale:1856573097, shift:40), (scale:1666253300, shift:40), (scale:1836782335, shift:40), (scale:1648265843, shift:40), (scale:2010780144, shift:41), (scale:2111302252, shift:40), (scale:1137174894, shift:40), (scale:2025401983, shift:40), (scale:1449875007, shift:40), (scale:1716168269, shift:40), (scale:1862022620, shift:40), (scale:1971034555, shift:40), (scale:1333213900, shift:40), (scale:1652887472, shift:40), (scale:1201946357, shift:39), (scale:1248990719, shift:40), (scale:1127998969, shift:40), (scale:1948032206, shift:39), (scale:1479375872, shift:40), (scale:1784414139, shift:39), (scale:1425116882, shift:40), (scale:1446636486, shift:40), (scale:1372451627, shift:39), (scale:1986458059, shift:40), (scale:1257976790, shift:40), (scale:1663258412, shift:40), (scale:1504264615, shift:40), (scale:1373403626, shift:40), (scale:1899730424, shift:40), (scale:1584147295, shift:40), (scale:1157830165, shift:40), (scale:1664592055, shift:40), (scale:1314646143, shift:40), (scale:1188685094, shift:40), (scale:1456879625, shift:40), (scale:1246969130, shift:39), (scale:1409821620, shift:40), (scale:1486315358, shift:40), (scale:1137958163, shift:40), (scale:1992023413, shift:41), (scale:1288106542, shift:40), (scale:1982021795, shift:41), (scale:1635218287, shift:40), (scale:1274854345, shift:40), (scale:1520449999, shift:40), (scale:1198902532, shift:40), (scale:1250226574, shift:39), (scale:1360068340, shift:40)], zero_point: [30], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11
33 Clamp aten_clamp_default_11
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_11
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [30], quantMax: [127], dimension: 0 aten_clamp_default_11
34 MemoryCopy aten_cat_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_11
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
35 Conv2D aten_convolution_default_12
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-24], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_10
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1301537414, shift:40), (scale:1650690552, shift:40), (scale:1316827131, shift:38), (scale:2146740610, shift:40), (scale:1411690058, shift:39), (scale:1791814660, shift:40), (scale:1570596454, shift:39), (scale:1634601459, shift:40), (scale:1277028556, shift:39), (scale:1519451527, shift:40), (scale:1214645363, shift:40), (scale:1244961008, shift:39), (scale:1418118337, shift:40), (scale:1136566076, shift:39), (scale:1752235953, shift:39), (scale:1495737292, shift:39), (scale:1398822498, shift:39), (scale:1754128156, shift:40), (scale:1145656800, shift:39), (scale:1750917449, shift:40), (scale:1205856100, shift:39), (scale:1198577307, shift:39), (scale:1401486704, shift:40), (scale:1534080056, shift:40), (scale:2086490260, shift:40), (scale:1122865342, shift:39), (scale:1898101394, shift:40), (scale:1727229619, shift:40), (scale:1196666884, shift:39), (scale:2131679035, shift:39), (scale:1276606864, shift:39), (scale:2080268999, shift:40), (scale:1926436027, shift:40), (scale:1843944152, shift:40), (scale:1531594086, shift:40), (scale:2065536258, shift:40), (scale:1341398834, shift:39), (scale:1946331178, shift:40), (scale:1208616772, shift:39), (scale:1602391235, shift:40), (scale:1655831310, shift:40), (scale:1338394265, shift:40), (scale:2131712658, shift:40), (scale:1480570272, shift:40), (scale:1365259619, shift:40), (scale:1947080912, shift:40), (scale:1185457312, shift:40), (scale:1979466643, shift:40), (scale:1797129516, shift:40), (scale:1757862215, shift:39), (scale:1563932640, shift:40), (scale:1495003049, shift:39), (scale:1083531939, shift:39), (scale:1835333036, shift:39), (scale:1303883524, shift:39), (scale:1551133470, shift:40), (scale:1941363111, shift:40), (scale:1314722894, shift:40), (scale:1595551386, shift:39), (scale:1196414626, shift:40), (scale:1493918144, shift:39), (scale:1218928125, shift:39), (scale:1140985001, shift:39), (scale:1935591091, shift:40), (scale:1264689982, shift:39), (scale:1566825955, shift:40), (scale:1366283000, shift:39), (scale:1240281639, shift:40), (scale:1462270704, shift:39), (scale:2118426574, shift:40), (scale:1189873596, shift:39), (scale:1548764916, shift:40), (scale:1907015115, shift:40), (scale:1671165035, shift:40), (scale:1852496825, shift:40), (scale:1147191475, shift:39), (scale:1483955957, shift:39), (scale:1721279100, shift:40), (scale:1459463910, shift:40), (scale:2014084501, shift:40), (scale:1612514846, shift:39), (scale:1579807322, shift:40), (scale:2105712252, shift:40), (scale:1904221435, shift:40), (scale:1217695262, shift:39), (scale:1336946727, shift:39), (scale:1100381769, shift:39), (scale:1137419495, shift:39), (scale:1585164955, shift:39), (scale:1250240921, shift:40), (scale:1954941413, shift:40), (scale:1204110534, shift:39), (scale:1683148810, shift:40), (scale:1951962897, shift:40), (scale:1074659147, shift:39), (scale:1569957622, shift:40), (scale:1341470041, shift:39), (scale:1102348346, shift:39), (scale:1672231985, shift:40), (scale:1365577275, shift:39), (scale:2021819138, shift:40), (scale:1989235367, shift:40), (scale:1816819586, shift:40), (scale:1931792955, shift:40), (scale:1992509094, shift:40), (scale:1313578577, shift:39), (scale:1289379452, shift:39), (scale:1595608597, shift:40), (scale:1133961723, shift:39), (scale:1461748584, shift:39), (scale:1716602547, shift:40), (scale:2010148177, shift:40), (scale:1979153652, shift:40), (scale:1442358566, shift:39), (scale:1153408072, shift:39), (scale:1785658708, shift:40), (scale:1619541645, shift:40), (scale:1727561797, shift:39), (scale:2138923237, shift:40), (scale:1230027587, shift:39), (scale:2088691405, shift:40), (scale:1089793601, shift:39), (scale:2001041522, shift:40), (scale:1634137783, shift:40), (scale:1802916147, shift:40), (scale:1466940039, shift:39), (scale:2008947265, shift:40), (scale:1681122645, shift:40)], zero_point: [30], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12
36 Clamp aten_clamp_default_12
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_12
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [30], quantMax: [127], dimension: 0 aten_clamp_default_12
37 MemoryCopy aten_cat_default_3
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_12
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
38 MaxPool aten_max_pool2d_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_3
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_2
39 Conv2D aten_convolution_default_13
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [30], quantMin: [], quantMax: [], dimension: 0 aten_max_pool2d_default_2
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1305632367, shift:38), (scale:1142288716, shift:38), (scale:1932750426, shift:39), (scale:1181714368, shift:38), (scale:1104570604, shift:37), (scale:2038145868, shift:39), (scale:1328757526, shift:38), (scale:1745508231, shift:39), (scale:1691485105, shift:38), (scale:1849462924, shift:39), (scale:1259675345, shift:38), (scale:2097139750, shift:39), (scale:1503532900, shift:38), (scale:1774498510, shift:38), (scale:1786936113, shift:39), (scale:2097847533, shift:39), (scale:1527429288, shift:39), (scale:1741399837, shift:39), (scale:2016628420, shift:39), (scale:1911073389, shift:39), (scale:1812567938, shift:39), (scale:1745514909, shift:39), (scale:1984024373, shift:38), (scale:2008279870, shift:39), (scale:1641304902, shift:39), (scale:1388672625, shift:38), (scale:1618488205, shift:39), (scale:1522053092, shift:38), (scale:1789605289, shift:39), (scale:1727732275, shift:38), (scale:2130939527, shift:39), (scale:1326908972, shift:38), (scale:1678993934, shift:39), (scale:2051916390, shift:39), (scale:1153276805, shift:38), (scale:1499181715, shift:39), (scale:1608165741, shift:39), (scale:1353244127, shift:38), (scale:1215371670, shift:38), (scale:1476451143, shift:38), (scale:1511783498, shift:39), (scale:1358414960, shift:38), (scale:1280478889, shift:38), (scale:1296125097, shift:38), (scale:1875216076, shift:39), (scale:1888871393, shift:39), (scale:1916417306, shift:39), (scale:2007499320, shift:39)], zero_point: [-14], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13
40 Clamp aten_clamp_default_13
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_13
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-14], quantMax: [127], dimension: 0 aten_clamp_default_13
41 Conv2D aten_convolution_default_14
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-14], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1921627894, shift:40), (scale:1092251466, shift:40), (scale:1887368123, shift:41), (scale:2040046401, shift:40), (scale:1694377664, shift:41), (scale:1989513965, shift:41), (scale:1986313145, shift:41), (scale:1245160978, shift:40), (scale:1085721179, shift:39), (scale:2070649577, shift:41), (scale:1518649262, shift:40), (scale:1655543936, shift:40), (scale:1082623208, shift:40), (scale:2008227705, shift:41), (scale:1952212227, shift:41), (scale:1398710477, shift:40), (scale:1799938300, shift:39), (scale:1341460019, shift:39), (scale:1345741632, shift:40), (scale:1881726228, shift:40), (scale:1924281008, shift:40), (scale:1415893020, shift:40), (scale:2085014607, shift:40), (scale:1332545452, shift:40), (scale:1438642698, shift:40), (scale:2136116762, shift:41), (scale:1109070339, shift:40), (scale:1348479895, shift:40), (scale:1110832067, shift:40), (scale:1247534779, shift:40), (scale:1311861028, shift:40), (scale:1755066603, shift:41), (scale:1217398385, shift:40), (scale:2090120948, shift:41), (scale:1261423956, shift:40), (scale:1654491320, shift:40), (scale:1197109771, shift:40), (scale:1911978591, shift:40), (scale:1204547735, shift:40), (scale:1777049670, shift:40), (scale:1310375091, shift:39), (scale:1407536812, shift:39), (scale:1270432831, shift:40), (scale:2091102063, shift:41), (scale:2040932063, shift:41), (scale:1169211484, shift:39), (scale:1267927475, shift:40), (scale:1093680079, shift:40), (scale:1225200106, shift:40), (scale:1416870788, shift:39), (scale:1412007569, shift:40), (scale:2093360335, shift:41), (scale:1480387543, shift:40), (scale:1823831644, shift:41), (scale:1385682835, shift:40), (scale:2063996276, shift:41), (scale:2116696640, shift:41), (scale:1838225908, shift:41), (scale:1796298260, shift:40), (scale:1318647117, shift:39), (scale:1966603497, shift:40), (scale:1163668124, shift:40), (scale:1932166558, shift:41), (scale:1839114388, shift:41), (scale:1452538303, shift:40), (scale:1241433410, shift:40), (scale:2061347918, shift:41), (scale:1591196994, shift:40), (scale:1733626486, shift:41), (scale:1658125900, shift:40), (scale:1125326538, shift:40), (scale:1191897763, shift:40), (scale:1178179413, shift:40), (scale:1595118283, shift:40), (scale:1178736101, shift:39), (scale:1137441044, shift:39), (scale:1855880164, shift:41), (scale:1079277978, shift:40), (scale:1684736110, shift:40), (scale:1180199408, shift:40), (scale:1226626254, shift:39), (scale:1986351009, shift:41), (scale:1250773814, shift:40), (scale:1123721549, shift:40), (scale:1147548587, shift:40), (scale:1092766855, shift:40), (scale:2140553881, shift:41), (scale:1716890649, shift:41), (scale:1600813892, shift:41), (scale:2141195630, shift:40), (scale:1256565580, shift:40), (scale:1418102509, shift:39), (scale:1803887767, shift:41), (scale:1298109570, shift:40), (scale:1157670484, shift:40), (scale:1988877675, shift:41), (scale:1223635359, shift:40), (scale:1228319298, shift:40), (scale:1820112354, shift:41), (scale:2123101976, shift:39), (scale:1113511333, shift:40), (scale:2138296841, shift:41), (scale:1862581015, shift:40), (scale:2007342571, shift:41), (scale:1119630489, shift:40), (scale:2115245308, shift:41), (scale:1220228842, shift:40), (scale:1393056959, shift:40), (scale:1240038434, shift:39), (scale:1078361408, shift:40), (scale:1240359925, shift:40), (scale:1661185302, shift:41), (scale:1168721543, shift:40), (scale:1725504243, shift:40), (scale:1642180274, shift:40), (scale:1613346486, shift:40), (scale:1961345524, shift:41), (scale:1129656845, shift:40), (scale:1212259287, shift:40), (scale:1207971334, shift:40), (scale:1658498728, shift:40), (scale:1178539385, shift:40), (scale:1146264650, shift:40), (scale:1497613145, shift:40), (scale:1461670808, shift:40), (scale:1206280756, shift:39), (scale:1155037888, shift:40), (scale:1611302364, shift:40), (scale:2028808576, shift:41), (scale:1352973546, shift:40), (scale:1331210354, shift:40), (scale:1533581898, shift:40), (scale:1181276327, shift:40), (scale:2087261432, shift:40), (scale:1211826934, shift:39), (scale:1434832799, shift:40), (scale:2066321999, shift:41), (scale:1599303387, shift:41), (scale:1164757987, shift:40), (scale:2130886439, shift:41), (scale:1290802456, shift:39), (scale:1153810834, shift:39), (scale:2098520918, shift:40), (scale:2063865602, shift:41), (scale:1505289123, shift:40), (scale:2127564983, shift:40), (scale:1351361513, shift:40), (scale:1505976308, shift:40), (scale:1507451767, shift:40), (scale:2050538571, shift:41), (scale:1156419743, shift:40), (scale:1288665965, shift:40), (scale:1237232808, shift:40), (scale:2044432095, shift:41), (scale:1858099692, shift:41), (scale:1424527570, shift:40), (scale:1154738587, shift:40), (scale:1184023308, shift:40), (scale:1656394728, shift:40), (scale:1095834974, shift:40), (scale:1913186537, shift:41), (scale:1101894253, shift:39), (scale:1916338221, shift:41), (scale:1976216873, shift:41), (scale:1698445478, shift:41), (scale:1813575110, shift:41), (scale:1117082073, shift:40), (scale:1414789948, shift:40), (scale:1522922246, shift:40), (scale:1750450203, shift:40), (scale:1185256526, shift:40), (scale:1196738705, shift:39), (scale:1124801639, shift:39), (scale:1307997855, shift:40), (scale:1284254118, shift:40), (scale:1149306528, shift:40), (scale:1795198446, shift:40), (scale:1444729802, shift:39), (scale:1301149423, shift:40), (scale:1341384027, shift:40), (scale:1238210137, shift:40), (scale:1125800805, shift:40), (scale:1090649824, shift:39), (scale:1436153192, shift:39), (scale:1343043170, shift:40), (scale:2122204514, shift:41), (scale:1531637808, shift:40), (scale:1967401280, shift:40), (scale:1435017451, shift:40), (scale:1809693269, shift:41), (scale:2010333466, shift:40), (scale:1164029592, shift:40)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14
42 Clamp aten_clamp_default_14
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [6], quantMax: [127], dimension: 0 aten_clamp_default_14
43 MemoryCopy aten_cat_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_14
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
44 Conv2D aten_convolution_default_15
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-14], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_13
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1306516761, shift:40), (scale:1439617033, shift:40), (scale:1526001548, shift:40), (scale:1387631592, shift:40), (scale:1338488585, shift:40), (scale:1313375495, shift:40), (scale:1520638613, shift:40), (scale:1192271471, shift:40), (scale:1229926047, shift:40), (scale:1902496770, shift:40), (scale:1604929256, shift:40), (scale:1877493750, shift:40), (scale:1919655802, shift:40), (scale:1287422356, shift:40), (scale:1440535892, shift:39), (scale:2079658716, shift:41), (scale:1770441982, shift:40), (scale:1609699049, shift:40), (scale:1452953750, shift:40), (scale:1432704848, shift:40), (scale:1903161061, shift:40), (scale:1321236742, shift:40), (scale:1321898127, shift:40), (scale:2102735961, shift:40), (scale:1350559591, shift:40), (scale:1857845036, shift:41), (scale:1955552526, shift:40), (scale:1582473859, shift:40), (scale:1946021393, shift:40), (scale:1542495409, shift:40), (scale:1418802375, shift:40), (scale:1542483785, shift:40), (scale:1904089695, shift:40), (scale:1401727084, shift:40), (scale:1530651409, shift:40), (scale:1459281597, shift:40), (scale:1266474294, shift:40), (scale:1742487692, shift:40), (scale:2101786546, shift:41), (scale:1101691549, shift:40), (scale:1605158553, shift:40), (scale:1391735333, shift:40), (scale:2088318803, shift:41), (scale:1395904323, shift:40), (scale:1367755959, shift:40), (scale:1108762849, shift:40), (scale:1404439106, shift:40), (scale:1593811011, shift:40), (scale:1575479959, shift:40), (scale:1519924307, shift:40), (scale:1322323524, shift:40), (scale:1146770000, shift:40), (scale:1795769398, shift:40), (scale:1232696627, shift:40), (scale:1546280564, shift:40), (scale:1469663434, shift:40), (scale:1688922799, shift:40), (scale:1395574907, shift:40), (scale:1689906908, shift:40), (scale:1296908844, shift:40), (scale:1124747573, shift:39), (scale:1439386944, shift:40), (scale:1456995323, shift:40), (scale:1213833544, shift:40), (scale:1270236907, shift:39), (scale:1428890458, shift:40), (scale:1372025332, shift:40), (scale:1211474272, shift:39), (scale:1378681010, shift:40), (scale:1699169999, shift:40), (scale:1507666975, shift:40), (scale:1577997467, shift:40), (scale:1933082512, shift:41), (scale:1557795764, shift:39), (scale:1274093916, shift:40), (scale:1400834553, shift:40), (scale:1842131876, shift:41), (scale:1501414150, shift:40), (scale:1487121855, shift:40), (scale:1086474670, shift:40), (scale:1276957131, shift:40), (scale:1489357937, shift:40), (scale:1404696053, shift:40), (scale:2121959015, shift:41), (scale:1519965341, shift:40), (scale:1132878093, shift:40), (scale:1376915496, shift:40), (scale:1366399727, shift:40), (scale:1628577982, shift:40), (scale:1521058990, shift:40), (scale:1524739888, shift:40), (scale:1128594367, shift:39), (scale:2115133301, shift:40), (scale:1317691626, shift:40), (scale:1207826131, shift:40), (scale:1372215620, shift:40), (scale:1181159477, shift:40), (scale:1406182166, shift:40), (scale:1280265642, shift:40), (scale:1322798495, shift:40), (scale:1550126654, shift:40), (scale:1937680596, shift:40), (scale:1189507055, shift:40), (scale:1424252837, shift:39), (scale:1522559281, shift:40), (scale:1141673698, shift:40), (scale:1549941209, shift:40), (scale:2081529368, shift:41), (scale:1153319924, shift:40), (scale:1736784158, shift:41), (scale:1266520699, shift:40), (scale:1309083404, shift:40), (scale:2110872646, shift:40), (scale:1405113172, shift:40), (scale:1359198368, shift:40), (scale:1085231061, shift:40), (scale:1797526635, shift:40), (scale:1921933271, shift:40), (scale:2017422994, shift:41), (scale:1333094478, shift:40), (scale:1696063223, shift:40), (scale:1202752018, shift:40), (scale:1745967471, shift:40), (scale:1660017686, shift:40), (scale:1116810158, shift:40), (scale:1398028751, shift:40), (scale:1725974812, shift:40), (scale:2057751377, shift:40), (scale:1981545818, shift:40), (scale:1162592525, shift:40), (scale:1584555844, shift:40), (scale:1470604572, shift:40), (scale:1567165050, shift:40), (scale:1925302276, shift:40), (scale:1355378783, shift:40), (scale:1520750620, shift:40), (scale:1278091198, shift:39), (scale:1461903187, shift:40), (scale:1265982680, shift:40), (scale:1800280836, shift:40), (scale:1578045193, shift:40), (scale:1375925576, shift:40), (scale:2106363497, shift:41), (scale:1531397944, shift:40), (scale:1855354296, shift:40), (scale:1116029369, shift:40), (scale:2086573366, shift:40), (scale:1177106016, shift:40), (scale:1540305995, shift:40), (scale:1155488908, shift:39), (scale:1356770765, shift:40), (scale:1676859189, shift:40), (scale:1770995499, shift:41), (scale:1679745474, shift:40), (scale:1785696372, shift:40), (scale:1468451261, shift:40), (scale:1464471591, shift:40), (scale:1314244163, shift:40), (scale:1474131372, shift:40), (scale:1129003297, shift:39), (scale:1138614296, shift:40), (scale:1329127752, shift:40), (scale:2004198812, shift:40), (scale:1191125428, shift:40), (scale:1134028187, shift:40), (scale:1725624351, shift:40), (scale:1974653535, shift:41), (scale:2132930384, shift:40), (scale:1425164476, shift:40), (scale:1987647363, shift:40), (scale:1901662356, shift:40), (scale:1338158288, shift:40), (scale:1117320351, shift:40), (scale:1431405325, shift:40), (scale:1811145305, shift:40), (scale:1377597663, shift:40), (scale:1223037462, shift:40), (scale:1349634039, shift:40), (scale:2111764472, shift:41), (scale:1310720797, shift:39), (scale:1166883912, shift:40), (scale:1771419046, shift:41), (scale:2127555473, shift:41), (scale:1479034834, shift:40), (scale:1657066240, shift:41), (scale:1283676561, shift:40), (scale:1560462262, shift:40), (scale:1808262015, shift:40), (scale:1629174118, shift:40), (scale:1224002903, shift:40), (scale:1357829809, shift:40), (scale:2147016453, shift:41)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15
45 Clamp aten_clamp_default_15
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_15
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [6], quantMax: [127], dimension: 0 aten_clamp_default_15
46 MemoryCopy aten_cat_default_4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_15
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
47 Conv2D aten_convolution_default_16
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [6], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_4
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1696446067, shift:39), (scale:1931478790, shift:39), (scale:1079648164, shift:38), (scale:1627539702, shift:38), (scale:1148014779, shift:38), (scale:1789233477, shift:39), (scale:1263915912, shift:38), (scale:1703287153, shift:38), (scale:1689776476, shift:39), (scale:1657076248, shift:39), (scale:1149228060, shift:38), (scale:1422097098, shift:39), (scale:1741417663, shift:38), (scale:1456840530, shift:38), (scale:1399690597, shift:39), (scale:1414813771, shift:38), (scale:1710233259, shift:39), (scale:1390890688, shift:39), (scale:1371055684, shift:38), (scale:1218630315, shift:38), (scale:1676081405, shift:39), (scale:1331332102, shift:38), (scale:1902964833, shift:39), (scale:1175342621, shift:38), (scale:2112978886, shift:39), (scale:1695167907, shift:39), (scale:1663537771, shift:39), (scale:1964483305, shift:39), (scale:1856316695, shift:39), (scale:1368636862, shift:38), (scale:1772816207, shift:39), (scale:2128661312, shift:39), (scale:1443146066, shift:39), (scale:1258529564, shift:38), (scale:1091535190, shift:38), (scale:1446821497, shift:39), (scale:1451395829, shift:38), (scale:1212859321, shift:38), (scale:1987243114, shift:39), (scale:1956662429, shift:39), (scale:1116458097, shift:38), (scale:1669310738, shift:39), (scale:1573631458, shift:38), (scale:1700029226, shift:39), (scale:1866978147, shift:39), (scale:1826174913, shift:39), (scale:1184067483, shift:38), (scale:2143371535, shift:39)], zero_point: [-20], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16
48 Clamp aten_clamp_default_16
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_16
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-20], quantMax: [127], dimension: 0 aten_clamp_default_16
49 Conv2D aten_convolution_default_17
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-20], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1528346472, shift:40), (scale:1331199835, shift:40), (scale:1335944485, shift:40), (scale:1221898667, shift:40), (scale:1793219322, shift:40), (scale:1339008961, shift:40), (scale:1402941943, shift:40), (scale:1102633441, shift:40), (scale:1251216191, shift:40), (scale:1176695033, shift:40), (scale:1184078869, shift:40), (scale:1117535185, shift:40), (scale:1187569774, shift:39), (scale:1581972810, shift:40), (scale:1097016469, shift:40), (scale:1244693759, shift:40), (scale:1116440295, shift:39), (scale:1429105737, shift:41), (scale:1223891137, shift:40), (scale:1128013122, shift:40), (scale:1760131069, shift:41), (scale:1485348759, shift:40), (scale:2048399582, shift:41), (scale:1707187055, shift:40), (scale:2129245768, shift:41), (scale:1266223142, shift:40), (scale:1708636970, shift:40), (scale:1449987929, shift:40), (scale:1461670900, shift:40), (scale:1449367703, shift:40), (scale:1369956809, shift:40), (scale:1170206783, shift:40), (scale:1265121796, shift:40), (scale:2046544223, shift:40), (scale:1407478078, shift:40), (scale:1583948568, shift:40), (scale:1354073073, shift:40), (scale:2062044733, shift:41), (scale:1863337468, shift:41), (scale:1398908958, shift:40), (scale:1507231363, shift:40), (scale:1561561010, shift:40), (scale:1213970757, shift:40), (scale:2090878952, shift:41), (scale:1468792858, shift:40), (scale:1558869030, shift:40), (scale:2101079630, shift:41), (scale:2105267197, shift:41), (scale:1171663629, shift:40), (scale:1972294721, shift:41), (scale:1172963520, shift:40), (scale:1196869358, shift:40), (scale:2131708820, shift:41), (scale:1408564802, shift:40), (scale:1469391815, shift:40), (scale:1477402048, shift:40), (scale:1866541903, shift:41), (scale:1084737485, shift:40), (scale:1860109295, shift:40), (scale:1358341918, shift:40), (scale:2015438375, shift:41), (scale:1346687432, shift:40), (scale:1276989258, shift:40), (scale:1175166972, shift:40), (scale:1994003374, shift:41), (scale:2063987258, shift:40), (scale:1261104573, shift:40), (scale:2087121898, shift:41), (scale:1441367060, shift:40), (scale:1279627590, shift:40), (scale:1200572006, shift:40), (scale:1210707641, shift:40), (scale:1323274489, shift:40), (scale:2071605435, shift:41), (scale:1681141002, shift:40), (scale:1868861577, shift:40), (scale:1956876249, shift:41), (scale:1985621594, shift:40), (scale:1120989343, shift:40), (scale:1817257903, shift:40), (scale:1221133735, shift:40), (scale:1684968891, shift:40), (scale:1478425344, shift:40), (scale:1307433671, shift:40), (scale:1763421721, shift:41), (scale:1164682674, shift:40), (scale:1279887284, shift:40), (scale:1152116745, shift:40), (scale:1107326246, shift:40), (scale:1460908151, shift:40), (scale:1269864641, shift:40), (scale:1792649041, shift:40), (scale:1964542852, shift:41), (scale:1147044037, shift:39), (scale:1387428140, shift:39), (scale:1134836172, shift:40), (scale:2014806375, shift:41), (scale:1984451597, shift:41), (scale:1283590121, shift:40), (scale:1391081602, shift:40), (scale:1389001682, shift:39), (scale:1306601703, shift:40), (scale:1260462793, shift:40), (scale:1993127918, shift:40), (scale:2123579991, shift:41), (scale:1455191005, shift:40), (scale:1192436531, shift:40), (scale:1999321057, shift:41), (scale:2085526896, shift:41), (scale:1295658217, shift:40), (scale:1547825465, shift:40), (scale:1107999455, shift:40), (scale:2116842588, shift:41), (scale:1204031766, shift:40), (scale:1165877358, shift:39), (scale:1543294742, shift:40), (scale:1456622499, shift:40), (scale:1243066094, shift:40), (scale:1283585089, shift:40), (scale:1218454099, shift:40), (scale:1202975617, shift:40), (scale:1981776993, shift:40), (scale:2110087333, shift:40), (scale:1078406380, shift:40), (scale:2079008262, shift:41), (scale:2127910555, shift:41), (scale:1176243632, shift:40), (scale:1674402838, shift:40), (scale:1620459930, shift:40), (scale:1491218587, shift:40), (scale:1355375242, shift:40), (scale:2028858680, shift:41), (scale:1273298764, shift:40), (scale:1259962397, shift:40), (scale:1234690771, shift:40), (scale:1355114694, shift:40), (scale:1141851595, shift:39), (scale:1238984304, shift:40), (scale:1089600634, shift:40), (scale:1165756864, shift:39), (scale:1952972398, shift:40), (scale:1158064529, shift:40), (scale:1193491921, shift:40), (scale:1122222008, shift:40), (scale:1229702000, shift:40), (scale:2051487986, shift:41), (scale:1233315868, shift:40), (scale:1076243947, shift:40), (scale:1215014563, shift:40), (scale:2015790266, shift:41), (scale:1093564494, shift:40), (scale:1968235909, shift:41), (scale:1190958225, shift:40), (scale:2117639993, shift:41), (scale:1729893461, shift:41), (scale:1156837846, shift:39), (scale:1214895778, shift:40), (scale:1265817223, shift:40), (scale:1284115395, shift:40), (scale:1452655506, shift:40), (scale:1453917226, shift:40), (scale:2101430003, shift:41), (scale:1156105008, shift:40), (scale:1407564295, shift:39), (scale:1387344297, shift:40), (scale:1201727855, shift:40), (scale:1727179548, shift:41), (scale:1155170397, shift:40), (scale:1664622797, shift:40), (scale:1202763495, shift:40), (scale:1727342485, shift:40), (scale:1096887619, shift:40), (scale:1487638143, shift:40), (scale:1132490577, shift:40), (scale:1486898088, shift:39), (scale:1193945221, shift:40), (scale:2071719757, shift:40), (scale:1558021775, shift:40), (scale:1215586173, shift:40), (scale:1375846957, shift:40), (scale:1779762365, shift:40), (scale:1551232717, shift:40), (scale:1293220803, shift:40), (scale:1988231060, shift:40), (scale:1291411021, shift:39), (scale:1973726975, shift:41), (scale:1860103978, shift:41), (scale:1740413557, shift:40), (scale:1151601631, shift:40), (scale:2027470104, shift:41), (scale:1413533348, shift:40), (scale:1608626650, shift:40)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17
50 Clamp aten_clamp_default_17
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_17
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [19], quantMax: [127], dimension: 0 aten_clamp_default_17
51 MemoryCopy aten_cat_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_17
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
52 Conv2D aten_convolution_default_18
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-20], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_16
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1966835749, shift:40), (scale:1438857482, shift:40), (scale:1435730813, shift:40), (scale:1192827543, shift:39), (scale:1825419964, shift:40), (scale:1651567867, shift:40), (scale:1320459451, shift:40), (scale:1490910848, shift:40), (scale:1268172314, shift:40), (scale:1354537197, shift:40), (scale:1371927155, shift:40), (scale:1277632842, shift:40), (scale:1332183252, shift:40), (scale:1258648358, shift:40), (scale:1164575188, shift:40), (scale:2009104327, shift:40), (scale:1845550327, shift:39), (scale:2107215609, shift:40), (scale:1512878150, shift:40), (scale:1078163019, shift:39), (scale:1222900599, shift:40), (scale:1461260233, shift:40), (scale:1455018383, shift:40), (scale:1450255883, shift:40), (scale:1960960128, shift:40), (scale:1804920049, shift:40), (scale:1833581266, shift:40), (scale:1918773019, shift:40), (scale:1775555428, shift:40), (scale:1132647817, shift:39), (scale:1443602606, shift:40), (scale:1817419131, shift:40), (scale:1539029315, shift:40), (scale:2069270759, shift:40), (scale:1835050551, shift:40), (scale:1498546877, shift:40), (scale:1567738198, shift:40), (scale:1608850167, shift:40), (scale:1776004930, shift:41), (scale:1101457272, shift:40), (scale:2118137921, shift:40), (scale:1403686746, shift:40), (scale:1651486398, shift:40), (scale:1143972630, shift:40), (scale:1183948975, shift:40), (scale:1571172891, shift:40), (scale:1744370391, shift:40), (scale:1877488523, shift:40), (scale:1424817521, shift:40), (scale:1470259485, shift:40), (scale:2021952452, shift:41), (scale:1345810457, shift:40), (scale:1569093351, shift:40), (scale:1186808260, shift:40), (scale:2076215728, shift:40), (scale:1463107521, shift:40), (scale:1734864001, shift:40), (scale:1577744604, shift:40), (scale:1215273781, shift:39), (scale:1382109601, shift:40), (scale:1264504798, shift:39), (scale:1591135759, shift:40), (scale:1330252026, shift:40), (scale:1403278643, shift:40), (scale:1103713993, shift:39), (scale:1725889342, shift:40), (scale:1668333136, shift:40), (scale:1310013798, shift:40), (scale:1352716211, shift:40), (scale:1165706065, shift:39), (scale:1602545933, shift:40), (scale:1368386115, shift:40), (scale:1347344879, shift:40), (scale:1870260028, shift:40), (scale:1395425934, shift:39), (scale:1529107606, shift:40), (scale:1339079130, shift:40), (scale:2036549021, shift:40), (scale:1551425944, shift:40), (scale:1595132093, shift:40), (scale:1285233928, shift:40), (scale:1197079012, shift:40), (scale:1539124267, shift:40), (scale:1311024276, shift:40), (scale:1148827803, shift:40), (scale:1423942350, shift:40), (scale:1335986928, shift:40), (scale:1785242986, shift:40), (scale:1289604656, shift:40), (scale:1753375055, shift:40), (scale:1491661063, shift:40), (scale:1890159278, shift:40), (scale:1282336947, shift:40), (scale:1651783977, shift:40), (scale:1274147919, shift:40), (scale:1300674239, shift:40), (scale:1756671594, shift:40), (scale:1433523467, shift:40), (scale:1589858941, shift:40), (scale:1376799704, shift:40), (scale:1528025344, shift:40), (scale:1480463296, shift:40), (scale:1781935433, shift:41), (scale:1350901396, shift:39), (scale:1601629458, shift:40), (scale:1219936392, shift:38), (scale:1372445592, shift:40), (scale:1272022327, shift:39), (scale:1170501133, shift:40), (scale:1188535814, shift:40), (scale:1217010261, shift:40), (scale:1250867528, shift:40), (scale:1486750438, shift:40), (scale:1989707942, shift:40), (scale:1673983341, shift:40), (scale:1787310468, shift:40), (scale:1526223443, shift:40), (scale:1482955497, shift:40), (scale:1289261216, shift:40), (scale:1531684694, shift:40), (scale:1303745076, shift:40), (scale:1288880649, shift:40), (scale:1317617258, shift:40), (scale:2039391120, shift:40), (scale:1931238489, shift:42), (scale:1822270221, shift:40), (scale:1159319888, shift:40), (scale:1413963385, shift:40), (scale:1404846298, shift:40), (scale:1904993606, shift:41), (scale:1976272444, shift:40), (scale:1788605611, shift:40), (scale:1128174350, shift:40), (scale:1488191997, shift:40), (scale:1994753683, shift:40), (scale:1102491393, shift:40), (scale:1771535357, shift:40), (scale:1630240161, shift:40), (scale:1966176024, shift:40), (scale:1491625171, shift:40), (scale:1625077249, shift:40), (scale:1517350192, shift:40), (scale:1257057345, shift:39), (scale:1940592766, shift:40), (scale:1088598322, shift:39), (scale:2089790424, shift:41), (scale:2079447889, shift:40), (scale:1765168455, shift:40), (scale:1642976243, shift:40), (scale:1842624291, shift:41), (scale:1978370690, shift:40), (scale:1160991325, shift:39), (scale:1574861961, shift:40), (scale:1296835620, shift:39), (scale:1593307118, shift:40), (scale:1533052380, shift:40), (scale:1838368739, shift:40), (scale:1712909233, shift:40), (scale:1479482823, shift:40), (scale:1681806044, shift:40), (scale:1915811281, shift:40), (scale:1214324453, shift:39), (scale:1534662764, shift:40), (scale:1106472914, shift:39), (scale:1329001320, shift:40), (scale:1790645937, shift:40), (scale:1370262743, shift:40), (scale:1642452299, shift:40), (scale:1944467561, shift:40), (scale:1623167197, shift:40), (scale:1432224620, shift:40), (scale:1717889268, shift:40), (scale:2135403776, shift:41), (scale:1751397018, shift:39), (scale:1593843026, shift:40), (scale:1454450381, shift:40), (scale:1345413368, shift:40), (scale:1257154196, shift:40), (scale:1189258872, shift:40), (scale:1254004548, shift:40), (scale:1085067538, shift:40), (scale:1746019515, shift:40), (scale:1162413894, shift:40), (scale:1264399687, shift:40), (scale:1380515170, shift:40), (scale:2089522280, shift:41), (scale:1722076265, shift:39), (scale:1478219584, shift:40), (scale:1502897381, shift:40), (scale:1321720032, shift:40), (scale:1768982481, shift:40), (scale:1773656771, shift:40)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18
53 Clamp aten_clamp_default_18
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_18
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [19], quantMax: [127], dimension: 0 aten_clamp_default_18
54 MemoryCopy aten_cat_default_5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_18
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
55 Conv2D aten_convolution_default_19
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [19], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_5
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1517369858, shift:39), (scale:1841159292, shift:40), (scale:1648432341, shift:39), (scale:1758468491, shift:39), (scale:1901944190, shift:39), (scale:2035862817, shift:40), (scale:1473293916, shift:40), (scale:1551690737, shift:39), (scale:1503951468, shift:40), (scale:1181607785, shift:39), (scale:1395845625, shift:39), (scale:1705333403, shift:40), (scale:1695275855, shift:40), (scale:1206719639, shift:39), (scale:1651795294, shift:40), (scale:1982180107, shift:40), (scale:1632659513, shift:40), (scale:1442673306, shift:40), (scale:1632280594, shift:40), (scale:1406729618, shift:40), (scale:1562054200, shift:40), (scale:1194559651, shift:39), (scale:1292710240, shift:39), (scale:1464758994, shift:40), (scale:1599354012, shift:39), (scale:2034103850, shift:40), (scale:1605660891, shift:40), (scale:1105820917, shift:39), (scale:1349429407, shift:39), (scale:1095159103, shift:39), (scale:1395092712, shift:39), (scale:2092915165, shift:40), (scale:1119445391, shift:39), (scale:1417400931, shift:39), (scale:1402235096, shift:39), (scale:1139041628, shift:39), (scale:1754455007, shift:39), (scale:2002045053, shift:40), (scale:1969123924, shift:40), (scale:1832952011, shift:40), (scale:1692065946, shift:40), (scale:1788939662, shift:40), (scale:1166183444, shift:39), (scale:1079091267, shift:39), (scale:1738573464, shift:40), (scale:1446155266, shift:40), (scale:2100699548, shift:40), (scale:1074143766, shift:39), (scale:1278608687, shift:40), (scale:1846866304, shift:40), (scale:1813878152, shift:39), (scale:1079531229, shift:39), (scale:1467145624, shift:40), (scale:1352605891, shift:39), (scale:1142438005, shift:39), (scale:1954942769, shift:40), (scale:1806829442, shift:39), (scale:1878539321, shift:40), (scale:1559003319, shift:40), (scale:1126910578, shift:39), (scale:1670102695, shift:40), (scale:1955963424, shift:40), (scale:1475389847, shift:39), (scale:2072210756, shift:40)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19
56 Clamp aten_clamp_default_19
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_19
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-26], quantMax: [127], dimension: 0 aten_clamp_default_19
57 Conv2D aten_convolution_default_20
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1434098405, shift:40), (scale:1587401890, shift:40), (scale:1304032983, shift:40), (scale:1833767394, shift:40), (scale:1683140786, shift:40), (scale:1584894680, shift:40), (scale:1733650704, shift:40), (scale:1477051660, shift:40), (scale:1405579402, shift:40), (scale:1262078903, shift:40), (scale:1847898994, shift:40), (scale:1590746853, shift:40), (scale:1964775391, shift:40), (scale:1140068633, shift:40), (scale:1320366119, shift:40), (scale:1633154153, shift:40), (scale:1335814245, shift:40), (scale:1160502947, shift:40), (scale:1391374990, shift:40), (scale:1558436535, shift:40), (scale:2078185105, shift:40), (scale:1624107863, shift:40), (scale:1480501993, shift:40), (scale:1851903387, shift:40), (scale:1666670268, shift:40), (scale:1586491326, shift:40), (scale:1139486342, shift:40), (scale:1508593466, shift:40), (scale:1347098066, shift:40), (scale:1157867560, shift:39), (scale:1441316011, shift:40), (scale:1228149683, shift:40), (scale:1475449380, shift:40), (scale:1989402379, shift:40), (scale:1765746844, shift:40), (scale:1264690936, shift:40), (scale:1708625172, shift:40), (scale:1254728312, shift:40), (scale:1898744810, shift:39), (scale:1739918455, shift:40), (scale:1772367253, shift:40), (scale:1829260048, shift:40), (scale:1165767127, shift:40), (scale:1164360205, shift:39), (scale:1483531116, shift:40), (scale:1883980981, shift:40), (scale:1448923094, shift:40), (scale:1434318765, shift:39), (scale:1171541680, shift:39), (scale:1686635768, shift:40), (scale:1206478730, shift:39), (scale:1506815888, shift:40), (scale:1891231421, shift:40), (scale:1850844863, shift:40), (scale:1569808005, shift:39), (scale:1934095927, shift:40), (scale:1477749969, shift:40), (scale:1757203919, shift:40), (scale:1351171561, shift:39), (scale:1245003495, shift:40), (scale:1261161470, shift:40), (scale:1343797203, shift:40), (scale:1636750386, shift:40), (scale:1513132959, shift:40), (scale:2105679106, shift:41), (scale:1584714985, shift:40), (scale:1135857412, shift:39), (scale:1334407322, shift:40), (scale:1726325390, shift:40), (scale:1692056563, shift:40), (scale:2039719551, shift:40), (scale:1144553242, shift:39), (scale:1333767949, shift:40), (scale:1444122851, shift:40), (scale:1577309578, shift:40), (scale:2069521285, shift:40), (scale:1643154832, shift:40), (scale:1255128504, shift:40), (scale:1391562928, shift:40), (scale:1428379080, shift:40), (scale:1345745134, shift:40), (scale:1961284255, shift:40), (scale:1424414391, shift:40), (scale:2002475318, shift:40), (scale:1437154179, shift:40), (scale:1186893638, shift:40), (scale:1407750577, shift:40), (scale:1789105390, shift:40), (scale:1617524135, shift:40), (scale:1391412083, shift:40), (scale:1657050410, shift:40), (scale:1451633629, shift:40), (scale:1175092714, shift:39), (scale:1390262200, shift:40), (scale:1823334514, shift:40), (scale:1177114831, shift:39), (scale:1262926409, shift:40), (scale:1401614300, shift:40), (scale:1348354693, shift:40), (scale:1282766480, shift:39), (scale:1238995257, shift:40), (scale:1846069486, shift:40), (scale:1556911189, shift:40), (scale:1411147332, shift:40), (scale:1415906361, shift:40), (scale:1470806576, shift:40), (scale:1535410314, shift:40), (scale:1663114288, shift:40), (scale:1956057442, shift:40), (scale:1478483311, shift:40), (scale:1567286781, shift:40), (scale:1270677742, shift:40), (scale:1736803194, shift:40), (scale:1251938782, shift:40), (scale:1354725068, shift:39), (scale:1584471957, shift:40), (scale:2074414261, shift:41), (scale:1404428832, shift:40), (scale:1663879227, shift:40), (scale:1883412084, shift:40), (scale:1482732655, shift:40), (scale:1600379074, shift:40), (scale:1739699606, shift:40), (scale:1437900298, shift:40), (scale:1573459740, shift:40), (scale:1091198499, shift:40), (scale:1410625558, shift:40), (scale:1233136765, shift:40), (scale:1991170753, shift:41), (scale:1312302381, shift:40), (scale:1556113415, shift:40), (scale:1354212361, shift:40), (scale:2031017814, shift:40), (scale:1720053243, shift:40), (scale:1527664064, shift:40), (scale:1513037891, shift:40), (scale:2105083832, shift:40), (scale:1438067216, shift:39), (scale:1715412774, shift:40), (scale:1370933875, shift:40), (scale:1752010216, shift:40), (scale:1595171495, shift:40), (scale:1492254834, shift:40), (scale:1277862789, shift:39), (scale:1657311985, shift:40), (scale:2053327590, shift:40), (scale:1327095472, shift:40), (scale:1351350844, shift:40), (scale:1264152538, shift:40), (scale:1624262555, shift:40), (scale:1425531576, shift:39), (scale:1077557488, shift:39), (scale:1233666645, shift:40), (scale:1179400994, shift:39), (scale:1382588488, shift:40), (scale:1860607460, shift:40), (scale:1362869999, shift:40), (scale:1769837924, shift:40), (scale:1103814576, shift:39), (scale:1925045791, shift:41), (scale:1331690605, shift:40), (scale:1224613211, shift:40), (scale:1630147836, shift:39), (scale:1195252197, shift:40), (scale:1275986709, shift:40), (scale:2072264104, shift:41), (scale:1322388785, shift:40), (scale:1078246181, shift:39), (scale:1262945368, shift:39), (scale:1196222934, shift:39), (scale:1367022489, shift:40), (scale:1937696968, shift:40), (scale:1530366768, shift:40), (scale:1838435752, shift:40), (scale:1850882643, shift:40), (scale:1508093535, shift:40), (scale:1590279481, shift:40), (scale:1678997088, shift:40), (scale:1605763875, shift:40), (scale:1465479338, shift:40), (scale:1455806039, shift:40), (scale:1460345670, shift:40), (scale:1645557907, shift:40), (scale:1315506115, shift:40), (scale:1326422441, shift:40), (scale:1897457409, shift:40), (scale:1659466674, shift:40), (scale:1268796166, shift:40), (scale:1371997069, shift:40), (scale:2113944383, shift:40), (scale:1164231616, shift:40), (scale:1385339688, shift:40), (scale:1308863450, shift:40), (scale:1139759524, shift:40), (scale:1213980715, shift:40), (scale:1709135818, shift:40), (scale:1707526945, shift:40), (scale:1242439271, shift:40), (scale:1633432076, shift:40), (scale:1388892783, shift:40), (scale:1833478756, shift:40), (scale:1225991284, shift:40), (scale:1449760434, shift:40), (scale:1247262320, shift:40), (scale:1750285667, shift:40), (scale:1515939112, shift:40), (scale:2135718922, shift:41), (scale:1166327506, shift:40), (scale:1529173748, shift:40), (scale:1408902932, shift:40), (scale:1452086438, shift:39), (scale:1311332468, shift:40), (scale:1304867575, shift:40), (scale:1335663400, shift:40), (scale:1452101550, shift:40), (scale:1116154104, shift:39), (scale:1533898432, shift:40), (scale:1803069522, shift:40), (scale:1188907787, shift:40), (scale:1871860507, shift:40), (scale:2147046567, shift:41), (scale:1180098205, shift:40), (scale:1927849745, shift:40), (scale:1316717819, shift:40), (scale:1818268438, shift:40), (scale:1155365570, shift:39), (scale:1628559845, shift:40), (scale:2132136015, shift:40), (scale:1549151338, shift:40), (scale:1290972134, shift:40), (scale:1297072829, shift:40), (scale:1625203617, shift:40), (scale:1695649774, shift:39), (scale:1887106820, shift:40), (scale:1448880780, shift:40), (scale:1114455108, shift:40), (scale:1348699933, shift:40), (scale:1481692266, shift:40), (scale:1860064529, shift:40), (scale:1781257066, shift:40), (scale:1960908104, shift:40), (scale:1707953514, shift:40), (scale:1518911221, shift:40), (scale:1571123019, shift:40), (scale:1396613068, shift:40), (scale:1401841391, shift:40), (scale:1156581258, shift:40), (scale:1679951477, shift:40), (scale:1778012530, shift:40), (scale:1509443994, shift:40), (scale:1645242754, shift:40), (scale:1700548932, shift:40), (scale:1157513665, shift:40), (scale:1498694175, shift:40), (scale:1205419519, shift:40), (scale:1499242739, shift:40)], zero_point: [15], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20
58 Clamp aten_clamp_default_20
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [15], quantMax: [127], dimension: 0 aten_clamp_default_20
59 MemoryCopy aten_cat_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_20
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
60 Conv2D aten_convolution_default_21
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_19
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1771468092, shift:40), (scale:1752138530, shift:40), (scale:1114988217, shift:39), (scale:1537514860, shift:39), (scale:1680475999, shift:40), (scale:1981360347, shift:40), (scale:1900026715, shift:40), (scale:1085196100, shift:39), (scale:1626259531, shift:40), (scale:1378292160, shift:40), (scale:1264706047, shift:39), (scale:2077776945, shift:40), (scale:1411891939, shift:40), (scale:2133451167, shift:40), (scale:1486278469, shift:40), (scale:1631940251, shift:40), (scale:2030254660, shift:40), (scale:1877989916, shift:40), (scale:1650336444, shift:40), (scale:1835615312, shift:40), (scale:1826573418, shift:40), (scale:1289449398, shift:40), (scale:1923412875, shift:40), (scale:1700302607, shift:40), (scale:1158261158, shift:39), (scale:1868881667, shift:39), (scale:1143964906, shift:39), (scale:2060580504, shift:40), (scale:1876770244, shift:40), (scale:1849837446, shift:40), (scale:1085588667, shift:39), (scale:1298656974, shift:40), (scale:2109969252, shift:40), (scale:1146410913, shift:39), (scale:1346919470, shift:39), (scale:1467540059, shift:40), (scale:1547897184, shift:40), (scale:1673163599, shift:40), (scale:1840945984, shift:40), (scale:1814829095, shift:40), (scale:1940767030, shift:40), (scale:1648224068, shift:40), (scale:1910452727, shift:40), (scale:1317084902, shift:39), (scale:1253031514, shift:39), (scale:1481082292, shift:39), (scale:1150868596, shift:39), (scale:1688886350, shift:40), (scale:1941792033, shift:40), (scale:1821837057, shift:40), (scale:1104964801, shift:39), (scale:1080154615, shift:39), (scale:1563673926, shift:40), (scale:1800158548, shift:40), (scale:1557121657, shift:40), (scale:1082967499, shift:38), (scale:1465550364, shift:39), (scale:1773624568, shift:39), (scale:1349760242, shift:40), (scale:1340720959, shift:39), (scale:1971971840, shift:40), (scale:1742515238, shift:40), (scale:2026299725, shift:40), (scale:1322732514, shift:39), (scale:1601413831, shift:39), (scale:1485712045, shift:39), (scale:1466548715, shift:40), (scale:1240898676, shift:39), (scale:1389700860, shift:40), (scale:2029924395, shift:40), (scale:1596355860, shift:40), (scale:1336442902, shift:40), (scale:1408374151, shift:40), (scale:1585783538, shift:40), (scale:2028054222, shift:40), (scale:1080520805, shift:38), (scale:1476731973, shift:40), (scale:1866253423, shift:40), (scale:2040771893, shift:40), (scale:1428998396, shift:40), (scale:1421659207, shift:40), (scale:1511947358, shift:40), (scale:1707872047, shift:40), (scale:1821741851, shift:40), (scale:1350375024, shift:40), (scale:1597849196, shift:40), (scale:1282717023, shift:40), (scale:1627530721, shift:40), (scale:1145853351, shift:39), (scale:1740433086, shift:40), (scale:1741889741, shift:40), (scale:1571047734, shift:40), (scale:1661646780, shift:39), (scale:1907227149, shift:40), (scale:2046037859, shift:39), (scale:1427189083, shift:40), (scale:1678752549, shift:40), (scale:1784021729, shift:40), (scale:1388739740, shift:40), (scale:1593518247, shift:40), (scale:1491681267, shift:39), (scale:1716869154, shift:40), (scale:1899558794, shift:40), (scale:2101532111, shift:41), (scale:1803852184, shift:40), (scale:2124989023, shift:40), (scale:1687603483, shift:40), (scale:1595246231, shift:40), (scale:1807268172, shift:40), (scale:2008716005, shift:40), (scale:1833305243, shift:40), (scale:1394927260, shift:40), (scale:1884852940, shift:40), (scale:1640454188, shift:40), (scale:1371624354, shift:39), (scale:1786070773, shift:40), (scale:1848582879, shift:40), (scale:1817188208, shift:40), (scale:1856320199, shift:40), (scale:1622015545, shift:39), (scale:1271206386, shift:39), (scale:1508085017, shift:39), (scale:1925174380, shift:40), (scale:1591062555, shift:40), (scale:1662861643, shift:40), (scale:1607944805, shift:40), (scale:1169189985, shift:39), (scale:1290745317, shift:39), (scale:1943439785, shift:40), (scale:1185475862, shift:39), (scale:1679720264, shift:40), (scale:1962316676, shift:40), (scale:1564431447, shift:40), (scale:1902871197, shift:40), (scale:1190818075, shift:39), (scale:1867356596, shift:40), (scale:1494684149, shift:40), (scale:1104822406, shift:39), (scale:1138449662, shift:39), (scale:1378327879, shift:40), (scale:1344568325, shift:40), (scale:1273224793, shift:39), (scale:1533201771, shift:39), (scale:1847991177, shift:40), (scale:1512513919, shift:40), (scale:1365074284, shift:40), (scale:1550043768, shift:39), (scale:1345881004, shift:39), (scale:1920350644, shift:40), (scale:1695918217, shift:40), (scale:1454933942, shift:40), (scale:1968061279, shift:40), (scale:2138557908, shift:40), (scale:1287564525, shift:40), (scale:1541336810, shift:40), (scale:1431803998, shift:40), (scale:1447760160, shift:39), (scale:1496924702, shift:40), (scale:1462773199, shift:40), (scale:1820029805, shift:40), (scale:1883867092, shift:40), (scale:1473306780, shift:40), (scale:1645010030, shift:40), (scale:1290787630, shift:40), (scale:1554090337, shift:39), (scale:1863039111, shift:40), (scale:1561172760, shift:40), (scale:1852346854, shift:40), (scale:1435016525, shift:40), (scale:1647086275, shift:40), (scale:1784447062, shift:39), (scale:1957052770, shift:40), (scale:1351076631, shift:40), (scale:1164588532, shift:39), (scale:2073027808, shift:40), (scale:1199521324, shift:39), (scale:1333021418, shift:39), (scale:1955640215, shift:40), (scale:1663775779, shift:40), (scale:1869334201, shift:40), (scale:1210732058, shift:39), (scale:1593136327, shift:39), (scale:1994999435, shift:40), (scale:1370244358, shift:40), (scale:1652741306, shift:40), (scale:1951293330, shift:40), (scale:1830768221, shift:40), (scale:1492961524, shift:40), (scale:1148373956, shift:39), (scale:1401350940, shift:40), (scale:1738325655, shift:40), (scale:1154764664, shift:39), (scale:1713191729, shift:40), (scale:1089901138, shift:39), (scale:1699686314, shift:39), (scale:1243920792, shift:39), (scale:1707602642, shift:40), (scale:1558254779, shift:39), (scale:1131244488, shift:39), (scale:1134546038, shift:40), (scale:1887511270, shift:40), (scale:1362429967, shift:40), (scale:1254668963, shift:39), (scale:1632525084, shift:40), (scale:1469226965, shift:39), (scale:1226802933, shift:39), (scale:1183438359, shift:39), (scale:1913446405, shift:40), (scale:1372710491, shift:39), (scale:1593275220, shift:40), (scale:1117721351, shift:39), (scale:1989515307, shift:40), (scale:1661286703, shift:40), (scale:1799393883, shift:40), (scale:1709329663, shift:40), (scale:1604397892, shift:40), (scale:2122433179, shift:40), (scale:2144314326, shift:40), (scale:1653534546, shift:40), (scale:2034770249, shift:40), (scale:1995690051, shift:40), (scale:1110407234, shift:39), (scale:1668329011, shift:40), (scale:1901777916, shift:40), (scale:1209675183, shift:39), (scale:2026712693, shift:40), (scale:2090362180, shift:41), (scale:1250543812, shift:39), (scale:1152661492, shift:39), (scale:1792264063, shift:40), (scale:1178717522, shift:39), (scale:1096546001, shift:39), (scale:1543795113, shift:40), (scale:1896669664, shift:40), (scale:1984107838, shift:40), (scale:1521737156, shift:40), (scale:1732596439, shift:39), (scale:1757072720, shift:39), (scale:1622743254, shift:40), (scale:1121765722, shift:39), (scale:1663751463, shift:40), (scale:1103593392, shift:39), (scale:1378131149, shift:40), (scale:1763914451, shift:40), (scale:2128382343, shift:40), (scale:1077296807, shift:39), (scale:1171236968, shift:39), (scale:1510340133, shift:40), (scale:1846703364, shift:40), (scale:1413484052, shift:40), (scale:1118029909, shift:39), (scale:1864871229, shift:40), (scale:1646815634, shift:40), (scale:1732087029, shift:40), (scale:1775182748, shift:40), (scale:1560580783, shift:39)], zero_point: [15], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21
61 Clamp aten_clamp_default_21
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_21
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [15], quantMax: [127], dimension: 0 aten_clamp_default_21
62 MemoryCopy aten_cat_default_6
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_21
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
63 Conv2D aten_convolution_default_22
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [15], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_6
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1784496632, shift:40), (scale:1646304890, shift:40), (scale:1279370839, shift:40), (scale:1587326589, shift:40), (scale:1308744012, shift:40), (scale:1994344383, shift:40), (scale:1769420699, shift:40), (scale:1459540406, shift:40), (scale:1667639047, shift:40), (scale:1800205718, shift:40), (scale:1082311969, shift:39), (scale:1683357255, shift:40), (scale:1543962898, shift:40), (scale:2108963374, shift:40), (scale:1519260507, shift:40), (scale:1303306509, shift:40), (scale:1923171831, shift:40), (scale:1552418993, shift:40), (scale:1379373409, shift:40), (scale:1076217368, shift:39), (scale:1995563287, shift:40), (scale:1224374215, shift:39), (scale:1751107657, shift:40), (scale:1554865923, shift:40), (scale:1426637169, shift:40), (scale:1892942265, shift:40), (scale:2064504087, shift:40), (scale:1503756825, shift:40), (scale:1494083292, shift:40), (scale:1340181324, shift:40), (scale:1373395030, shift:39), (scale:1624342141, shift:40), (scale:1386312674, shift:40), (scale:2073543653, shift:40), (scale:1851716176, shift:40), (scale:1234056300, shift:40), (scale:1191764993, shift:39), (scale:1428547539, shift:39), (scale:1833562115, shift:40), (scale:1400829896, shift:39), (scale:1373896567, shift:40), (scale:1533399499, shift:40), (scale:1518686972, shift:40), (scale:1994971672, shift:40), (scale:1101225177, shift:39), (scale:1160174321, shift:39), (scale:1240896528, shift:39), (scale:1564912473, shift:40), (scale:1882199524, shift:40), (scale:1509389063, shift:40), (scale:1511379739, shift:40), (scale:1804477990, shift:40), (scale:1100890683, shift:39), (scale:1437478621, shift:40), (scale:1369104188, shift:40), (scale:1150217930, shift:40), (scale:1872781891, shift:40), (scale:1694546477, shift:40), (scale:1638032861, shift:40), (scale:1822266690, shift:40), (scale:1376133532, shift:40), (scale:1314567157, shift:40), (scale:1283948937, shift:40), (scale:1901808679, shift:40)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22
64 Clamp aten_clamp_default_22
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_22
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-8], quantMax: [127], dimension: 0 aten_clamp_default_22
65 Conv2D aten_convolution_default_23
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1408833788, shift:40), (scale:1170268646, shift:41), (scale:1240863838, shift:41), (scale:1374089943, shift:41), (scale:1477302683, shift:41), (scale:1747260650, shift:41), (scale:1265246066, shift:40), (scale:1097789432, shift:40), (scale:1871702270, shift:41), (scale:1840760900, shift:41), (scale:1571494746, shift:41), (scale:1899736154, shift:41), (scale:1388262206, shift:41), (scale:1450927050, shift:41), (scale:1725324198, shift:41), (scale:1501198856, shift:41), (scale:2039365664, shift:41), (scale:1360485855, shift:41), (scale:1245266002, shift:41), (scale:1340928649, shift:41), (scale:1893256548, shift:41), (scale:1163890085, shift:41), (scale:1446650944, shift:41), (scale:1525400169, shift:41), (scale:1231901772, shift:41), (scale:1814996878, shift:41), (scale:1220355201, shift:41), (scale:1473902974, shift:41), (scale:1539062200, shift:41), (scale:1253365843, shift:41), (scale:1930398231, shift:41), (scale:1632615641, shift:41), (scale:1179840071, shift:40), (scale:1274804735, shift:41), (scale:1274283582, shift:41), (scale:1419228670, shift:41), (scale:1858675693, shift:41), (scale:1525829864, shift:41), (scale:1292454242, shift:41), (scale:1704339012, shift:41), (scale:1816317144, shift:41), (scale:1328159357, shift:41), (scale:1810984058, shift:40), (scale:2050434352, shift:41), (scale:1403511374, shift:41), (scale:1460910413, shift:41), (scale:1524654664, shift:41), (scale:1839663569, shift:41), (scale:1634716260, shift:41), (scale:1208841144, shift:41), (scale:1611552919, shift:41), (scale:1427483415, shift:41), (scale:1639983483, shift:41), (scale:1641832655, shift:41), (scale:1778809795, shift:41), (scale:1271795454, shift:41), (scale:1947209524, shift:41), (scale:1196322465, shift:41), (scale:1340198235, shift:41), (scale:1237675475, shift:41), (scale:1225712465, shift:41), (scale:1566577015, shift:41), (scale:1279279849, shift:41), (scale:1892631097, shift:41), (scale:1305031114, shift:41), (scale:1550689641, shift:41), (scale:1438304491, shift:41), (scale:1343158911, shift:41), (scale:1539821711, shift:41), (scale:1185094622, shift:41), (scale:2083227229, shift:42), (scale:1700481928, shift:41), (scale:1516535147, shift:41), (scale:1177818737, shift:41), (scale:1502607329, shift:41), (scale:1666612564, shift:41), (scale:1297281265, shift:41), (scale:1272443081, shift:41), (scale:1386158752, shift:41), (scale:1259122454, shift:40), (scale:1433592186, shift:41), (scale:1769319657, shift:41), (scale:1599188476, shift:41), (scale:1399023004, shift:41), (scale:1665012504, shift:41), (scale:1423838095, shift:41), (scale:1390617774, shift:41), (scale:1289808459, shift:41), (scale:1211183373, shift:41), (scale:1430072090, shift:40), (scale:1231544860, shift:41), (scale:2037054449, shift:41), (scale:1360096011, shift:41), (scale:1418067310, shift:41), (scale:1467275134, shift:41), (scale:1922898162, shift:41), (scale:1482479364, shift:41), (scale:1245947561, shift:41), (scale:1299618909, shift:41), (scale:1603187957, shift:41), (scale:1438572779, shift:41), (scale:1213393960, shift:41), (scale:1252599245, shift:41), (scale:1880556120, shift:41), (scale:1800999362, shift:41), (scale:1438141083, shift:41), (scale:1292981398, shift:41), (scale:1409744868, shift:41), (scale:1845625773, shift:41), (scale:1928836856, shift:41), (scale:1432647591, shift:41), (scale:1320656536, shift:41), (scale:1189433256, shift:41), (scale:1371740127, shift:41), (scale:1502517288, shift:41), (scale:1551160522, shift:41), (scale:1547472849, shift:40), (scale:1635138785, shift:41), (scale:1648940380, shift:41), (scale:1616300406, shift:41), (scale:1611849553, shift:41), (scale:1525820860, shift:41), (scale:1696788253, shift:41), (scale:1455916981, shift:41), (scale:1180881544, shift:41), (scale:1513115429, shift:41), (scale:1204050971, shift:41), (scale:1383737403, shift:41), (scale:1212072027, shift:41), (scale:1529986249, shift:41), (scale:1480742576, shift:41), (scale:1376029907, shift:41), (scale:1256627990, shift:41), (scale:1851102925, shift:41), (scale:1132340856, shift:41), (scale:1318433028, shift:41), (scale:1489618603, shift:41), (scale:1994764764, shift:41), (scale:1745215555, shift:41), (scale:1436923031, shift:41), (scale:1141466746, shift:41), (scale:1445882429, shift:41), (scale:1767847656, shift:41), (scale:1671904631, shift:41), (scale:1631923493, shift:41), (scale:1326551961, shift:40), (scale:1248729656, shift:40), (scale:1493903047, shift:41), (scale:1697886918, shift:41), (scale:2020012718, shift:41), (scale:1687710301, shift:41), (scale:1415850471, shift:41), (scale:2097125700, shift:42), (scale:1404376933, shift:41), (scale:1554272600, shift:41), (scale:1330974134, shift:40), (scale:1690524578, shift:41), (scale:1444413929, shift:41), (scale:1299950893, shift:41), (scale:1293515640, shift:41), (scale:1599410577, shift:41), (scale:1743938142, shift:41), (scale:1681186842, shift:41), (scale:1270767321, shift:41), (scale:1504328276, shift:41), (scale:1219582100, shift:41), (scale:1216834020, shift:41), (scale:2018431834, shift:41), (scale:1495515445, shift:41), (scale:1420080724, shift:41), (scale:1184526697, shift:40), (scale:1214482037, shift:40), (scale:1251563859, shift:41), (scale:1960677467, shift:41), (scale:1306852524, shift:41), (scale:1678713053, shift:41), (scale:1500382319, shift:41), (scale:2088830937, shift:41), (scale:1438960789, shift:41), (scale:1206353682, shift:41), (scale:1229478256, shift:41), (scale:1911237038, shift:41), (scale:1339432721, shift:41), (scale:1447802633, shift:41), (scale:1359332915, shift:41), (scale:1249239888, shift:41), (scale:1844395715, shift:41), (scale:1354415768, shift:41), (scale:1410439850, shift:41), (scale:1479828328, shift:41), (scale:1558672429, shift:41), (scale:1579707472, shift:41), (scale:1119753563, shift:40), (scale:1203057854, shift:41), (scale:1309122054, shift:41), (scale:1743006887, shift:41), (scale:1434332356, shift:41), (scale:1710971854, shift:41), (scale:1298212438, shift:41), (scale:1376581157, shift:41), (scale:2144966237, shift:42), (scale:1819449565, shift:41), (scale:2027605496, shift:41), (scale:1311763335, shift:41), (scale:1528296149, shift:41), (scale:1496868058, shift:41), (scale:1351573979, shift:41), (scale:1504704614, shift:41), (scale:2004186872, shift:42), (scale:1337045805, shift:41), (scale:1368287061, shift:41), (scale:1324126861, shift:40), (scale:1249184613, shift:41), (scale:1927906934, shift:41), (scale:1762583434, shift:41), (scale:1364773550, shift:41), (scale:1333340290, shift:41), (scale:1876606161, shift:41), (scale:1515207545, shift:41), (scale:1494474806, shift:41), (scale:1099148715, shift:41), (scale:2058915700, shift:41), (scale:1775694882, shift:41), (scale:1491748236, shift:41), (scale:1602045773, shift:41), (scale:1461461163, shift:41), (scale:1202050397, shift:41), (scale:1911011769, shift:41), (scale:1090820104, shift:41), (scale:1597966255, shift:41), (scale:1239151228, shift:41), (scale:1204806647, shift:41), (scale:1926525140, shift:41), (scale:1497050141, shift:41), (scale:1201701239, shift:41), (scale:1124895062, shift:41), (scale:1149705734, shift:40), (scale:1642862289, shift:41), (scale:1091895425, shift:40), (scale:1703788095, shift:41), (scale:1264105632, shift:41), (scale:1708018347, shift:41), (scale:1420368187, shift:41), (scale:1806775482, shift:41), (scale:1464406666, shift:41), (scale:1587742784, shift:41), (scale:1419848952, shift:41), (scale:1281150531, shift:41), (scale:1998652361, shift:41), (scale:1731326755, shift:41), (scale:1856986094, shift:41), (scale:1901158466, shift:41), (scale:1999364851, shift:41), (scale:1374399417, shift:41), (scale:1487213679, shift:41), (scale:1824531704, shift:41)], zero_point: [59], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23
66 Clamp aten_clamp_default_23
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_23
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [59], quantMax: [127], dimension: 0 aten_clamp_default_23
67 MemoryCopy aten_cat_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_23
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
68 Conv2D aten_convolution_default_24
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-8], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_22
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1724557350, shift:40), (scale:1201935178, shift:40), (scale:1767059131, shift:40), (scale:1439109189, shift:40), (scale:1239410429, shift:40), (scale:1768326706, shift:40), (scale:1412223326, shift:40), (scale:1853246397, shift:40), (scale:1707756395, shift:40), (scale:1439413494, shift:40), (scale:1124583420, shift:40), (scale:1330572201, shift:40), (scale:1277053089, shift:40), (scale:1414469511, shift:40), (scale:1607956454, shift:40), (scale:1424460044, shift:40), (scale:1421644433, shift:40), (scale:1703114956, shift:40), (scale:1563491616, shift:40), (scale:1519459807, shift:40), (scale:1203728325, shift:40), (scale:1896603733, shift:40), (scale:1845520892, shift:40), (scale:1365708974, shift:40), (scale:1382604056, shift:40), (scale:1159928121, shift:40), (scale:1102147159, shift:39), (scale:1213509846, shift:40), (scale:1534572163, shift:40), (scale:1295644523, shift:40), (scale:1733921598, shift:40), (scale:1875681908, shift:40), (scale:1348662324, shift:40), (scale:1522881526, shift:40), (scale:1559157149, shift:40), (scale:1600359841, shift:40), (scale:1381247023, shift:40), (scale:1696147129, shift:40), (scale:1389580137, shift:40), (scale:1823255458, shift:40), (scale:1631661875, shift:40), (scale:1871580548, shift:40), (scale:1309545329, shift:40), (scale:1185234101, shift:40), (scale:1776447557, shift:40), (scale:2044516000, shift:40), (scale:1770999752, shift:40), (scale:2119144189, shift:40), (scale:1174299474, shift:39), (scale:1193124014, shift:40), (scale:1750411746, shift:40), (scale:1402484408, shift:40), (scale:1451574678, shift:40), (scale:1623758456, shift:40), (scale:1438761532, shift:40), (scale:1472231048, shift:40), (scale:1278591453, shift:40), (scale:1215773956, shift:39), (scale:1175899283, shift:40), (scale:1149419021, shift:40), (scale:1372434275, shift:40), (scale:1267164853, shift:40), (scale:1467974618, shift:40), (scale:1613124298, shift:40), (scale:1301929958, shift:40), (scale:1644809339, shift:40), (scale:1652258552, shift:40), (scale:1451857306, shift:40), (scale:1807413605, shift:40), (scale:1535289822, shift:40), (scale:1401203827, shift:40), (scale:1752282428, shift:40), (scale:1831827180, shift:40), (scale:1881293621, shift:40), (scale:1295788672, shift:40), (scale:1654262294, shift:40), (scale:1789785775, shift:40), (scale:1640295791, shift:40), (scale:1718109258, shift:40), (scale:1701977607, shift:40), (scale:1870163572, shift:40), (scale:1518096856, shift:40), (scale:1862026213, shift:40), (scale:1601057324, shift:40), (scale:1456332669, shift:40), (scale:1550048184, shift:40), (scale:1567246152, shift:40), (scale:1300395511, shift:39), (scale:1512887659, shift:40), (scale:1506403551, shift:40), (scale:1512212186, shift:40), (scale:1490957711, shift:40), (scale:1450093005, shift:40), (scale:1202856096, shift:40), (scale:1141333352, shift:40), (scale:1838339134, shift:40), (scale:1512584855, shift:40), (scale:1429759281, shift:40), (scale:1406173081, shift:40), (scale:1729015706, shift:40), (scale:1294991226, shift:40), (scale:1512879155, shift:40), (scale:1372922413, shift:40), (scale:1146014643, shift:39), (scale:2107415035, shift:41), (scale:1443490177, shift:40), (scale:1313129538, shift:40), (scale:1806815500, shift:40), (scale:1538524122, shift:40), (scale:1449104224, shift:40), (scale:1665179580, shift:40), (scale:1875618880, shift:40), (scale:1078770887, shift:39), (scale:1126843946, shift:39), (scale:1322252844, shift:40), (scale:1405965820, shift:40), (scale:1913228608, shift:40), (scale:1320556824, shift:40), (scale:2079525883, shift:40), (scale:1790531613, shift:40), (scale:2071561436, shift:40), (scale:1310785975, shift:40), (scale:1125675749, shift:39), (scale:1214728482, shift:40), (scale:1250164307, shift:40), (scale:1569717774, shift:40), (scale:1803551686, shift:40), (scale:1455153968, shift:40), (scale:1206313748, shift:40), (scale:2088953660, shift:40), (scale:1221805609, shift:40), (scale:1497170362, shift:40), (scale:2074318854, shift:40), (scale:1532182579, shift:39), (scale:1546430209, shift:40), (scale:1710568171, shift:40), (scale:1258151264, shift:40), (scale:1636569768, shift:40), (scale:1266465786, shift:40), (scale:1981809887, shift:40), (scale:1399055686, shift:40), (scale:1458801289, shift:39), (scale:1299183878, shift:40), (scale:1884373185, shift:40), (scale:1386116482, shift:40), (scale:1169173065, shift:39), (scale:1663572684, shift:40), (scale:1441945476, shift:40), (scale:1529819340, shift:40), (scale:1728638535, shift:40), (scale:1415411439, shift:40), (scale:1742341251, shift:40), (scale:1208885915, shift:40), (scale:1650836073, shift:40), (scale:1437114785, shift:40), (scale:1130459836, shift:39), (scale:1992205937, shift:40), (scale:1904288386, shift:40), (scale:1924327643, shift:40), (scale:1512459632, shift:40), (scale:1378120522, shift:40), (scale:1462101620, shift:40), (scale:1167985527, shift:40), (scale:1606780587, shift:40), (scale:1519317409, shift:40), (scale:1444808942, shift:40), (scale:1980702718, shift:40), (scale:1565792993, shift:40), (scale:1490806142, shift:40), (scale:1262705997, shift:40), (scale:1753171998, shift:40), (scale:1494894663, shift:40), (scale:1371382715, shift:40), (scale:1534877635, shift:40), (scale:1518194233, shift:40), (scale:1726550755, shift:40), (scale:1347849789, shift:40), (scale:1839650229, shift:40), (scale:1526168517, shift:40), (scale:1109910681, shift:40), (scale:1400610057, shift:40), (scale:1801959131, shift:40), (scale:1707571311, shift:40), (scale:1351430163, shift:40), (scale:1088412762, shift:39), (scale:1510102562, shift:40), (scale:1301285248, shift:40), (scale:1978790684, shift:40), (scale:1714263014, shift:40), (scale:1627325741, shift:40), (scale:1571236963, shift:40), (scale:1395780533, shift:40), (scale:1446255431, shift:40), (scale:1203020337, shift:40), (scale:1330157263, shift:40), (scale:1168406801, shift:40), (scale:1225181307, shift:40), (scale:1494820463, shift:40), (scale:1767637727, shift:40), (scale:1590834687, shift:40), (scale:1194049934, shift:40), (scale:1470667506, shift:40), (scale:1401248847, shift:40), (scale:1354827871, shift:40), (scale:1559150980, shift:40), (scale:1344570635, shift:40), (scale:1497153188, shift:40), (scale:1125979971, shift:40), (scale:1389244151, shift:40), (scale:1444489130, shift:40), (scale:1361183421, shift:40), (scale:1157299345, shift:39), (scale:1299060573, shift:40), (scale:1757555820, shift:40), (scale:1337713691, shift:40), (scale:1399687972, shift:40), (scale:1497773636, shift:40), (scale:1302191326, shift:40), (scale:1474906095, shift:40), (scale:1358576322, shift:40), (scale:1393451727, shift:40), (scale:1308451583, shift:40), (scale:1857278726, shift:40), (scale:1534467282, shift:40), (scale:1403798504, shift:40), (scale:1086242360, shift:39), (scale:1964443842, shift:40), (scale:1444839956, shift:40), (scale:1221638616, shift:40), (scale:1547490023, shift:40), (scale:1628318024, shift:40), (scale:1220434820, shift:40), (scale:1346513516, shift:40), (scale:1822385564, shift:40), (scale:1287645477, shift:39), (scale:1074669027, shift:40), (scale:1480938665, shift:40), (scale:2011481681, shift:40), (scale:1506352194, shift:40), (scale:1159904027, shift:40), (scale:1349789086, shift:39), (scale:1506054726, shift:40), (scale:1453750498, shift:40), (scale:1342845519, shift:40), (scale:1332915598, shift:40), (scale:1644049995, shift:40), (scale:1961067310, shift:40), (scale:1398784396, shift:40), (scale:1157472507, shift:39), (scale:1083628258, shift:39), (scale:1952447233, shift:40), (scale:1475425831, shift:40), (scale:1979885514, shift:40), (scale:1450669934, shift:40), (scale:1260199943, shift:40), (scale:1790204465, shift:40)], zero_point: [59], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24
69 Clamp aten_clamp_default_24
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_24
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [59], quantMax: [127], dimension: 0 aten_clamp_default_24
70 MemoryCopy aten_cat_default_7
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_24
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
71 Conv2D aten_convolution_default_25
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [59], quantMin: [], quantMax: [], dimension: 0 aten_cat_default_7
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1227061508, shift:38), (scale:1936772311, shift:39), (scale:1129617906, shift:38), (scale:1217840579, shift:38)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25
72 Clamp aten_clamp_default_25
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_convolution_default_25
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [-26], quantMax: [127], dimension: 0 aten_clamp_default_25
73 AvgPool aten_view_copy_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_clamp_default_25
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-26], quantMin: [], quantMax: [], dimension: 0 aten_view_copy_default

Schedule: 'graph_MAX_BUFFERED'
	0: Operation Transpose  - OFM 1, 224, 224, 3
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 0
		Operator Config = OFM Block=[16, 2, 128], IFM Block=[1, 16, 2, 128], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 3, 224, 224]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 224, 224, 3]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=150528 Cycles=2140919
		Memory Used: 953344 bytes
	1: Operation MemoryCopy  - OFM 1, 223, 224, 3
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 2
		Operator Config = OFM Block=[1, 10, 6, 16], IFM Block=[1, 10, 6, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 223, 224, 3]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 223, 224, 3]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=149856 Cycles=399751
		Memory Used: 952672 bytes
	2: Operation Conv2D  - OFM 1, 111, 111, 64
		Kernel: size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 4
		Operator Config = OFM Block=[1, 4, 16, 32], IFM Block=[1, 9, 36, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 223, 223, 3]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 111, 111, 64]
		Assigned Cascade = 0
		Encoded Weights = 3264 bytes
		Weight buffer = 3264 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=21290688 Cycles=296216
		Memory Used: 941664 bytes
	3: Operation MaxPool  - OFM 1, 55, 55, 64
		Kernel: size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 6
		Operator Config = OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 111, 111, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=1742400 Cycles=48859
		Memory Used: 982144 bytes
	4: Operation Conv2D  - OFM 1, 55, 55, 16
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 8
		Operator Config = OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 16]
		Assigned Cascade = 0
		Encoded Weights = 1168 bytes
		Weight buffer = 1168 bytes
		Depth slices = [0, 16]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=3097600 Cycles=12760
		Memory Used: 243168 bytes
	5: Operation Conv2D  - OFM 1, 55, 55, 64
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 10
		Operator Config = OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 16]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		Encoded Weights = 9904 bytes
		Weight buffer = 9904 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=27878400 Cycles=145754
		Memory Used: 251904 bytes
	6: Operation MemoryCopy  - OFM 1, 55, 55, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 12
		Operator Config = OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=193600 Cycles=97167
		Memory Used: 629200 bytes
	7: Operation Conv2D  - OFM 1, 55, 55, 64
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 14
		Operator Config = OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 16]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		Encoded Weights = 2160 bytes
		Weight buffer = 2160 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=3097600 Cycles=48912
		Memory Used: 631360 bytes
	8: Operation MemoryCopy  - OFM 1, 55, 55, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 16
		Operator Config = OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=193600 Cycles=97167
		Memory Used: 580800 bytes
	9: Operation Conv2D  - OFM 1, 55, 55, 16
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 18
		Operator Config = OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 16]
		Assigned Cascade = 0
		Encoded Weights = 2048 bytes
		Weight buffer = 2048 bytes
		Depth slices = [0, 16]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=6195200 Cycles=25080
		Memory Used: 437648 bytes
	10: Operation Conv2D  - OFM 1, 55, 55, 64
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 20
		Operator Config = OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 16]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		Encoded Weights = 10288 bytes
		Weight buffer = 10288 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=27878400 Cycles=145754
		Memory Used: 252288 bytes
	11: Operation MemoryCopy  - OFM 1, 55, 55, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 22
		Operator Config = OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=193600 Cycles=97167
		Memory Used: 629200 bytes
	12: Operation Conv2D  - OFM 1, 55, 55, 64
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 24
		Operator Config = OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 16]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		Encoded Weights = 2176 bytes
		Weight buffer = 2176 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=3097600 Cycles=48912
		Memory Used: 631376 bytes
	13: Operation MemoryCopy  - OFM 1, 55, 55, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 26
		Operator Config = OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 55, 55, 64]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=193600 Cycles=97167
		Memory Used: 580800 bytes
	14: Operation MaxPool  - OFM 1, 27, 27, 128
		Kernel: size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 28
		Operator Config = OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 55, 55, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=839808 Cycles=12697
		Memory Used: 480512 bytes
	15: Operation Conv2D  - OFM 1, 27, 27, 32
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 30
		Operator Config = OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 32]
		Assigned Cascade = 0
		Encoded Weights = 4000 bytes
		Weight buffer = 4000 bytes
		Depth slices = [0, 32]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2985984 Cycles=12528
		Memory Used: 120640 bytes
	16: Operation Conv2D  - OFM 1, 27, 27, 128
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 32
		Operator Config = OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 32]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		Encoded Weights = 38896 bytes
		Weight buffer = 38896 bytes
		Depth slices = [0, 64, 128]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=26873856 Cycles=145966
		Memory Used: 155536 bytes
	17: Operation MemoryCopy  - OFM 1, 27, 27, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 34
		Operator Config = OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=93312 Cycles=46960
		Memory Used: 303264 bytes
	18: Operation Conv2D  - OFM 1, 27, 27, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 36
		Operator Config = OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 32]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		Encoded Weights = 7264 bytes
		Weight buffer = 7264 bytes
		Depth slices = [0, 128]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2985984 Cycles=48816
		Memory Used: 310528 bytes
	19: Operation MemoryCopy  - OFM 1, 27, 27, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 38
		Operator Config = OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=93312 Cycles=46960
		Memory Used: 279936 bytes
	20: Operation Conv2D  - OFM 1, 27, 27, 32
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 40
		Operator Config = OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 32]
		Assigned Cascade = 0
		Encoded Weights = 7552 bytes
		Weight buffer = 7552 bytes
		Depth slices = [0, 32]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=5971968 Cycles=24789
		Memory Used: 217504 bytes
	21: Operation Conv2D  - OFM 1, 27, 27, 128
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 42
		Operator Config = OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 32]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		Encoded Weights = 39008 bytes
		Weight buffer = 39008 bytes
		Depth slices = [0, 128]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=26873856 Cycles=145966
		Memory Used: 155648 bytes
	22: Operation MemoryCopy  - OFM 1, 27, 27, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 44
		Operator Config = OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=93312 Cycles=46960
		Memory Used: 303264 bytes
	23: Operation Conv2D  - OFM 1, 27, 27, 128
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 46
		Operator Config = OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 32]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		Encoded Weights = 7264 bytes
		Weight buffer = 7264 bytes
		Depth slices = [0, 128]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2985984 Cycles=48816
		Memory Used: 310528 bytes
	24: Operation MemoryCopy  - OFM 1, 27, 27, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 48
		Operator Config = OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 128]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 27, 27, 128]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=93312 Cycles=46960
		Memory Used: 279936 bytes
	25: Operation MaxPool  - OFM 1, 13, 13, 256
		Kernel: size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 50
		Operator Config = OFM Block=[13, 14, 16], IFM Block=[1, 28, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 27, 27, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=389376 Cycles=4377
		Memory Used: 229888 bytes
	26: Operation Conv2D  - OFM 1, 13, 13, 48
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 52
		Operator Config = OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 48]
		Assigned Cascade = 0
		Encoded Weights = 11392 bytes
		Weight buffer = 11392 bytes
		Depth slices = [0, 32, 48]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2076672 Cycles=9173
		Memory Used: 62768 bytes
	27: Operation Conv2D  - OFM 1, 13, 13, 192
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 54
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 48]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		Encoded Weights = 86384 bytes
		Weight buffer = 43312 bytes
		Depth slices = [0, 16, 64, 112, 160, 192]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=14017536 Cycles=79766
		Memory Used: 83872 bytes
	28: Operation MemoryCopy  - OFM 1, 13, 13, 384
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 56
		Operator Config = OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 192]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=32448 Cycles=16303
		Memory Used: 105456 bytes
	29: Operation Conv2D  - OFM 1, 13, 13, 192
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 58
		Operator Config = OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 48]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		Encoded Weights = 12352 bytes
		Weight buffer = 12352 bytes
		Depth slices = [0, 192]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=1557504 Cycles=8496
		Memory Used: 117808 bytes
	30: Operation MemoryCopy  - OFM 1, 13, 13, 384
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 60
		Operator Config = OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 192]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=32448 Cycles=16303
		Memory Used: 97344 bytes
	31: Operation Conv2D  - OFM 1, 13, 13, 48
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 62
		Operator Config = OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 384]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 48]
		Assigned Cascade = 0
		Encoded Weights = 16624 bytes
		Weight buffer = 16624 bytes
		Depth slices = [0, 48]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=3115008 Cycles=13709
		Memory Used: 89632 bytes
	32: Operation Conv2D  - OFM 1, 13, 13, 192
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 64
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 48]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		Encoded Weights = 87360 bytes
		Weight buffer = 65600 bytes
		Depth slices = [0, 48, 144, 192]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=14017536 Cycles=79766
		Memory Used: 106160 bytes
	33: Operation MemoryCopy  - OFM 1, 13, 13, 384
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 66
		Operator Config = OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 192]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=32448 Cycles=16303
		Memory Used: 105456 bytes
	34: Operation Conv2D  - OFM 1, 13, 13, 192
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 68
		Operator Config = OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 48]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		Encoded Weights = 12384 bytes
		Weight buffer = 12384 bytes
		Depth slices = [0, 192]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=1557504 Cycles=8496
		Memory Used: 117840 bytes
	35: Operation MemoryCopy  - OFM 1, 13, 13, 384
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 70
		Operator Config = OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 192]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 192]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=32448 Cycles=16303
		Memory Used: 97344 bytes
	36: Operation Conv2D  - OFM 1, 13, 13, 64
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 72
		Operator Config = OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 384]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 64]
		Assigned Cascade = 0
		Encoded Weights = 22336 bytes
		Weight buffer = 22336 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=4153344 Cycles=18288
		Memory Used: 98048 bytes
	37: Operation Conv2D  - OFM 1, 13, 13, 256
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 74
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		Encoded Weights = 154704 bytes
		Weight buffer = 116000 bytes
		Depth slices = [0, 48, 144, 240, 256]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=24920064 Cycles=141254
		Memory Used: 170080 bytes
	38: Operation MemoryCopy  - OFM 1, 13, 13, 512
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 76
		Operator Config = OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=43264 Cycles=21808
		Memory Used: 140608 bytes
	39: Operation Conv2D  - OFM 1, 13, 13, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 78
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		Encoded Weights = 18560 bytes
		Weight buffer = 18560 bytes
		Depth slices = [0, 256]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2768896 Cycles=12064
		Memory Used: 159168 bytes
	40: Operation MemoryCopy  - OFM 1, 13, 13, 512
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 80
		Operator Config = OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=43264 Cycles=21808
		Memory Used: 129792 bytes
	41: Operation Conv2D  - OFM 1, 13, 13, 64
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 82
		Operator Config = OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 512]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 64]
		Assigned Cascade = 0
		Encoded Weights = 29856 bytes
		Weight buffer = 29856 bytes
		Depth slices = [0, 64]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=5537792 Cycles=24336
		Memory Used: 127200 bytes
	42: Operation Conv2D  - OFM 1, 13, 13, 256
		Kernel: size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0]
		Time index = 84
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		Encoded Weights = 157248 bytes
		Weight buffer = 118064 bytes
		Depth slices = [0, 64, 192, 256]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=24920064 Cycles=141254
		Memory Used: 172144 bytes
	43: Operation MemoryCopy  - OFM 1, 13, 13, 512
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 86
		Operator Config = OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=43264 Cycles=21808
		Memory Used: 140608 bytes
	44: Operation Conv2D  - OFM 1, 13, 13, 256
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 88
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 64]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		Encoded Weights = 18880 bytes
		Weight buffer = 18880 bytes
		Depth slices = [0, 256]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=2768896 Cycles=12064
		Memory Used: 159488 bytes
	45: Operation MemoryCopy  - OFM 1, 13, 13, 512
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 90
		Operator Config = OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 256]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 256]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=43264 Cycles=21808
		Memory Used: 129792 bytes
	46: Operation Conv2D  - OFM 1, 13, 13, 4
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 92
		Operator Config = OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 512]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 13, 13, 4]
		Assigned Cascade = 0
		Encoded Weights = 2528 bytes
		Weight buffer = 2528 bytes
		Depth slices = [0, 4]
		sub-operations: [ Clamp ]
		Estimated Perf: Macs=346112 Cycles=3442
		Memory Used: 91760 bytes
	47: Operation AvgPool  - OFM 1, 1, 1, 4
		Kernel: size=13,13 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 94
		Operator Config = OFM Block=[1, 2, 16], IFM Block=[1, 8, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 13, 13, 4]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 4]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=676 Cycles=858
		Memory Used: 2720 bytes
	Cascades:
################################################################################
Allocation, memory Sram, usage mask: FeatureMap|Staging
Start Time - End Time  : Start Addr -   End Addr: Tensor Size: Memory Usage : Name
         0 -          1:    0xc4000 -    0xe8c00:      150528:       953344 : quantized_decomposed_quantize_per_tensor_default
         0 -          3:        0x0 -    0xc4000:      802816:       955936 : tosa_transpose_default
         2 -          5:    0xc4000 -    0xe8960:      149856:       955936 : aten_slice_copy_tensor
         3 -          5:    0xe8960 -    0xe9620:        3264:       955936 : const_values
         4 -          7:        0x0 -    0xc0840:      788544:       983312 : aten_clamp_default
         6 -          9:    0xc0840 -    0xefc80:      193600:       983312 : aten_max_pool2d_default
         7 -          9:    0xefc80 -    0xf0110:        1168:       983312 : const_values
         8 -         15:        0x0 -     0xbd10:       48400:       631360 : aten_clamp_default_1
         9 -         11:     0xbd10 -     0xe3c0:        9904:       253072 : const_values
        10 -         13:    0x6a590 -    0x999d0:      193600:       631360 : aten_clamp_default_2
        12 -         19:     0xbd10 -    0x6a590:      387200:       631360 : aten_cat_default
        13 -         15:    0x999d0 -    0x9a240:        2160:       631360 : const_values
        14 -         17:    0x6a590 -    0x999d0:      193600:       631360 : aten_clamp_default_3
        17 -         19:    0x999d0 -    0x9a1d0:        2048:       582848 : const_values
        18 -         25:        0x0 -     0xbd10:       48400:       631376 : aten_clamp_default_4
        19 -         21:    0x9a1d0 -    0x9ca00:       10288:       447936 : const_values
        20 -         23:    0x6a590 -    0x999d0:      193600:       631376 : aten_clamp_default_5
        22 -         29:     0xbd10 -    0x6a590:      387200:       631376 : aten_cat_default_1
        23 -         25:    0x999d0 -    0x9a250:        2176:       631376 : const_values
        24 -         27:    0x6a590 -    0x999d0:      193600:       631376 : aten_clamp_default_6
        28 -         31:    0x6a590 -    0x81210:       93312:       484512 : aten_max_pool2d_default_1
        29 -         31:        0x0 -      0xfa0:        4000:       484512 : const_values
        30 -         37:      0xfa0 -     0x6ac0:       23328:       310528 : aten_clamp_default_7
        31 -         33:     0x6ac0 -    0x102b0:       38896:       159536 : const_values
        32 -         35:    0x343c0 -    0x4b040:       93312:       310528 : aten_clamp_default_8
        34 -         41:     0x6ac0 -    0x343c0:      186624:       310528 : aten_cat_default_2
        35 -         37:    0x4b040 -    0x4cca0:        7264:       310528 : const_values
        36 -         39:    0x343c0 -    0x4b040:       93312:       310528 : aten_clamp_default_9
        39 -         41:    0x4b040 -    0x4cdc0:        7552:       287488 : const_values
        40 -         47:        0x0 -     0x5b20:       23328:       310528 : aten_clamp_default_10
        41 -         43:    0x4cdc0 -    0x56620:       39008:       256512 : const_values
        42 -         45:    0x33420 -    0x4a0a0:       93312:       310528 : aten_clamp_default_11
        44 -         51:     0x5b20 -    0x33420:      186624:       310528 : aten_cat_default_3
        45 -         47:    0x4a0a0 -    0x4bd00:        7264:       310528 : const_values
        46 -         49:    0x33420 -    0x4a0a0:       93312:       310528 : aten_clamp_default_12
        50 -         53:    0x33420 -    0x3dd20:       43264:       241280 : aten_max_pool2d_default_2
        51 -         53:        0x0 -     0x2c80:       11392:       241280 : const_values
        52 -         59:     0xfd80 -    0x11d30:        8112:       117808 : aten_clamp_default_13
        53 -         55:     0x2c80 -     0xd5b0:       43312:       106080 : const_values
        54 -         57:    0x11d30 -    0x19bf0:       32448:       117808 : aten_clamp_default_14
        56 -         63:        0x0 -     0xfd80:       64896:       155232 : aten_cat_default_4
        57 -         59:    0x1dce0 -    0x20d20:       12352:       117808 : const_values
        58 -         61:    0x15e20 -    0x1dce0:       32448:       117808 : aten_clamp_default_15
        61 -         63:    0x11d30 -    0x15e20:       16624:       155232 : const_values
        62 -         69:     0xfd80 -    0x11d30:        8112:       155232 : aten_clamp_default_16
        63 -         65:    0x15e20 -    0x25e60:       65600:       155232 : const_values
        64 -         67:    0x25e60 -    0x2dd20:       32448:       117840 : aten_clamp_default_17
        66 -         73:        0x0 -     0xfd80:       64896:       214048 : aten_cat_default_5
        67 -         69:    0x11d30 -    0x14d90:       12384:       117840 : const_values
        68 -         71:    0x1d380 -    0x25240:       32448:       119680 : aten_clamp_default_18
        71 -         73:    0x17c40 -    0x1d380:       22336:       214048 : const_values
        72 -         79:    0x15200 -    0x17c40:       10816:       214048 : aten_clamp_default_19
        73 -         75:    0x1d380 -    0x398a0:      116000:       214048 : const_values
        74 -         77:    0x398a0 -    0x441a0:       43264:       170080 : aten_clamp_default_20
        76 -         83:        0x0 -    0x15200:       86528:       245264 : aten_cat_default_6
        77 -         79:    0x17c40 -    0x1c4c0:       18560:       159168 : const_values
        78 -         81:    0x1f0e0 -    0x299e0:       43264:       159648 : aten_clamp_default_21
        81 -         83:    0x17c40 -    0x1f0e0:       29856:       245264 : const_values
        82 -         89:    0x15200 -    0x17c40:       10816:       245264 : aten_clamp_default_22
        83 -         85:    0x1f0e0 -    0x3be10:      118064:       245264 : const_values
        84 -         87:        0x0 -     0xa900:       43264:       172144 : aten_clamp_default_23
        86 -         93:    0x17c40 -    0x2ce40:       86528:       159488 : aten_cat_default_7
        87 -         89:     0xa900 -     0xf2c0:       18880:       159488 : const_values
        88 -         91:        0x0 -     0xa900:       43264:       159488 : aten_clamp_default_24
        91 -         93:     0xa900 -     0xb2e0:        2528:       132320 : const_values
        92 -         95:        0x0 -      0xa90:        2704:        91760 : aten_clamp_default_25
        94 -         97:      0xa90 -      0xaa0:          16:         2720 : aten_view_copy_default
Allocation Peak Tensor Size: 983312 bytes == 960.265625 KiB
################################################################################
Tensor Allocation for read-only NPU tensors:
Start Time - End Time  : Start Addr -   End Addr: Tensor Size: Memory Usage : Name
         4 -          5:        0x0 -      0xcc0:        3264:         3264 : const_values
         8 -          9:      0xcc0 -     0x1150:        1168:         1168 : const_values
        10 -         11:     0x1150 -     0x3800:        9904:         9904 : const_values
        14 -         15:     0x3800 -     0x4070:        2160:         2160 : const_values
        18 -         19:     0x4070 -     0x4870:        2048:         2048 : const_values
        20 -         21:     0x4870 -     0x70a0:       10288:        10288 : const_values
        24 -         25:     0x70a0 -     0x7920:        2176:         2176 : const_values
        30 -         31:     0x7920 -     0x88c0:        4000:         4000 : const_values
        32 -         33:     0x88c0 -    0x120b0:       38896:        38896 : const_values
        36 -         37:    0x120b0 -    0x13d10:        7264:         7264 : const_values
        40 -         41:    0x13d10 -    0x15a90:        7552:         7552 : const_values
        42 -         43:    0x15a90 -    0x1f2f0:       39008:        39008 : const_values
        46 -         47:    0x1f2f0 -    0x20f50:        7264:         7264 : const_values
        52 -         53:    0x20f50 -    0x23bd0:       11392:        11392 : const_values
        54 -         55:    0x23bd0 -    0x38d40:       86384:        86384 : const_values
        58 -         59:    0x38d40 -    0x3bd80:       12352:        12352 : const_values
        62 -         63:    0x3bd80 -    0x3fe70:       16624:        16624 : const_values
        64 -         65:    0x3fe70 -    0x553b0:       87360:        87360 : const_values
        68 -         69:    0x553b0 -    0x58410:       12384:        12384 : const_values
        72 -         73:    0x58410 -    0x5db50:       22336:        22336 : const_values
        74 -         75:    0x5db50 -    0x837a0:      154704:       154704 : const_values
        78 -         79:    0x837a0 -    0x88020:       18560:        18560 : const_values
        82 -         83:    0x88020 -    0x8f4c0:       29856:        29856 : const_values
        84 -         85:    0x8f4c0 -    0xb5b00:      157248:       157248 : const_values
        88 -         89:    0xb5b00 -    0xba4c0:       18880:        18880 : const_values
        92 -         93:    0xba4c0 -    0xbaea0:        2528:         2528 : const_values
Allocation Peak Tensor Size: 765600 bytes == 747.65625 KiB
High level NPU operations:
0 Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[16, 2, 128], IFM Block=[1, 16, 2, 128], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: quantized_decomposed_quantize_per_tensor_default, [1, 3, 224, 224], format: 1, Sram:FeatureMap|Staging, address: 0xc4000
  OFM: tosa_transpose_default, [1, 224, 224, 3], format: 2, Sram:FeatureMap|Staging, address: 0x0
1 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 10, 6, 16], IFM Block=[1, 10, 6, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: tosa_transpose_default, [1, 224, 224, 3], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_slice_copy_tensor, [1, 223, 224, 3], format: 1, Sram:FeatureMap|Staging, address: 0xc4000
2 Conv2D , subOps: Clamp, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 16, 32], IFM Block=[1, 9, 36, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_slice_copy_tensor, [1, 223, 224, 3], format: 1, Sram:FeatureMap|Staging, address: 0xc4000
  OFM: aten_clamp_default, [1, 111, 111, 64], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xe8960, format: Default
3 MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default, [1, 111, 111, 64], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_max_pool2d_default, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0xc0840
4 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_max_pool2d_default, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0xc0840
  OFM: aten_clamp_default_1, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xefc80, format: Default
5 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_1, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_2, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xbd10, format: Default
6 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_2, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_cat_default, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
7 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_1, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_3, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x999d0, format: Default
8 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_3, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_cat_default, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
9 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
  OFM: aten_clamp_default_4, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x999d0, format: Default
10 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_4, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_5, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x9a1d0, format: Default
11 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_5, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_cat_default_1, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
12 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_4, [1, 55, 55, 16], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_6, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x999d0, format: Default
13 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_6, [1, 55, 55, 64], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_cat_default_1, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
14 MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_1, [1, 55, 55, 128], format: 2, Sram:FeatureMap|Staging, address: 0xbd10
  OFM: aten_max_pool2d_default_1, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
15 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_max_pool2d_default_1, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x6a590
  OFM: aten_clamp_default_7, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0xfa0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x0, format: Default
16 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_7, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0xfa0
  OFM: aten_clamp_default_8, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x343c0
  Weights: const_values, 2 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x6ac0, format: Default
17 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_8, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x343c0
  OFM: aten_cat_default_2, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x6ac0
18 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_7, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0xfa0
  OFM: aten_clamp_default_9, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x343c0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x4b040, format: Default
19 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_9, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x343c0
  OFM: aten_cat_default_2, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x6ac0
20 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_2, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x6ac0
  OFM: aten_clamp_default_10, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x4b040, format: Default
21 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_10, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_11, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x4cdc0, format: Default
22 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_11, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  OFM: aten_cat_default_3, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x5b20
23 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_10, [1, 27, 27, 32], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_12, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x4a0a0, format: Default
24 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_12, [1, 27, 27, 128], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  OFM: aten_cat_default_3, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x5b20
25 MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[13, 14, 16], IFM Block=[1, 28, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_3, [1, 27, 27, 256], format: 2, Sram:FeatureMap|Staging, address: 0x5b20
  OFM: aten_max_pool2d_default_2, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x33420
26 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_max_pool2d_default_2, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x33420
  OFM: aten_clamp_default_13, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  Weights: const_values, 2 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x0, format: Default
27 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_13, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  OFM: aten_clamp_default_14, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x11d30
  Weights: const_values, 5 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x2c80, format: Default
28 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_14, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x11d30
  OFM: aten_cat_default_4, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
29 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_13, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  OFM: aten_clamp_default_15, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x15e20
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x1dce0, format: Default
30 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_15, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x15e20
  OFM: aten_cat_default_4, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
31 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_4, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_16, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x11d30, format: Default
32 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_16, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  OFM: aten_clamp_default_17, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x25e60
  Weights: const_values, 3 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x15e20, format: Default
33 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_17, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x25e60
  OFM: aten_cat_default_5, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
34 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_16, [1, 13, 13, 48], format: 2, Sram:FeatureMap|Staging, address: 0xfd80
  OFM: aten_clamp_default_18, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x1d380
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x11d30, format: Default
35 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_18, [1, 13, 13, 192], format: 2, Sram:FeatureMap|Staging, address: 0x1d380
  OFM: aten_cat_default_5, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
36 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_5, [1, 13, 13, 384], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_19, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x17c40, format: Default
37 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_19, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  OFM: aten_clamp_default_20, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x398a0
  Weights: const_values, 4 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x1d380, format: Default
38 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_20, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x398a0
  OFM: aten_cat_default_6, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x0
39 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_19, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  OFM: aten_clamp_default_21, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x1f0e0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x17c40, format: Default
40 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_21, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x1f0e0
  OFM: aten_cat_default_6, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x0
41 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_6, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_clamp_default_22, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0x17c40, format: Default
42 Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
  IFM: aten_clamp_default_22, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  OFM: aten_clamp_default_23, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 3 ranges, buffering: 2, Sram:FeatureMap|Staging, address: 0x1f0e0, format: Default
43 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_23, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_cat_default_7, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x17c40
44 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_22, [1, 13, 13, 64], format: 2, Sram:FeatureMap|Staging, address: 0x15200
  OFM: aten_clamp_default_24, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xa900, format: Default
45 MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_24, [1, 13, 13, 256], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_cat_default_7, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x17c40
46 Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_cat_default_7, [1, 13, 13, 512], format: 2, Sram:FeatureMap|Staging, address: 0x17c40
  OFM: aten_clamp_default_25, [1, 13, 13, 4], format: 2, Sram:FeatureMap|Staging, address: 0x0
  Weights: const_values, 1 ranges, buffering: 1, Sram:FeatureMap|Staging, address: 0xa900, format: Default
47 AvgPool , subOps: -, size=13,13 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 16], IFM Block=[1, 8, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
  IFM: aten_clamp_default_25, [1, 13, 13, 4], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: aten_view_copy_default, [1, 1, 1, 4], format: 1, Sram:FeatureMap|Staging, address: 0xa90
High level command stream:
0 Transpose OFM area [0, 0, 0, 0 - 1, 224, 224, 3], IFM [0, 0, 0, 0 - 1, 3, 224, 224]
1 MemoryCopy OFM area [0, 0, 0, 0 - 1, 223, 224, 3], IFM [0, 0, 0, 0 - 1, 223, 224, 3]
2 DMA src: OffChipFlash:ReadOnly, address: 0x0, dest: Sram:FeatureMap|Staging, address: 0xe8960, sizes: (N/A), length: 3264
3 Conv2D OFM area [0, 0, 0, 0 - 1, 111, 111, 64], IFM [0, 0, 0, 0 - 1, 223, 224, 3], Weight depth: 0
4 MaxPool OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 111, 111, 64]
5 DMA src: OffChipFlash:ReadOnly, address: 0xcc0, dest: Sram:FeatureMap|Staging, address: 0xefc80, sizes: (N/A), length: 1168
6 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 16], IFM [0, 0, 0, 0 - 1, 55, 55, 64], Weight depth: 0
7 DMA src: OffChipFlash:ReadOnly, address: 0x1150, dest: Sram:FeatureMap|Staging, address: 0xbd10, sizes: (N/A), length: 9904
8 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 16], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1]
9 MemoryCopy OFM area [0, 0, 0, 64 - 1, 55, 55, 128], IFM [0, 0, 0, 0 - 1, 55, 55, 64], buffered
10 DMA src: OffChipFlash:ReadOnly, address: 0x3800, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2160
11 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 16], Weight depth: 0
12 MemoryCopy OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 64], buffered
13 DMA src: OffChipFlash:ReadOnly, address: 0x4070, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2048
14 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 16], IFM [0, 0, 0, 0 - 1, 55, 55, 128], Weight depth: 0
15 DMA src: OffChipFlash:ReadOnly, address: 0x4870, dest: Sram:FeatureMap|Staging, address: 0x9a1d0, sizes: (N/A), length: 10288
16 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 16], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1]
17 MemoryCopy OFM area [0, 0, 0, 64 - 1, 55, 55, 128], IFM [0, 0, 0, 0 - 1, 55, 55, 64], buffered
18 DMA src: OffChipFlash:ReadOnly, address: 0x70a0, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2176
19 Conv2D OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 16], Weight depth: 0
20 MemoryCopy OFM area [0, 0, 0, 0 - 1, 55, 55, 64], IFM [0, 0, 0, 0 - 1, 55, 55, 64], buffered
21 MaxPool OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 55, 55, 128]
22 DMA src: OffChipFlash:ReadOnly, address: 0x7920, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 4000
23 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 32], IFM [0, 0, 0, 0 - 1, 27, 27, 128], Weight depth: 0
24 DMA src: OffChipFlash:ReadOnly, address: 0x88c0, dest: Sram:FeatureMap|Staging, address: 0x6ac0, sizes: (N/A), length: 19296
25 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 64], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
26 DMA src: OffChipFlash:ReadOnly, address: 0xd420, dest: Sram:FeatureMap|Staging, address: 0xb620, sizes: (N/A), length: 19600
27 Conv2D OFM area [0, 0, 0, 64 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 64, padding: [top:1,left:1,bottom:1,right:1], buffered
28 MemoryCopy OFM area [0, 0, 0, 128 - 1, 27, 27, 256], IFM [0, 0, 0, 0 - 1, 27, 27, 128], buffered
29 DMA src: OffChipFlash:ReadOnly, address: 0x120b0, dest: Sram:FeatureMap|Staging, address: 0x4b040, sizes: (N/A), length: 7264
30 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 0
31 MemoryCopy OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 128], buffered
32 DMA src: OffChipFlash:ReadOnly, address: 0x13d10, dest: Sram:FeatureMap|Staging, address: 0x4b040, sizes: (N/A), length: 7552
33 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 32], IFM [0, 0, 0, 0 - 1, 27, 27, 256], Weight depth: 0
34 DMA src: OffChipFlash:ReadOnly, address: 0x15a90, dest: Sram:FeatureMap|Staging, address: 0x4cdc0, sizes: (N/A), length: 39008
35 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1]
36 MemoryCopy OFM area [0, 0, 0, 128 - 1, 27, 27, 256], IFM [0, 0, 0, 0 - 1, 27, 27, 128], buffered
37 DMA src: OffChipFlash:ReadOnly, address: 0x1f2f0, dest: Sram:FeatureMap|Staging, address: 0x4a0a0, sizes: (N/A), length: 7264
38 Conv2D OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 32], Weight depth: 0
39 MemoryCopy OFM area [0, 0, 0, 0 - 1, 27, 27, 128], IFM [0, 0, 0, 0 - 1, 27, 27, 128], buffered
40 MaxPool OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 27, 27, 256]
41 DMA src: OffChipFlash:ReadOnly, address: 0x20f50, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 7568
42 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 32], IFM [0, 0, 0, 0 - 1, 13, 13, 256], Weight depth: 0, buffered
43 DMA src: OffChipFlash:ReadOnly, address: 0x22ce0, dest: Sram:FeatureMap|Staging, address: 0x1d90, sizes: (N/A), length: 3824
44 Conv2D OFM area [0, 0, 0, 32 - 1, 13, 13, 48], IFM [0, 0, 0, 0 - 1, 13, 13, 256], Weight depth: 32, buffered
45 DMA src: OffChipFlash:ReadOnly, address: 0x23bd0, dest: Sram:FeatureMap|Staging, address: 0x2c80, sizes: (N/A), length: 7248
46 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 16], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
47 DMA src: OffChipFlash:ReadOnly, address: 0x25820, dest: Sram:FeatureMap|Staging, address: 0x8160, sizes: (N/A), length: 21472
48 Conv2D OFM area [0, 0, 0, 16 - 1, 13, 13, 64], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 16, padding: [top:1,left:1,bottom:1,right:1], buffered
49 DMA src: OffChipFlash:ReadOnly, address: 0x2ac00, dest: Sram:FeatureMap|Staging, address: 0x2c80, sizes: (N/A), length: 21728
50 Conv2D OFM area [0, 0, 0, 64 - 1, 13, 13, 112], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 64, padding: [top:1,left:1,bottom:1,right:1], buffered
51 DMA src: OffChipFlash:ReadOnly, address: 0x300e0, dest: Sram:FeatureMap|Staging, address: 0x8160, sizes: (N/A), length: 21584
52 Conv2D OFM area [0, 0, 0, 112 - 1, 13, 13, 160], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 112, padding: [top:1,left:1,bottom:1,right:1], buffered
53 DMA src: OffChipFlash:ReadOnly, address: 0x35530, dest: Sram:FeatureMap|Staging, address: 0x2c80, sizes: (N/A), length: 14352
54 Conv2D OFM area [0, 0, 0, 160 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 160, padding: [top:1,left:1,bottom:1,right:1], buffered
55 MemoryCopy OFM area [0, 0, 0, 192 - 1, 13, 13, 384], IFM [0, 0, 0, 0 - 1, 13, 13, 192], buffered
56 DMA src: OffChipFlash:ReadOnly, address: 0x38d40, dest: Sram:FeatureMap|Staging, address: 0x1dce0, sizes: (N/A), length: 12352
57 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 0
58 MemoryCopy OFM area [0, 0, 0, 0 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 192], buffered
59 DMA src: OffChipFlash:ReadOnly, address: 0x3bd80, dest: Sram:FeatureMap|Staging, address: 0x11d30, sizes: (N/A), length: 16624
60 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 48], IFM [0, 0, 0, 0 - 1, 13, 13, 384], Weight depth: 0
61 DMA src: OffChipFlash:ReadOnly, address: 0x3fe70, dest: Sram:FeatureMap|Staging, address: 0x15e20, sizes: (N/A), length: 21952
62 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 48], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
63 DMA src: OffChipFlash:ReadOnly, address: 0x45430, dest: Sram:FeatureMap|Staging, address: 0x1b3e0, sizes: (N/A), length: 43648
64 Conv2D OFM area [0, 0, 0, 48 - 1, 13, 13, 144], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 48, padding: [top:1,left:1,bottom:1,right:1], buffered
65 DMA src: OffChipFlash:ReadOnly, address: 0x4feb0, dest: Sram:FeatureMap|Staging, address: 0x15e20, sizes: (N/A), length: 21760
66 Conv2D OFM area [0, 0, 0, 144 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 144, padding: [top:1,left:1,bottom:1,right:1], buffered
67 MemoryCopy OFM area [0, 0, 0, 192 - 1, 13, 13, 384], IFM [0, 0, 0, 0 - 1, 13, 13, 192], buffered
68 DMA src: OffChipFlash:ReadOnly, address: 0x553b0, dest: Sram:FeatureMap|Staging, address: 0x11d30, sizes: (N/A), length: 12384
69 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 48], Weight depth: 0
70 MemoryCopy OFM area [0, 0, 0, 0 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 192], buffered
71 DMA src: OffChipFlash:ReadOnly, address: 0x58410, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 22336
72 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 64], IFM [0, 0, 0, 0 - 1, 13, 13, 384], Weight depth: 0
73 DMA src: OffChipFlash:ReadOnly, address: 0x5db50, dest: Sram:FeatureMap|Staging, address: 0x1d380, sizes: (N/A), length: 28928
74 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 48], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
75 DMA src: OffChipFlash:ReadOnly, address: 0x64c50, dest: Sram:FeatureMap|Staging, address: 0x2b6e0, sizes: (N/A), length: 57792
76 Conv2D OFM area [0, 0, 0, 48 - 1, 13, 13, 144], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 48, padding: [top:1,left:1,bottom:1,right:1], buffered
77 DMA src: OffChipFlash:ReadOnly, address: 0x72e10, dest: Sram:FeatureMap|Staging, address: 0x1d380, sizes: (N/A), length: 58208
78 Conv2D OFM area [0, 0, 0, 144 - 1, 13, 13, 240], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 144, padding: [top:1,left:1,bottom:1,right:1], buffered
79 DMA src: OffChipFlash:ReadOnly, address: 0x81170, dest: Sram:FeatureMap|Staging, address: 0x2b6e0, sizes: (N/A), length: 9776
80 Conv2D OFM area [0, 0, 0, 240 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 240, padding: [top:1,left:1,bottom:1,right:1], buffered
81 MemoryCopy OFM area [0, 0, 0, 256 - 1, 13, 13, 512], IFM [0, 0, 0, 0 - 1, 13, 13, 256], buffered
82 DMA src: OffChipFlash:ReadOnly, address: 0x837a0, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 18560
83 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 0
84 MemoryCopy OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 256], buffered
85 DMA src: OffChipFlash:ReadOnly, address: 0x88020, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 29856
86 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 64], IFM [0, 0, 0, 0 - 1, 13, 13, 512], Weight depth: 0
87 DMA src: OffChipFlash:ReadOnly, address: 0x8f4c0, dest: Sram:FeatureMap|Staging, address: 0x1f0e0, sizes: (N/A), length: 39184
88 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 64], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 0, padding: [top:1,left:1,bottom:1,right:1], buffered
89 DMA src: OffChipFlash:ReadOnly, address: 0x98dd0, dest: Sram:FeatureMap|Staging, address: 0x28a80, sizes: (N/A), length: 78736
90 Conv2D OFM area [0, 0, 0, 64 - 1, 13, 13, 192], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 64, padding: [top:1,left:1,bottom:1,right:1], buffered
91 DMA src: OffChipFlash:ReadOnly, address: 0xac160, dest: Sram:FeatureMap|Staging, address: 0x1f0e0, sizes: (N/A), length: 39328
92 Conv2D OFM area [0, 0, 0, 192 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 192, padding: [top:1,left:1,bottom:1,right:1], buffered
93 MemoryCopy OFM area [0, 0, 0, 256 - 1, 13, 13, 512], IFM [0, 0, 0, 0 - 1, 13, 13, 256], buffered
94 DMA src: OffChipFlash:ReadOnly, address: 0xb5b00, dest: Sram:FeatureMap|Staging, address: 0xa900, sizes: (N/A), length: 18880
95 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 64], Weight depth: 0
96 MemoryCopy OFM area [0, 0, 0, 0 - 1, 13, 13, 256], IFM [0, 0, 0, 0 - 1, 13, 13, 256], buffered
97 DMA src: OffChipFlash:ReadOnly, address: 0xba4c0, dest: Sram:FeatureMap|Staging, address: 0xa900, sizes: (N/A), length: 2528
98 Conv2D OFM area [0, 0, 0, 0 - 1, 13, 13, 4], IFM [0, 0, 0, 0 - 1, 13, 13, 512], Weight depth: 0
99 AvgPool OFM area [0, 0, 0, 0 - 1, 1, 1, 4], IFM [0, 0, 0, 0 - 1, 13, 13, 4]
Register command stream: 1862 words
  Offset: Payload Param Code - Command                        Param, Fields
// Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[16, 2, 128], IFM Block=[1, 16, 2, 128], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000000:          0001 0105 - NPU_SET_IFM_PRECISION              1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000004:          0001 010f - NPU_SET_IFM_REGION                 1, region = 1
0x000008: 000c4000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xc4000
0x000010: 00000000 0000 4001 - NPU_SET_IFM_BASE1                  0, addr = 0x0
0x000018: 00000000 0000 4002 - NPU_SET_IFM_BASE2                  0, addr = 0x0
0x000020: 00000000 0000 4003 - NPU_SET_IFM_BASE3                  0, addr = 0x0
0x000028:          0002 010b - NPU_SET_IFM_HEIGHT0_M1             2, height_m1 = 2
0x00002c:          0002 010c - NPU_SET_IFM_HEIGHT1_M1             2, height_m1 = 2
0x000030:          00df 010a - NPU_SET_IFM_WIDTH0_M1            223, width_m1 = 223
0x000034:          00df 0104 - NPU_SET_IFM_DEPTH_M1             223, depth_m1 = 223
0x000038: 0000c400 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xc400
0x000040: 000000e0 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0xe0
0x000048: 00000001 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1
0x000050:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000054:          0000 0107 - NPU_SET_IFM_UPSCALE                0, mode = IFM_UPSCALE_MODE_NONE
0x000058:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x00005c:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x000060:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x000064:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000068:          1941 0114 - NPU_SET_OFM_PRECISION           6465, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_WCH, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00006c:          0001 011f - NPU_SET_OFM_REGION                 1, region = 1
0x000070: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000078: 00000000 0000 4011 - NPU_SET_OFM_BASE1                  0, addr = 0x0
0x000080: 00000000 0000 4012 - NPU_SET_OFM_BASE2                  0, addr = 0x0
0x000088: 00000000 0000 4013 - NPU_SET_OFM_BASE3                  0, addr = 0x0
0x000090:          0002 0112 - NPU_SET_OFM_HEIGHT_M1              2, height_m1 = 2
0x000094:          00df 0111 - NPU_SET_OFM_WIDTH_M1             223, width_m1 = 223
0x000098:          00df 0113 - NPU_SET_OFM_DEPTH_M1             223, depth_m1 = 223
0x00009c:          00df 011b - NPU_SET_OFM_HEIGHT0_M1           223, height_m1 = 223
0x0000a0:          00df 011c - NPU_SET_OFM_HEIGHT1_M1           223, height_m1 = 223
0x0000a4:          00df 011a - NPU_SET_OFM_WIDTH0_M1            223, width_m1 = 223
0x0000a8: 00000e00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xe00
0x0000b0: 00000010 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x10
0x0000b8: 00000e00 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0xe00
0x0000c0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0000c4:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x0000c8:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x0000cc:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0000d0:          0000 0125 - NPU_SET_ACTIVATION                 0, activation_function = ACTIVATION_FUNCTION_LUT_NONE, table = 0, activation_clip_range = ACTIVATION_CLIP_RANGE_B16
0x0000d4:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0000d8:          007f 0127 - NPU_SET_ACTIVATION_MAX           127, clip_boundary = 127
0x0000dc: 00000001 0000 4024 - NPU_SET_OFM_SCALE                  0, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_DOUBLE_SYMMETRIC, scale = 1
0x0000e4:          000f 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         15, height_m1 = 15
0x0000e8:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x0000ec:          007f 0117 - NPU_SET_OFM_BLK_DEPTH_M1         127, depth_m1 = 127
0x0000f0:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x0000f4:          0000 012f - NPU_SET_BLOCKDEP                   0, blockdep = 0
0x0000f8:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 10, 6, 16], IFM Block=[1, 10, 6, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0000fc:          0041 0105 - NPU_SET_IFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000100: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000108:          00de 010b - NPU_SET_IFM_HEIGHT0_M1           222, height_m1 = 222
0x00010c:          00de 010c - NPU_SET_IFM_HEIGHT1_M1           222, height_m1 = 222
0x000110:          0002 0104 - NPU_SET_IFM_DEPTH_M1               2, depth_m1 = 2
0x000114: 00000e00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xe00
0x00011c: 00000010 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x10
0x000124: 00000e00 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0xe00
0x00012c:          0101 0114 - NPU_SET_OFM_PRECISION            257, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000130: 000c4000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xc4000
0x000138:          00de 0112 - NPU_SET_OFM_HEIGHT_M1            222, height_m1 = 222
0x00013c:          0002 0113 - NPU_SET_OFM_DEPTH_M1               2, depth_m1 = 2
0x000140:          00de 011b - NPU_SET_OFM_HEIGHT0_M1           222, height_m1 = 222
0x000144:          00de 011c - NPU_SET_OFM_HEIGHT1_M1           222, height_m1 = 222
0x000148: 000002a0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x2a0
0x000150: 00000003 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x3
0x000158: 00000001 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x1
0x000160: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000168:          0009 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          9, height_m1 = 9
0x00016c:          0005 0115 - NPU_SET_OFM_BLK_WIDTH_M1           5, width_m1 = 5
0x000170:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000174:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x0, dest: Sram:FeatureMap|Staging, address: 0xe8960, sizes: (N/A), length: 3264
0x000178:          0000 0130 - NPU_SET_DMA0_SRC_REGION            0, region = 0, region_mode = DMA_REGION_MODE_EXTERNAL, stride_mode = DMA_STRIDE_MODE_D1, idx_mode = DMA_IDX_MODE_DISABLED
0x00017c: 00000000 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x0
0x000184:          0001 0131 - NPU_SET_DMA0_DST_REGION            1, region = 1, region_mode = DMA_REGION_MODE_EXTERNAL, idx_mode = DMA_IDX_MODE_DISABLED
0x000188: 000e8960 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xe8960
0x000190: 00000cc0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xcc0
0x000198:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x00019c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 16, 32], IFM Block=[1, 9, 36, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x0001a0: 67e4b55d 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1743041885
0x0001a8:          0001 0105 - NPU_SET_IFM_PRECISION              1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0001ac: 000c4000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xc4000
0x0001b4: 000002a0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x2a0
0x0001bc: 00000003 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x3
0x0001c4: 00000001 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1
0x0001cc:          ffff 0109 - NPU_SET_IFM_ZERO_POINT         65535, zero_point = 65535
0x0001d0:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0001d4: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x0001dc:          006e 0112 - NPU_SET_OFM_HEIGHT_M1            110, height_m1 = 110
0x0001e0:          006e 0111 - NPU_SET_OFM_WIDTH_M1             110, width_m1 = 110
0x0001e4:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x0001e8:          006e 011b - NPU_SET_OFM_HEIGHT0_M1           110, height_m1 = 110
0x0001ec:          006e 011c - NPU_SET_OFM_HEIGHT1_M1           110, height_m1 = 110
0x0001f0:          006e 011a - NPU_SET_OFM_WIDTH0_M1            110, width_m1 = 110
0x0001f4: 00001bc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1bc0
0x0001fc: 00000010 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x10
0x000204: 000006f0 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x6f0
0x00020c:          ffed 0118 - NPU_SET_OFM_ZERO_POINT         65517, zero_point = 65517
0x000210:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000214:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000218:          0007 0122 - NPU_SET_KERNEL_STRIDE              7, stride_x_lsb = 1, stride_y_lsb = 1, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x00021c:          0000 012e - NPU_SET_WEIGHT_FORMAT              0, weight_format = WEIGHT_FORMAT_SWD, weight_sparsity = WEIGHT_SPARSITY_NONE
0x000220:          0001 0128 - NPU_SET_WEIGHT_REGION              1, region = 1
0x000224: 000e8be0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xe8be0
0x00022c: 00000a40 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 2624
0x000234:          0001 0129 - NPU_SET_SCALE_REGION               1, region = 1
0x000238: 000e8960 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xe8960
0x000240: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x000248:          ffed 0126 - NPU_SET_ACTIVATION_MIN         65517, clip_boundary = 65517
0x00024c:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x000250:          000f 0115 - NPU_SET_OFM_BLK_WIDTH_M1          15, width_m1 = 15
0x000254:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000258:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x00025c:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x000260:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000264:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000268:          0041 0105 - NPU_SET_IFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00026c: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000274:          006e 010b - NPU_SET_IFM_HEIGHT0_M1           110, height_m1 = 110
0x000278:          006e 010c - NPU_SET_IFM_HEIGHT1_M1           110, height_m1 = 110
0x00027c:          006e 010a - NPU_SET_IFM_WIDTH0_M1            110, width_m1 = 110
0x000280:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x000284: 00001bc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1bc0
0x00028c: 00000010 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x10
0x000294: 000006f0 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x6f0
0x00029c:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0002a0:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0002a4: 000c0840 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xc0840
0x0002ac:          0036 0112 - NPU_SET_OFM_HEIGHT_M1             54, height_m1 = 54
0x0002b0:          0036 0111 - NPU_SET_OFM_WIDTH_M1              54, width_m1 = 54
0x0002b4:          0036 011b - NPU_SET_OFM_HEIGHT0_M1            54, height_m1 = 54
0x0002b8:          0036 011c - NPU_SET_OFM_HEIGHT1_M1            54, height_m1 = 54
0x0002bc:          0036 011a - NPU_SET_OFM_WIDTH0_M1             54, width_m1 = 54
0x0002c0: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x0002c8: 00000370 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x370
0x0002d0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0002d4:          0003 0122 - NPU_SET_KERNEL_STRIDE              3, stride_x_lsb = 1, stride_y_lsb = 1, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0002d8:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0002dc: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0002e4:          000d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         13, height_m1 = 13
0x0002e8:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x0002ec:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x0002f0:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x0002f4:          0000 0005 - NPU_OP_POOL                        0, pooling_mode = POOLING_MODE_MAX
// DMA src: OffChipFlash:ReadOnly, address: 0xcc0, dest: Sram:FeatureMap|Staging, address: 0xefc80, sizes: (N/A), length: 1168
0x0002f8: 00000cc0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xcc0
0x000300: 000efc80 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xefc80
0x000308: 00000490 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x490
0x000310:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x000314: 466b8965 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1181452645
0x00031c: 000c0840 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xc0840
0x000324:          0036 010b - NPU_SET_IFM_HEIGHT0_M1            54, height_m1 = 54
0x000328:          0036 010c - NPU_SET_IFM_HEIGHT1_M1            54, height_m1 = 54
0x00032c:          0036 010a - NPU_SET_IFM_WIDTH0_M1             54, width_m1 = 54
0x000330: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x000338: 00000370 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x370
0x000340:          ffed 0109 - NPU_SET_IFM_ZERO_POINT         65517, zero_point = 65517
0x000344:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000348: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000350:          000f 0113 - NPU_SET_OFM_DEPTH_M1              15, depth_m1 = 15
0x000354: 00000370 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x370
0x00035c:          0006 0118 - NPU_SET_OFM_ZERO_POINT             6, zero_point = 6
0x000360:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000364:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000368:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x00036c: 000efd20 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xefd20
0x000374: 000003f0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 1008
0x00037c: 000efc80 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xefc80
0x000384: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x00038c:          0006 0126 - NPU_SET_ACTIVATION_MIN             6, clip_boundary = 6
0x000390:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000394:          0037 0115 - NPU_SET_OFM_BLK_WIDTH_M1          55, width_m1 = 55
0x000398:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x00039c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0003a0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x1150, dest: Sram:FeatureMap|Staging, address: 0xbd10, sizes: (N/A), length: 9904
0x0003a4: 00001150 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x1150
0x0003ac: 0000bd10 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xbd10
0x0003b4: 000026b0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x26b0
0x0003bc:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0003c0:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x0003c4: 49b095bd 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1236309437
0x0003cc: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x0003d4:          000f 0104 - NPU_SET_IFM_DEPTH_M1              15, depth_m1 = 15
0x0003d8: 00000370 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x370
0x0003e0:          0006 0109 - NPU_SET_IFM_ZERO_POINT             6, zero_point = 6
0x0003e4:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x0003e8:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x0003ec:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x0003f0:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x0003f4: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x0003fc:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x000400: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x000408:          0029 0118 - NPU_SET_OFM_ZERO_POINT            41, zero_point = 41
0x00040c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000410:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000414:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000418: 0000bf90 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xbf90
0x000420: 00002430 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 9264
0x000428: 0000bd10 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xbd10
0x000430: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x000438:          0029 0126 - NPU_SET_ACTIVATION_MIN            41, clip_boundary = 41
0x00043c:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x000440:          000f 0115 - NPU_SET_OFM_BLK_WIDTH_M1          15, width_m1 = 15
0x000444:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000448:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x00044c: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x000454:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x000458: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x000460:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000464:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x000468:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x00046c:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x000470:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000474:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000478: 0000cad0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xcad0
0x000480: 00001b80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b80
0x000488:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x00048c:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000490:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000494:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000498:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x00049c: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0004a4:          002d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         45, height_m1 = 45
0x0004a8:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x0004ac:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x0004b0:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x3800, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2160
0x0004b4: 00003800 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x3800
0x0004bc: 000999d0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x999d0
0x0004c4: 00000870 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x870
0x0004cc:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x0004d0: 5bf3adb7 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1542696375
0x0004d8: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x0004e0:          000f 0104 - NPU_SET_IFM_DEPTH_M1              15, depth_m1 = 15
0x0004e4: 00000370 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x370
0x0004ec:          0006 0109 - NPU_SET_IFM_ZERO_POINT             6, zero_point = 6
0x0004f0:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0004f4: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x0004fc: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x000504:          0029 0118 - NPU_SET_OFM_ZERO_POINT            41, zero_point = 41
0x000508:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x00050c: 00099c50 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x99c50
0x000514: 000005f0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 1520
0x00051c: 000999d0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x999d0
0x000524:          0029 0126 - NPU_SET_ACTIVATION_MIN            41, clip_boundary = 41
0x000528:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x00052c:          0007 0115 - NPU_SET_OFM_BLK_WIDTH_M1           7, width_m1 = 7
0x000530:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000534:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000538:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x00053c:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000540: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x000548:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x00054c: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x000554:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000558:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00055c: 0000bd10 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xbd10
0x000564: 00001b80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b80
0x00056c:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000570:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000574:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000578: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000580:          002d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         45, height_m1 = 45
0x000584:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000588:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x00058c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000590:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x4070, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2048
0x000594: 00004070 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x4070
0x00059c: 00000800 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x800
0x0005a4:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0005a8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 56, 16], IFM Block=[1, 2, 56, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x0005ac: 5b6f07eb 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1534003179
0x0005b4: 0000bd10 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xbd10
0x0005bc:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x0005c0: 00001b80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1b80
0x0005c8:          0029 0109 - NPU_SET_IFM_ZERO_POINT            41, zero_point = 41
0x0005cc:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0005d0: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x0005d8:          000f 0113 - NPU_SET_OFM_DEPTH_M1              15, depth_m1 = 15
0x0005dc: 00000370 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x370
0x0005e4:          ffe8 0118 - NPU_SET_OFM_ZERO_POINT         65512, zero_point = 65512
0x0005e8: 00099a70 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x99a70
0x0005f0: 00000760 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 1888
0x0005f8: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x000600:          ffe8 0126 - NPU_SET_ACTIVATION_MIN         65512, clip_boundary = 65512
0x000604:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000608:          0037 0115 - NPU_SET_OFM_BLK_WIDTH_M1          55, width_m1 = 55
0x00060c:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000610:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000614:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x4870, dest: Sram:FeatureMap|Staging, address: 0x9a1d0, sizes: (N/A), length: 10288
0x000618: 00004870 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x4870
0x000620: 0009a1d0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x9a1d0
0x000628: 00002830 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x2830
0x000630:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 16, 16], IFM Block=[1, 10, 20, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000634: 5c432376 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1547903862
0x00063c: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000644:          000f 0104 - NPU_SET_IFM_DEPTH_M1              15, depth_m1 = 15
0x000648: 00000370 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x370
0x000650:          ffe8 0109 - NPU_SET_IFM_ZERO_POINT         65512, zero_point = 65512
0x000654:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x000658:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x00065c:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x000660:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x000664: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x00066c:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x000670: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x000678:          0013 0118 - NPU_SET_OFM_ZERO_POINT            19, zero_point = 19
0x00067c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000680:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000684:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000688: 0009a450 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x9a450
0x000690: 000025b0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 9648
0x000698: 0009a1d0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x9a1d0
0x0006a0: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x0006a8:          0013 0126 - NPU_SET_ACTIVATION_MIN            19, clip_boundary = 19
0x0006ac:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0006b0:          000f 0115 - NPU_SET_OFM_BLK_WIDTH_M1          15, width_m1 = 15
0x0006b4:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0006b8:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0006bc: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x0006c4:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x0006c8: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x0006d0:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0006d4:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x0006d8:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x0006dc:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x0006e0:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x0006e4:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0006e8: 0000cad0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xcad0
0x0006f0: 00001b80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b80
0x0006f8:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0006fc:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000700:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000704:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000708:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x00070c: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000714:          002d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         45, height_m1 = 45
0x000718:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x00071c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000720:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x70a0, dest: Sram:FeatureMap|Staging, address: 0x999d0, sizes: (N/A), length: 2176
0x000724: 000070a0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x70a0
0x00072c: 000999d0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x999d0
0x000734: 00000880 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x880
0x00073c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x000740:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000744: 793d55f0 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 2034062832
0x00074c: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000754:          000f 0104 - NPU_SET_IFM_DEPTH_M1              15, depth_m1 = 15
0x000758: 00000370 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x370
0x000760:          ffe8 0109 - NPU_SET_IFM_ZERO_POINT         65512, zero_point = 65512
0x000764:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000768: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x000770: 00000dc0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xdc0
0x000778:          0013 0118 - NPU_SET_OFM_ZERO_POINT            19, zero_point = 19
0x00077c:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000780: 00099c50 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x99c50
0x000788: 00000600 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 1536
0x000790: 000999d0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x999d0
0x000798:          0013 0126 - NPU_SET_ACTIVATION_MIN            19, clip_boundary = 19
0x00079c:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0007a0:          0007 0115 - NPU_SET_OFM_BLK_WIDTH_M1           7, width_m1 = 7
0x0007a4:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x0007a8:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x0007ac:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0007b0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 46, 2, 16], IFM Block=[1, 46, 2, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0007b4: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x0007bc:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x0007c0: 00000dc0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xdc0
0x0007c8:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0007cc:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0007d0: 0000bd10 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xbd10
0x0007d8: 00001b80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b80
0x0007e0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0007e4:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0007e8:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0007ec: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0007f4:          002d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         45, height_m1 = 45
0x0007f8:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x0007fc:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000800:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000804:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[14, 14, 16], IFM Block=[1, 30, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000808: 0000bd10 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xbd10
0x000810:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000814: 00001b80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1b80
0x00081c: 0006a590 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6a590
0x000824:          001a 0112 - NPU_SET_OFM_HEIGHT_M1             26, height_m1 = 26
0x000828:          001a 0111 - NPU_SET_OFM_WIDTH_M1              26, width_m1 = 26
0x00082c:          007f 0113 - NPU_SET_OFM_DEPTH_M1             127, depth_m1 = 127
0x000830:          001a 011b - NPU_SET_OFM_HEIGHT0_M1            26, height_m1 = 26
0x000834:          001a 011c - NPU_SET_OFM_HEIGHT1_M1            26, height_m1 = 26
0x000838:          001a 011a - NPU_SET_OFM_WIDTH0_M1             26, width_m1 = 26
0x00083c: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000844: 000001b0 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x1b0
0x00084c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000850:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000854:          0003 0122 - NPU_SET_KERNEL_STRIDE              3, stride_x_lsb = 1, stride_y_lsb = 1, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000858:          000d 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         13, height_m1 = 13
0x00085c:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x000860:          0000 0005 - NPU_OP_POOL                        0, pooling_mode = POOLING_MODE_MAX
// DMA src: OffChipFlash:ReadOnly, address: 0x7920, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 4000
0x000864: 00007920 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x7920
0x00086c: 00000000 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x0
0x000874: 00000fa0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xfa0
0x00087c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x000880: 51cd9c27 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1372429351
0x000888: 0006a590 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6a590
0x000890:          001a 010b - NPU_SET_IFM_HEIGHT0_M1            26, height_m1 = 26
0x000894:          001a 010c - NPU_SET_IFM_HEIGHT1_M1            26, height_m1 = 26
0x000898:          001a 010a - NPU_SET_IFM_WIDTH0_M1             26, width_m1 = 26
0x00089c: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x0008a4: 000001b0 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1b0
0x0008ac:          0013 0109 - NPU_SET_IFM_ZERO_POINT            19, zero_point = 19
0x0008b0:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0008b4: 00000fa0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xfa0
0x0008bc:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x0008c0: 00000360 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x360
0x0008c8:          ffdb 0118 - NPU_SET_OFM_ZERO_POINT         65499, zero_point = 65499
0x0008cc:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x0008d0:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x0008d4:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0008d8: 00000140 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x140
0x0008e0: 00000e60 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 3680
0x0008e8: 00000000 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x0
0x0008f0: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x0008f8:          ffdb 0126 - NPU_SET_ACTIVATION_MIN         65499, clip_boundary = 65499
0x0008fc:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000900:          001b 0115 - NPU_SET_OFM_BLK_WIDTH_M1          27, width_m1 = 27
0x000904:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000908:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x00090c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000910:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x88c0, dest: Sram:FeatureMap|Staging, address: 0x6ac0, sizes: (N/A), length: 19296
0x000914: 000088c0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x88c0
0x00091c: 00006ac0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x6ac0
0x000924: 00004b60 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x4b60
0x00092c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000930: 4bab2700 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1269507840
0x000938: 00000fa0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfa0
0x000940:          001f 0104 - NPU_SET_IFM_DEPTH_M1              31, depth_m1 = 31
0x000944: 00000360 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x360
0x00094c:          ffdb 0109 - NPU_SET_IFM_ZERO_POINT         65499, zero_point = 65499
0x000950:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x000954:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x000958:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x00095c:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x000960: 000343c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x343c0
0x000968:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x00096c: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000974:          0010 0118 - NPU_SET_OFM_ZERO_POINT            16, zero_point = 16
0x000978:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x00097c:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000980:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000984: 00006d40 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x6d40
0x00098c: 000048e0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 18656
0x000994: 00006ac0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x6ac0
0x00099c: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x0009a4:          0010 0126 - NPU_SET_ACTIVATION_MIN            16, clip_boundary = 16
0x0009a8:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x0009ac:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x0009b0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0009b4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0xd420, dest: Sram:FeatureMap|Staging, address: 0xb620, sizes: (N/A), length: 19600
0x0009b8: 0000d420 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xd420
0x0009c0: 0000b620 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xb620
0x0009c8: 00004c90 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x4c90
0x0009d0:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x0009d4: 00034a80 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x34a80
0x0009dc: 0000b8a0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xb8a0
0x0009e4: 00004a10 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 18960
0x0009ec: 0000b620 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xb620
0x0009f4:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0009f8:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0009fc: 000343c0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x343c0
0x000a04:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000a08: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x000a10:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000a14:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x000a18:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x000a1c:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x000a20:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000a24:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000a28: 00007840 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x7840
0x000a30:          007f 0113 - NPU_SET_OFM_DEPTH_M1             127, depth_m1 = 127
0x000a34: 00001b00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b00
0x000a3c:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000a40:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000a44:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000a48:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000a4c:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000a50: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000a58:          0012 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         18, height_m1 = 18
0x000a5c:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000a60:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000a64:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000a68:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x120b0, dest: Sram:FeatureMap|Staging, address: 0x4b040, sizes: (N/A), length: 7264
0x000a6c: 000120b0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x120b0
0x000a74: 0004b040 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x4b040
0x000a7c: 00001c60 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1c60
0x000a84:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000a88: 716ee9f9 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1903094265
0x000a90: 00000fa0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfa0
0x000a98:          001f 0104 - NPU_SET_IFM_DEPTH_M1              31, depth_m1 = 31
0x000a9c: 00000360 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x360
0x000aa4:          ffdb 0109 - NPU_SET_IFM_ZERO_POINT         65499, zero_point = 65499
0x000aa8:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000aac: 000343c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x343c0
0x000ab4: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000abc:          0010 0118 - NPU_SET_OFM_ZERO_POINT            16, zero_point = 16
0x000ac0:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000ac4: 0004b540 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x4b540
0x000acc: 00001760 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 5984
0x000ad4: 0004b040 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x4b040
0x000adc: 00000500 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1280
0x000ae4:          0010 0126 - NPU_SET_ACTIVATION_MIN            16, clip_boundary = 16
0x000ae8:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000aec:          001b 0115 - NPU_SET_OFM_BLK_WIDTH_M1          27, width_m1 = 27
0x000af0:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000af4:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000af8:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000afc: 000343c0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x343c0
0x000b04:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000b08: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x000b10:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000b14:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000b18: 00006ac0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x6ac0
0x000b20: 00001b00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b00
0x000b28:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000b2c:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000b30:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000b34: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000b3c:          0012 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         18, height_m1 = 18
0x000b40:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000b44:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000b48:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x13d10, dest: Sram:FeatureMap|Staging, address: 0x4b040, sizes: (N/A), length: 7552
0x000b4c: 00013d10 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x13d10
0x000b54: 00001d80 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1d80
0x000b5c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x000b60:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x000b64: 694c9b41 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1766628161
0x000b6c: 00006ac0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x6ac0
0x000b74:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x000b78: 00001b00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1b00
0x000b80:          0010 0109 - NPU_SET_IFM_ZERO_POINT            16, zero_point = 16
0x000b84:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000b88: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000b90:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x000b94: 00000360 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x360
0x000b9c:          ffe8 0118 - NPU_SET_OFM_ZERO_POINT         65512, zero_point = 65512
0x000ba0: 0004b180 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x4b180
0x000ba8: 00001c40 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 7232
0x000bb0: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x000bb8:          ffe8 0126 - NPU_SET_ACTIVATION_MIN         65512, clip_boundary = 65512
0x000bbc:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000bc0:          001b 0115 - NPU_SET_OFM_BLK_WIDTH_M1          27, width_m1 = 27
0x000bc4:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000bc8:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000bcc:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x15a90, dest: Sram:FeatureMap|Staging, address: 0x4cdc0, sizes: (N/A), length: 39008
0x000bd0: 00015a90 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x15a90
0x000bd8: 0004cdc0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x4cdc0
0x000be0: 00009860 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x9860
0x000be8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 4, 28, 16], IFM Block=[1, 6, 32, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000bec: 690307cd 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1761806285
0x000bf4: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000bfc:          001f 0104 - NPU_SET_IFM_DEPTH_M1              31, depth_m1 = 31
0x000c00: 00000360 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x360
0x000c08:          ffe8 0109 - NPU_SET_IFM_ZERO_POINT         65512, zero_point = 65512
0x000c0c:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x000c10:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x000c14:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x000c18:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x000c1c: 00033420 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x33420
0x000c24:          007f 0113 - NPU_SET_OFM_DEPTH_M1             127, depth_m1 = 127
0x000c28: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000c30:          001e 0118 - NPU_SET_OFM_ZERO_POINT            30, zero_point = 30
0x000c34:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000c38:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000c3c:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000c40: 0004d2c0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x4d2c0
0x000c48: 00009360 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 37728
0x000c50: 0004cdc0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x4cdc0
0x000c58: 00000500 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1280
0x000c60:          001e 0126 - NPU_SET_ACTIVATION_MIN            30, clip_boundary = 30
0x000c64:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x000c68:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000c6c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000c70:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000c74: 00033420 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x33420
0x000c7c:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000c80: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x000c88:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000c8c:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x000c90:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x000c94:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x000c98:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000c9c:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000ca0: 000068a0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x68a0
0x000ca8: 00001b00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b00
0x000cb0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000cb4:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000cb8:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000cbc:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000cc0:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000cc4: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000ccc:          0012 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         18, height_m1 = 18
0x000cd0:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000cd4:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x000cd8:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000cdc:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x1f2f0, dest: Sram:FeatureMap|Staging, address: 0x4a0a0, sizes: (N/A), length: 7264
0x000ce0: 0001f2f0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x1f2f0
0x000ce8: 0004a0a0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x4a0a0
0x000cf0: 00001c60 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1c60
0x000cf8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 28, 32], IFM Block=[1, 2, 28, 16], OFM UBlock=[1, 4, 8] Traversal=PartKernel, AccType=Acc32
0x000cfc: 4d93e286 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1301537414
0x000d04: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x000d0c:          001f 0104 - NPU_SET_IFM_DEPTH_M1              31, depth_m1 = 31
0x000d10: 00000360 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x360
0x000d18:          ffe8 0109 - NPU_SET_IFM_ZERO_POINT         65512, zero_point = 65512
0x000d1c:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000d20: 00033420 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x33420
0x000d28: 00000d80 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd80
0x000d30:          001e 0118 - NPU_SET_OFM_ZERO_POINT            30, zero_point = 30
0x000d34:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000d38: 0004a5a0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x4a5a0
0x000d40: 00001760 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 5984
0x000d48: 0004a0a0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x4a0a0
0x000d50:          001e 0126 - NPU_SET_ACTIVATION_MIN            30, clip_boundary = 30
0x000d54:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000d58:          001b 0115 - NPU_SET_OFM_BLK_WIDTH_M1          27, width_m1 = 27
0x000d5c:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000d60:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000d64:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 19, 2, 32], IFM Block=[1, 19, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000d68: 00033420 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x33420
0x000d70:          007f 0104 - NPU_SET_IFM_DEPTH_M1             127, depth_m1 = 127
0x000d74: 00000d80 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd80
0x000d7c:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x000d80:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000d84: 00005b20 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x5b20
0x000d8c: 00001b00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1b00
0x000d94:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x000d98:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000d9c:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000da0: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x000da8:          0012 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         18, height_m1 = 18
0x000dac:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x000db0:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x000db4:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// MaxPool , subOps: -, size=3,3 stride=2,2, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[13, 14, 16], IFM Block=[1, 28, 30, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x000db8: 00005b20 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x5b20
0x000dc0:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x000dc4: 00001b00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1b00
0x000dcc: 00033420 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x33420
0x000dd4:          000c 0112 - NPU_SET_OFM_HEIGHT_M1             12, height_m1 = 12
0x000dd8:          000c 0111 - NPU_SET_OFM_WIDTH_M1              12, width_m1 = 12
0x000ddc:          00ff 0113 - NPU_SET_OFM_DEPTH_M1             255, depth_m1 = 255
0x000de0:          000c 011b - NPU_SET_OFM_HEIGHT0_M1            12, height_m1 = 12
0x000de4:          000c 011c - NPU_SET_OFM_HEIGHT1_M1            12, height_m1 = 12
0x000de8:          000c 011a - NPU_SET_OFM_WIDTH0_M1             12, width_m1 = 12
0x000dec: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x000df4: 000000d0 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0xd0
0x000dfc:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000e00:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000e04:          0003 0122 - NPU_SET_KERNEL_STRIDE              3, stride_x_lsb = 1, stride_y_lsb = 1, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000e08:          000c 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         12, height_m1 = 12
0x000e0c:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x000e10:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000e14:          0003 012f - NPU_SET_BLOCKDEP                   3, blockdep = 3
0x000e18:          0000 0005 - NPU_OP_POOL                        0, pooling_mode = POOLING_MODE_MAX
// DMA src: OffChipFlash:ReadOnly, address: 0x20f50, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 7568
0x000e1c: 00020f50 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x20f50
0x000e24: 00000000 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x0
0x000e2c: 00001d90 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1d90
0x000e34:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x000e38: 4dd25e6f 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1305632367
0x000e40: 00033420 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x33420
0x000e48:          000c 010b - NPU_SET_IFM_HEIGHT0_M1            12, height_m1 = 12
0x000e4c:          000c 010c - NPU_SET_IFM_HEIGHT1_M1            12, height_m1 = 12
0x000e50:          000c 010a - NPU_SET_IFM_WIDTH0_M1             12, width_m1 = 12
0x000e54: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x000e5c: 000000d0 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0xd0
0x000e64:          001e 0109 - NPU_SET_IFM_ZERO_POINT            30, zero_point = 30
0x000e68:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000e6c: 0000fd80 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xfd80
0x000e74:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x000e78: 00000270 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x270
0x000e80:          fff2 0118 - NPU_SET_OFM_ZERO_POINT         65522, zero_point = 65522
0x000e84:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x000e88:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x000e8c:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000e90: 00000140 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x140
0x000e98: 00001c50 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 7248
0x000ea0: 00000000 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x0
0x000ea8: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x000eb0:          fff2 0126 - NPU_SET_ACTIVATION_MIN         65522, clip_boundary = 65522
0x000eb4:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x000eb8:          002f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          47, depth_m1 = 47
0x000ebc:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x000ec0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000ec4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x22ce0, dest: Sram:FeatureMap|Staging, address: 0x1d90, sizes: (N/A), length: 3824
0x000ec8: 00022ce0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x22ce0
0x000ed0: 00001d90 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1d90
0x000ed8: 00000ef0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xef0
0x000ee0:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x000ee4: 0000ff20 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xff20
0x000eec:          000f 0113 - NPU_SET_OFM_DEPTH_M1              15, depth_m1 = 15
0x000ef0: 00001e30 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1e30
0x000ef8: 00000e50 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 3664
0x000f00: 00001d90 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1d90
0x000f08: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x000f10:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x000f14:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000f18:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x23bd0, dest: Sram:FeatureMap|Staging, address: 0x2c80, sizes: (N/A), length: 7248
0x000f1c: 00023bd0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x23bd0
0x000f24: 00002c80 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2c80
0x000f2c: 00001c50 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1c50
0x000f34:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x000f38: 7289b6f6 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1921627894
0x000f40: 0000fd80 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfd80
0x000f48:          002f 0104 - NPU_SET_IFM_DEPTH_M1              47, depth_m1 = 47
0x000f4c: 00000270 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x270
0x000f54:          fff2 0109 - NPU_SET_IFM_ZERO_POINT         65522, zero_point = 65522
0x000f58:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x000f5c:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x000f60:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x000f64:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x000f68: 00011d30 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x11d30
0x000f70: 000009c0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x9c0
0x000f78:          0006 0118 - NPU_SET_OFM_ZERO_POINT             6, zero_point = 6
0x000f7c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x000f80:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x000f84:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x000f88: 00002d20 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2d20
0x000f90: 00001bb0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 7088
0x000f98: 00002c80 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2c80
0x000fa0:          0006 0126 - NPU_SET_ACTIVATION_MIN             6, clip_boundary = 6
0x000fa4:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x000fa8:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x000fac:          0004 012f - NPU_SET_BLOCKDEP                   4, blockdep = 4
0x000fb0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000fb4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x25820, dest: Sram:FeatureMap|Staging, address: 0x8160, sizes: (N/A), length: 21472
0x000fb8: 00025820 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x25820
0x000fc0: 00008160 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x8160
0x000fc8: 000053e0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x53e0
0x000fd0:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x000fd4: 00011e00 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x11e00
0x000fdc:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x000fe0: 00008340 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x8340
0x000fe8: 00005200 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 20992
0x000ff0: 00008160 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x8160
0x000ff8: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x001000:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x001004:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001008:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x2ac00, dest: Sram:FeatureMap|Staging, address: 0x2c80, sizes: (N/A), length: 21728
0x00100c: 0002ac00 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x2ac00
0x001014: 00002c80 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2c80
0x00101c: 000054e0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x54e0
0x001024:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001028:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x00102c: 00012070 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x12070
0x001034: 00002e60 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2e60
0x00103c: 00005300 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21248
0x001044: 00002c80 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2c80
0x00104c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001050:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x300e0, dest: Sram:FeatureMap|Staging, address: 0x8160, sizes: (N/A), length: 21584
0x001054: 000300e0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x300e0
0x00105c: 00008160 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x8160
0x001064: 00005450 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x5450
0x00106c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001070:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001074: 000122e0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x122e0
0x00107c: 00008340 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x8340
0x001084: 00005270 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21104
0x00108c: 00008160 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x8160
0x001094:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001098:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x35530, dest: Sram:FeatureMap|Staging, address: 0x2c80, sizes: (N/A), length: 14352
0x00109c: 00035530 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x35530
0x0010a4: 00002c80 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2c80
0x0010ac: 00003810 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x3810
0x0010b4:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0010b8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0010bc: 00012550 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x12550
0x0010c4:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x0010c8: 00002dc0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2dc0
0x0010d0: 000036d0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 14032
0x0010d8: 00002c80 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2c80
0x0010e0: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x0010e8:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0010ec:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0010f0: 00011d30 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x11d30
0x0010f8:          00bf 0104 - NPU_SET_IFM_DEPTH_M1             191, depth_m1 = 191
0x0010fc: 000009c0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x9c0
0x001104:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001108:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x00110c:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x001110:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x001114:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x001118:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00111c: 000009c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x9c0
0x001124:          00bf 0113 - NPU_SET_OFM_DEPTH_M1             191, depth_m1 = 191
0x001128: 00001380 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1380
0x001130:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001134:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x001138:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x00113c:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001140:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001144: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x00114c:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x001150:          0009 0115 - NPU_SET_OFM_BLK_WIDTH_M1           9, width_m1 = 9
0x001154:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001158:          0004 012f - NPU_SET_BLOCKDEP                   4, blockdep = 4
0x00115c:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x38d40, dest: Sram:FeatureMap|Staging, address: 0x1dce0, sizes: (N/A), length: 12352
0x001160: 00038d40 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x38d40
0x001168: 0001dce0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1dce0
0x001170: 00003040 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x3040
0x001178:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x00117c: 4ddfdd19 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1306516761
0x001184: 0000fd80 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfd80
0x00118c:          002f 0104 - NPU_SET_IFM_DEPTH_M1              47, depth_m1 = 47
0x001190: 00000270 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x270
0x001198:          fff2 0109 - NPU_SET_IFM_ZERO_POINT         65522, zero_point = 65522
0x00119c:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0011a0: 00015e20 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x15e20
0x0011a8: 000009c0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x9c0
0x0011b0:          0006 0118 - NPU_SET_OFM_ZERO_POINT             6, zero_point = 6
0x0011b4: 0001e460 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1e460
0x0011bc: 000028c0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 10432
0x0011c4: 0001dce0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1dce0
0x0011cc: 00000780 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1920
0x0011d4:          0006 0126 - NPU_SET_ACTIVATION_MIN             6, clip_boundary = 6
0x0011d8:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0011dc:          0007 0115 - NPU_SET_OFM_BLK_WIDTH_M1           7, width_m1 = 7
0x0011e0:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x0011e4:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x0011e8:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x0011ec:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0011f0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0011f4: 00015e20 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15e20
0x0011fc:          00bf 0104 - NPU_SET_IFM_DEPTH_M1             191, depth_m1 = 191
0x001200: 000009c0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x9c0
0x001208:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x00120c:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001210: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x001218: 00001380 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1380
0x001220:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001224:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001228: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x001230:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x001234:          0009 0115 - NPU_SET_OFM_BLK_WIDTH_M1           9, width_m1 = 9
0x001238:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x00123c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001240:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x3bd80, dest: Sram:FeatureMap|Staging, address: 0x11d30, sizes: (N/A), length: 16624
0x001244: 0003bd80 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x3bd80
0x00124c: 00011d30 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x11d30
0x001254: 000040f0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x40f0
0x00125c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 14, 48], IFM Block=[1, 2, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001260: 651db673 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1696446067
0x001268: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001270:          017f 0104 - NPU_SET_IFM_DEPTH_M1             383, depth_m1 = 383
0x001274: 00001380 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1380
0x00127c:          0006 0109 - NPU_SET_IFM_ZERO_POINT             6, zero_point = 6
0x001280:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001284: 0000fd80 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xfd80
0x00128c:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x001290: 00000270 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x270
0x001298:          ffec 0118 - NPU_SET_OFM_ZERO_POINT         65516, zero_point = 65516
0x00129c: 00011f10 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x11f10
0x0012a4: 00003f10 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 16144
0x0012ac: 00011d30 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x11d30
0x0012b4: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x0012bc:          ffec 0126 - NPU_SET_ACTIVATION_MIN         65516, clip_boundary = 65516
0x0012c0:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x0012c4:          002f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          47, depth_m1 = 47
0x0012c8:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x0012cc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0012d0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x3fe70, dest: Sram:FeatureMap|Staging, address: 0x15e20, sizes: (N/A), length: 21952
0x0012d4: 0003fe70 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x3fe70
0x0012dc: 00015e20 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x15e20
0x0012e4: 000055c0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x55c0
0x0012ec:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0012f0:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0012f4: 5b18b768 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1528346472
0x0012fc: 0000fd80 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfd80
0x001304:          002f 0104 - NPU_SET_IFM_DEPTH_M1              47, depth_m1 = 47
0x001308: 00000270 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x270
0x001310:          ffec 0109 - NPU_SET_IFM_ZERO_POINT         65516, zero_point = 65516
0x001314:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x001318:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x00131c:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x001320:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x001324: 00025e60 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x25e60
0x00132c: 000009c0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x9c0
0x001334:          0013 0118 - NPU_SET_OFM_ZERO_POINT            19, zero_point = 19
0x001338:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x00133c:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x001340:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001344: 00016000 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x16000
0x00134c: 000053e0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21472
0x001354: 00015e20 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x15e20
0x00135c:          0013 0126 - NPU_SET_ACTIVATION_MIN            19, clip_boundary = 19
0x001360:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001364:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001368:          0002 012f - NPU_SET_BLOCKDEP                   2, blockdep = 2
0x00136c:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001370:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x45430, dest: Sram:FeatureMap|Staging, address: 0x1b3e0, sizes: (N/A), length: 43648
0x001374: 00045430 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x45430
0x00137c: 0001b3e0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1b3e0
0x001384: 0000aa80 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xaa80
0x00138c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001390: 000260d0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x260d0
0x001398:          005f 0113 - NPU_SET_OFM_DEPTH_M1              95, depth_m1 = 95
0x00139c: 0001b7a0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1b7a0
0x0013a4: 0000a6c0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 42688
0x0013ac: 0001b3e0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1b3e0
0x0013b4: 000003c0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 960
0x0013bc:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x0013c0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0013c4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x4feb0, dest: Sram:FeatureMap|Staging, address: 0x15e20, sizes: (N/A), length: 21760
0x0013c8: 0004feb0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x4feb0
0x0013d0: 00015e20 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x15e20
0x0013d8: 00005500 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x5500
0x0013e0:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x0013e4:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0013e8: 000265b0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x265b0
0x0013f0:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x0013f4: 00016000 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x16000
0x0013fc: 00005320 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21280
0x001404: 00015e20 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x15e20
0x00140c: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x001414:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001418:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x00141c: 00025e60 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x25e60
0x001424:          00bf 0104 - NPU_SET_IFM_DEPTH_M1             191, depth_m1 = 191
0x001428: 000009c0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x9c0
0x001430:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001434:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x001438:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x00143c:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x001440:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x001444:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001448: 000009c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x9c0
0x001450:          00bf 0113 - NPU_SET_OFM_DEPTH_M1             191, depth_m1 = 191
0x001454: 00001380 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1380
0x00145c:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001460:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x001464:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x001468:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x00146c:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001470: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x001478:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x00147c:          0009 0115 - NPU_SET_OFM_BLK_WIDTH_M1           9, width_m1 = 9
0x001480:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001484:          0006 012f - NPU_SET_BLOCKDEP                   6, blockdep = 6
0x001488:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x553b0, dest: Sram:FeatureMap|Staging, address: 0x11d30, sizes: (N/A), length: 12384
0x00148c: 000553b0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x553b0
0x001494: 00011d30 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x11d30
0x00149c: 00003060 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x3060
0x0014a4:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 8, 32], IFM Block=[1, 8, 8, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x0014a8: 753b8825 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1966835749
0x0014b0: 0000fd80 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0xfd80
0x0014b8:          002f 0104 - NPU_SET_IFM_DEPTH_M1              47, depth_m1 = 47
0x0014bc: 00000270 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x270
0x0014c4:          ffec 0109 - NPU_SET_IFM_ZERO_POINT         65516, zero_point = 65516
0x0014c8:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0014cc: 0001d380 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x1d380
0x0014d4: 000009c0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x9c0
0x0014dc:          0013 0118 - NPU_SET_OFM_ZERO_POINT            19, zero_point = 19
0x0014e0: 000124b0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x124b0
0x0014e8: 000028e0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 10464
0x0014f0: 00011d30 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x11d30
0x0014f8: 00000780 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1920
0x001500:          0013 0126 - NPU_SET_ACTIVATION_MIN            19, clip_boundary = 19
0x001504:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001508:          0007 0115 - NPU_SET_OFM_BLK_WIDTH_M1           7, width_m1 = 7
0x00150c:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001510:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x001514:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x001518:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x00151c:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 10, 16], IFM Block=[1, 2, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001520: 0001d380 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x1d380
0x001528:          00bf 0104 - NPU_SET_IFM_DEPTH_M1             191, depth_m1 = 191
0x00152c: 000009c0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x9c0
0x001534:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001538:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00153c: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x001544: 00001380 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1380
0x00154c:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001550:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001554: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x00155c:          0001 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          1, height_m1 = 1
0x001560:          0009 0115 - NPU_SET_OFM_BLK_WIDTH_M1           9, width_m1 = 9
0x001564:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001568:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x00156c:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x58410, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 22336
0x001570: 00058410 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x58410
0x001578: 00017c40 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x17c40
0x001580: 00005740 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x5740
0x001588:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x00158c: 5a713a02 2027 4024 - NPU_SET_OFM_SCALE               8231, shift = 39, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1517369858
0x001594: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x00159c:          017f 0104 - NPU_SET_IFM_DEPTH_M1             383, depth_m1 = 383
0x0015a0: 00001380 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1380
0x0015a8:          0013 0109 - NPU_SET_IFM_ZERO_POINT            19, zero_point = 19
0x0015ac:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0015b0: 00015200 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x15200
0x0015b8:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x0015bc: 00000340 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x340
0x0015c4:          ffe6 0118 - NPU_SET_OFM_ZERO_POINT         65510, zero_point = 65510
0x0015c8: 00017ec0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x17ec0
0x0015d0: 000054c0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 21696
0x0015d8: 00017c40 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x17c40
0x0015e0: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x0015e8:          ffe6 0126 - NPU_SET_ACTIVATION_MIN         65510, clip_boundary = 65510
0x0015ec:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x0015f0:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x0015f4:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x0015f8:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x0015fc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001600:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x5db50, dest: Sram:FeatureMap|Staging, address: 0x1d380, sizes: (N/A), length: 28928
0x001604: 0005db50 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x5db50
0x00160c: 0001d380 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1d380
0x001614: 00007100 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x7100
0x00161c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001620:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001624: 557a9ae5 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1434098405
0x00162c: 00015200 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15200
0x001634:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x001638: 00000340 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x340
0x001640:          ffe6 0109 - NPU_SET_IFM_ZERO_POINT         65510, zero_point = 65510
0x001644:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x001648:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x00164c:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x001650:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x001654: 000398a0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x398a0
0x00165c:          002f 0113 - NPU_SET_OFM_DEPTH_M1              47, depth_m1 = 47
0x001660: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x001668:          000f 0118 - NPU_SET_OFM_ZERO_POINT            15, zero_point = 15
0x00166c:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x001670:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x001674:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001678: 0001d560 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1d560
0x001680: 00006f20 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 28448
0x001688: 0001d380 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1d380
0x001690: 000001e0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 480
0x001698:          000f 0126 - NPU_SET_ACTIVATION_MIN            15, clip_boundary = 15
0x00169c:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x0016a0:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x0016a4:          0003 012f - NPU_SET_BLOCKDEP                   3, blockdep = 3
0x0016a8:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0016ac:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x64c50, dest: Sram:FeatureMap|Staging, address: 0x2b6e0, sizes: (N/A), length: 57792
0x0016b0: 00064c50 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x64c50
0x0016b8: 0002b6e0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2b6e0
0x0016c0: 0000e1c0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xe1c0
0x0016c8:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x0016cc: 00039b10 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x39b10
0x0016d4:          005f 0113 - NPU_SET_OFM_DEPTH_M1              95, depth_m1 = 95
0x0016d8: 0002baa0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2baa0
0x0016e0: 0000de00 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 56832
0x0016e8: 0002b6e0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2b6e0
0x0016f0: 000003c0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 960
0x0016f8:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x0016fc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001700:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x72e10, dest: Sram:FeatureMap|Staging, address: 0x1d380, sizes: (N/A), length: 58208
0x001704: 00072e10 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x72e10
0x00170c: 0001d380 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1d380
0x001714: 0000e360 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0xe360
0x00171c:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001720:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001724: 00039ff0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x39ff0
0x00172c: 0001d740 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1d740
0x001734: 0000dfa0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 57248
0x00173c: 0001d380 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1d380
0x001744:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001748:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x81170, dest: Sram:FeatureMap|Staging, address: 0x2b6e0, sizes: (N/A), length: 9776
0x00174c: 00081170 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x81170
0x001754: 0002b6e0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x2b6e0
0x00175c: 00002630 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x2630
0x001764:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001768:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x00176c: 0003a4d0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x3a4d0
0x001774:          000f 0113 - NPU_SET_OFM_DEPTH_M1              15, depth_m1 = 15
0x001778: 0002b780 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x2b780
0x001780: 00002590 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 9616
0x001788: 0002b6e0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x2b6e0
0x001790: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x001798:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x00179c:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0017a0: 000398a0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x398a0
0x0017a8:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x0017ac: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x0017b4:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0017b8:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x0017bc:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x0017c0:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x0017c4:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x0017c8:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0017cc: 00000d00 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xd00
0x0017d4:          00ff 0113 - NPU_SET_OFM_DEPTH_M1             255, depth_m1 = 255
0x0017d8: 00001a00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1a00
0x0017e0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0017e4:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x0017e8:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x0017ec:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0017f0:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0017f4: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0017fc:          000a 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         10, height_m1 = 10
0x001800:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x001804:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001808:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x00180c:          0002 012f - NPU_SET_BLOCKDEP                   2, blockdep = 2
0x001810:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x837a0, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 18560
0x001814: 000837a0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x837a0
0x00181c: 00017c40 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x17c40
0x001824: 00004880 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x4880
0x00182c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001830: 6996753c 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1771468092
0x001838: 00015200 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15200
0x001840:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x001844: 00000340 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x340
0x00184c:          ffe6 0109 - NPU_SET_IFM_ZERO_POINT         65510, zero_point = 65510
0x001850:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001854: 0001f0e0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x1f0e0
0x00185c: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x001864:          000f 0118 - NPU_SET_OFM_ZERO_POINT            15, zero_point = 15
0x001868: 00018640 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x18640
0x001870: 00003e80 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 16000
0x001878: 00017c40 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x17c40
0x001880: 00000a00 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 2560
0x001888:          000f 0126 - NPU_SET_ACTIVATION_MIN            15, clip_boundary = 15
0x00188c:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001890:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x001894:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001898:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x00189c:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x0018a0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0018a4:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x0018a8: 0001f0e0 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x1f0e0
0x0018b0:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x0018b4: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x0018bc:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0018c0:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x0018c4: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x0018cc: 00001a00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1a00
0x0018d4:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x0018d8:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x0018dc: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x0018e4:          000a 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         10, height_m1 = 10
0x0018e8:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x0018ec:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x0018f0:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x0018f4:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0x88020, dest: Sram:FeatureMap|Staging, address: 0x17c40, sizes: (N/A), length: 29856
0x0018f8: 00088020 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x88020
0x001900: 000074a0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x74a0
0x001908:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x00190c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 4, 14, 32], IFM Block=[1, 4, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001910: 6a5d41f8 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1784496632
0x001918: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001920:          01ff 0104 - NPU_SET_IFM_DEPTH_M1             511, depth_m1 = 511
0x001924: 00001a00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1a00
0x00192c:          000f 0109 - NPU_SET_IFM_ZERO_POINT            15, zero_point = 15
0x001930:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001934: 00015200 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x15200
0x00193c:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x001940: 00000340 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x340
0x001948:          fff8 0118 - NPU_SET_OFM_ZERO_POINT         65528, zero_point = 65528
0x00194c: 00017ec0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x17ec0
0x001954: 00007220 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 29216
0x00195c: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x001964:          fff8 0126 - NPU_SET_ACTIVATION_MIN         65528, clip_boundary = 65528
0x001968:          0003 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          3, height_m1 = 3
0x00196c:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x001970:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x001974:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001978:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x8f4c0, dest: Sram:FeatureMap|Staging, address: 0x1f0e0, sizes: (N/A), length: 39184
0x00197c: 0008f4c0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x8f4c0
0x001984: 0001f0e0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1f0e0
0x00198c: 00009910 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x9910
0x001994:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001998:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x00199c: 53f918fc 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1408833788
0x0019a4: 00015200 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15200
0x0019ac:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x0019b0: 00000340 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x340
0x0019b8:          fff8 0109 - NPU_SET_IFM_ZERO_POINT         65528, zero_point = 65528
0x0019bc:          0001 0100 - NPU_SET_IFM_PAD_TOP                1, pad = 1
0x0019c0:          0001 0101 - NPU_SET_IFM_PAD_LEFT               1, pad = 1
0x0019c4:          0001 0103 - NPU_SET_IFM_PAD_BOTTOM             1, pad = 1
0x0019c8:          0001 0102 - NPU_SET_IFM_PAD_RIGHT              1, pad = 1
0x0019cc: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x0019d4: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x0019dc:          003b 0118 - NPU_SET_OFM_ZERO_POINT            59, zero_point = 59
0x0019e0:          0002 0121 - NPU_SET_KERNEL_HEIGHT_M1           2, height_m1 = 2
0x0019e4:          0002 0120 - NPU_SET_KERNEL_WIDTH_M1            2, width_m1 = 2
0x0019e8:          0004 0122 - NPU_SET_KERNEL_STRIDE              4, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_PART_KERNEL_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0019ec: 0001f360 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1f360
0x0019f4: 00009690 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 38544
0x0019fc: 0001f0e0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1f0e0
0x001a04:          003b 0126 - NPU_SET_ACTIVATION_MIN            59, clip_boundary = 59
0x001a08:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001a0c:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001a10:          0003 012f - NPU_SET_BLOCKDEP                   3, blockdep = 3
0x001a14:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001a18:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0x98dd0, dest: Sram:FeatureMap|Staging, address: 0x28a80, sizes: (N/A), length: 78736
0x001a1c: 00098dd0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x98dd0
0x001a24: 00028a80 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x28a80
0x001a2c: 00013390 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x13390
0x001a34:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001a38: 00000340 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x340
0x001a40:          007f 0113 - NPU_SET_OFM_DEPTH_M1             127, depth_m1 = 127
0x001a44: 00028f80 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x28f80
0x001a4c: 00012e90 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 77456
0x001a54: 00028a80 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x28a80
0x001a5c: 00000500 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 1280
0x001a64:          0007 012f - NPU_SET_BLOCKDEP                   7, blockdep = 7
0x001a68:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001a6c:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// DMA src: OffChipFlash:ReadOnly, address: 0xac160, dest: Sram:FeatureMap|Staging, address: 0x1f0e0, sizes: (N/A), length: 39328
0x001a70: 000ac160 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xac160
0x001a78: 0001f0e0 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x1f0e0
0x001a80: 000099a0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x99a0
0x001a88:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001a8c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=3,3 stride=1,1, dilation=1,1 padding=[t:1,l:1,b:1,r:1,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 10, 16, 16], OFM UBlock=[2, 2, 8] Traversal=PartKernel, AccType=Acc32
0x001a90: 000009c0 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x9c0
0x001a98:          003f 0113 - NPU_SET_OFM_DEPTH_M1              63, depth_m1 = 63
0x001a9c: 0001f360 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x1f360
0x001aa4: 00009720 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 38688
0x001aac: 0001f0e0 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x1f0e0
0x001ab4: 00000280 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 640
0x001abc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001ac0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001ac4: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001acc:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x001ad0: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x001ad8:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001adc:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x001ae0:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x001ae4:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x001ae8:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x001aec:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001af0: 00018940 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x18940
0x001af8:          00ff 0113 - NPU_SET_OFM_DEPTH_M1             255, depth_m1 = 255
0x001afc: 00001a00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1a00
0x001b04:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001b08:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x001b0c:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x001b10:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x001b14:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001b18: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x001b20:          000a 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         10, height_m1 = 10
0x001b24:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x001b28:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001b2c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001b30:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0xb5b00, dest: Sram:FeatureMap|Staging, address: 0xa900, sizes: (N/A), length: 18880
0x001b34: 000b5b00 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xb5b00
0x001b3c: 0000a900 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0xa900
0x001b44: 000049c0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x49c0
0x001b4c:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001b50: 66caa826 2028 4024 - NPU_SET_OFM_SCALE               8232, shift = 40, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1724557350
0x001b58: 00015200 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x15200
0x001b60:          003f 0104 - NPU_SET_IFM_DEPTH_M1              63, depth_m1 = 63
0x001b64: 00000340 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x340
0x001b6c:          fff8 0109 - NPU_SET_IFM_ZERO_POINT         65528, zero_point = 65528
0x001b70:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001b74: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x001b7c: 00000d00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd00
0x001b84:          003b 0118 - NPU_SET_OFM_ZERO_POINT            59, zero_point = 59
0x001b88: 0000b300 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xb300
0x001b90: 00003fc0 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 16320
0x001b98: 0000a900 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0xa900
0x001ba0: 00000a00 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 2560
0x001ba8:          003b 0126 - NPU_SET_ACTIVATION_MIN            59, clip_boundary = 59
0x001bac:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001bb0:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x001bb4:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001bb8:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x001bbc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001bc0:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// MemoryCopy , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 11, 2, 32], IFM Block=[1, 11, 2, 32], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001bc4: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001bcc:          00ff 0104 - NPU_SET_IFM_DEPTH_M1             255, depth_m1 = 255
0x001bd0: 00000d00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd00
0x001bd8:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x001bdc:          0141 0114 - NPU_SET_OFM_PRECISION            321, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001be0: 00017c40 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x17c40
0x001be8: 00001a00 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1a00
0x001bf0:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x001bf4:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001bf8: 00000001 2000 4024 - NPU_SET_OFM_SCALE               8192, shift = 0, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1
0x001c00:          000a 0116 - NPU_SET_OFM_BLK_HEIGHT_M1         10, height_m1 = 10
0x001c04:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x001c08:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x001c0c:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001c10:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
// DMA src: OffChipFlash:ReadOnly, address: 0xba4c0, dest: Sram:FeatureMap|Staging, address: 0xa900, sizes: (N/A), length: 2528
0x001c14: 000ba4c0 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0xba4c0
0x001c1c: 000009e0 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x9e0
0x001c24:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x001c28:          0000 0010 - NPU_OP_DMA_START                   0
// Conv2D , subOps: Clamp, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 8, 14, 16], IFM Block=[1, 8, 14, 64], OFM UBlock=[2, 2, 8] Traversal=DepthFirst, AccType=Acc32
0x001c2c: 49237904 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1227061508
0x001c34: 00017c40 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x17c40
0x001c3c:          01ff 0104 - NPU_SET_IFM_DEPTH_M1             511, depth_m1 = 511
0x001c40: 00001a00 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1a00
0x001c48:          003b 0109 - NPU_SET_IFM_ZERO_POINT            59, zero_point = 59
0x001c4c:          0041 0114 - NPU_SET_OFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001c50: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x001c58:          0003 0113 - NPU_SET_OFM_DEPTH_M1               3, depth_m1 = 3
0x001c5c: 000000d0 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0xd0
0x001c64:          ffe6 0118 - NPU_SET_OFM_ZERO_POINT         65510, zero_point = 65510
0x001c68: 0000a9a0 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0xa9a0
0x001c70: 00000940 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 2368
0x001c78: 000000a0 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 160
0x001c80:          ffe6 0126 - NPU_SET_ACTIVATION_MIN         65510, clip_boundary = 65510
0x001c84:          0007 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          7, height_m1 = 7
0x001c88:          000d 0115 - NPU_SET_OFM_BLK_WIDTH_M1          13, width_m1 = 13
0x001c8c:          000f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          15, depth_m1 = 15
0x001c90:          0300 0124 - NPU_SET_ACC_FORMAT               768, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U2X2
0x001c94:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x001c98:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
// AvgPool , subOps: -, size=13,13 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 2, 16], IFM Block=[1, 8, 10, 16], OFM UBlock=[1, 2, 16] Traversal=DepthFirst, AccType=Acc32
0x001c9c: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x001ca4:          0003 0104 - NPU_SET_IFM_DEPTH_M1               3, depth_m1 = 3
0x001ca8: 000000d0 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xd0
0x001cb0:          ffe6 0109 - NPU_SET_IFM_ZERO_POINT         65510, zero_point = 65510
0x001cb4:          0101 0114 - NPU_SET_OFM_PRECISION            257, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, scale_mode = OFM_SCALE_MODE_GLOBAL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x001cb8: 00000a90 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0xa90
0x001cc0:          0000 0112 - NPU_SET_OFM_HEIGHT_M1              0, height_m1 = 0
0x001cc4:          0000 0111 - NPU_SET_OFM_WIDTH_M1               0, width_m1 = 0
0x001cc8:          0000 011b - NPU_SET_OFM_HEIGHT0_M1             0, height_m1 = 0
0x001ccc:          0000 011c - NPU_SET_OFM_HEIGHT1_M1             0, height_m1 = 0
0x001cd0:          0000 011a - NPU_SET_OFM_WIDTH0_M1              0, width_m1 = 0
0x001cd4: 00000004 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x4
0x001cdc: 00000004 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x4
0x001ce4: 00000001 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x1
0x001cec:          000c 0121 - NPU_SET_KERNEL_HEIGHT_M1          12, height_m1 = 12
0x001cf0:          000c 0120 - NPU_SET_KERNEL_WIDTH_M1           12, width_m1 = 12
0x001cf4:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x001cf8: 60f25dec 2026 4024 - NPU_SET_OFM_SCALE               8230, shift = 38, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_NATURAL, scale = 1626496492
0x001d00:          0000 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          0, height_m1 = 0
0x001d04:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x001d08:          0100 0124 - NPU_SET_ACC_FORMAT               256, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X2
0x001d0c:          0000 012f - NPU_SET_BLOCKDEP                   0, blockdep = 0
0x001d10:          0003 0005 - NPU_OP_POOL                        3, pooling_mode = POOLING_MODE_SUM
0x001d14:          ffff 0000 - NPU_OP_STOP                    65535, mask = 65535

Configuration files:
   original = ['/workspace/executorch-venv/lib/python3.12/site-packages/ethosu/config_files/Arm/vela.ini']
   used = ['/workspace/executorch-venv/lib/python3.12/site-packages/ethosu/config_files/Arm/vela.ini']
System Configuration (Ethos_U85_SRAM_MRAM):
   core_clock = 400000000.0
   axi0_port = Sram
   axi1_port = OffChipFlash
   Sram_clock_scales = 1.0
   Sram_burst_length = 32
   Sram_read_latency = 2
   Sram_write_latency = 2
   Dram_clock_scales = 1.0
   Dram_burst_length = 1
   Dram_read_latency = 0
   Dram_write_latency = 0
   OnChipFlash_clock_scales = 1.0
   OnChipFlash_burst_length = 1
   OnChipFlash_read_latency = 0
   OnChipFlash_write_latency = 0
   OffChipFlash_clock_scales = 0.121
   OffChipFlash_burst_length = 256
   OffChipFlash_read_latency = 15
   OffChipFlash_write_latency = 0
Memory Mode (Shared_Sram):
   const_mem_area = Axi1
   arena_mem_area = Axi0
   cache_mem_area = Axi0
   arena_cache_size = 1099511627776 from Default
Architecture Settings:
   permanent_storage_mem_area = OffChipFlash
   feature_map_storage_mem_area = Sram
   fast_storage_mem_area = Sram

################################################################################
Performance for NPU Grap /tmp/tmpwi96ywea/output/out
Original Operator    NNG Operator         Target Staging Usage  Peak% (Staging)  Op Cycles Network% (cycles)        NPU    SRAM AC    DRAM AC OnFlash AC OffFlash AC  MAC Count Network% (MAC)  Util% (MAC) Name                 
-------------------- -------------------- ------ ------------- ---------------- ---------- ----------------- ---------- ---------- ---------- ---------- ----------- ---------- -------------- ------------ -------------------- 
Transpose            Transpose            NPU           953344            96.95    2140919             42.12    2140919      90944          0          0           0     150528           0.06         0.03 tosa_transpose_default 
Slice                MemoryCopy           NPU           952672            96.88     399751              7.86     399751      35586          0          0           0     149856           0.06         0.15 aten_slice_copy_tensor 
Conv2D               Conv2D               NPU           941664            95.76     296216              5.83     296216      64528          0          0       64793   21290688           7.95        28.08 aten_clamp_default   
Clamp                Clamp                NPU           941664            95.76     394784              7.77     394784     151650          0          0           0     788544           0.29         0.78 aten_clamp_default   
MaxPool              MaxPool              NPU           982144            99.88      48859              0.96      48859      34850          0          0           0    1742400           0.65        13.93 aten_max_pool2d_default 
Conv2D               Conv2D               NPU           243168            24.73      12760              0.25      12760       8554          0          0        2314    3097600           1.16        94.83 aten_clamp_default_1 
Clamp                Clamp                NPU           243168            24.73      25080              0.49      25080       3052          0          0           0      48400           0.02         0.75 aten_clamp_default_1 
Conv2D               Conv2D               NPU           251904            25.62     145754              2.87     145754      25356          0          0        9256   27878400          10.41        74.71 aten_clamp_default_2 
Clamp                Clamp                NPU           251904            25.62      97312              1.91      97312      17250          0          0           0     193600           0.07         0.78 aten_clamp_default_2 
Concat               MemoryCopy           NPU           629200            63.99      97167              1.91      97167      16354          0          0           0     193600           0.07         0.78 aten_cat_default     
Conv2D               Conv2D               NPU           631360            64.21      48912              0.96      48912      11513          0          0       16198    3097600           1.16        24.74 aten_clamp_default_3 
Clamp                Clamp                NPU           631360            64.21      97312              1.91      97312      12322          0          0           0     193600           0.07         0.78 aten_clamp_default_3 
Concat               MemoryCopy           NPU           580800            59.07      97167              1.91      97167      16354          0          0           0     193600           0.07         0.78 aten_cat_default     
Conv2D               Conv2D               NPU           437648            44.51      25080              0.49      25080      15484          0          0        2314    6195200           2.31        96.49 aten_clamp_default_4 
Clamp                Clamp                NPU           437648            44.51      25080              0.49      25080       3052          0          0           0      48400           0.02         0.75 aten_clamp_default_4 
Conv2D               Conv2D               NPU           252288            25.66     145754              2.87     145754      25692          0          0        9256   27878400          10.41        74.71 aten_clamp_default_5 
Clamp                Clamp                NPU           252288            25.66      97312              1.91      97312      17250          0          0           0     193600           0.07         0.78 aten_clamp_default_5 
Concat               MemoryCopy           NPU           629200            63.99      97167              1.91      97167      16354          0          0           0     193600           0.07         0.78 aten_cat_default_1   
Conv2D               Conv2D               NPU           631376            64.21      48912              0.96      48912      11538          0          0       16198    3097600           1.16        24.74 aten_clamp_default_6 
Clamp                Clamp                NPU           631376            64.21      97312              1.91      97312      12322          0          0           0     193600           0.07         0.78 aten_clamp_default_6 
Concat               MemoryCopy           NPU           580800            59.07      97167              1.91      97167      16354          0          0           0     193600           0.07         0.78 aten_cat_default_1   
MaxPool              MaxPool              NPU           480512            48.87      17316              0.34      12697      17316          0          0           0     839808           0.31        18.94 aten_max_pool2d_default_1 
Conv2D               Conv2D               NPU           120640            12.27      12528              0.25      12528       5363          0          0        2314    2985984           1.12        93.10 aten_clamp_default_7 
Clamp                Clamp                NPU           120640            12.27      12528              0.25      12528       1485          0          0           0      23328           0.01         0.73 aten_clamp_default_7 
Conv2D               Conv2D               NPU           155536            15.82     145966              2.87     145966      20804          0          0       14342   26873856          10.04        71.92 aten_clamp_default_8 
Clamp                Clamp                NPU           155536            15.82      48816              0.96      48816       7452          0          0           0      93312           0.03         0.75 aten_clamp_default_8 
Concat               MemoryCopy           NPU           303264            30.84      46960              0.92      46960       7172          0          0           0      93312           0.03         0.78 aten_cat_default_2   
Conv2D               Conv2D               NPU           310528            31.58      48816              0.96      48816       8558          0          0        9256    2985984           1.12        23.89 aten_clamp_default_9 
Clamp                Clamp                NPU           310528            31.58      48816              0.96      48816       5940          0          0           0      93312           0.03         0.75 aten_clamp_default_9 
Concat               MemoryCopy           NPU           279936            28.47      46960              0.92      46960       7172          0          0           0      93312           0.03         0.78 aten_cat_default_2   
Conv2D               Conv2D               NPU           217504            22.12      24789              0.49      24789       9941          0          0        2314    5971968           2.23        94.11 aten_clamp_default_10 
Clamp                Clamp                NPU           217504            22.12      12528              0.25      12528       1485          0          0           0      23328           0.01         0.73 aten_clamp_default_10 
Conv2D               Conv2D               NPU           155648            15.83     153601              3.02     153601      20241          0          0        4628   26873856          10.04        68.34 aten_clamp_default_11 
Clamp                Clamp                NPU           155648            15.83      48816              0.96      48816       7452          0          0           0      93312           0.03         0.75 aten_clamp_default_11 
Concat               MemoryCopy           NPU           303264            30.84      46960              0.92      46960       7172          0          0           0      93312           0.03         0.78 aten_cat_default_3   
Conv2D               Conv2D               NPU           310528            31.58      48816              0.96      48816       8558          0          0        9256    2985984           1.12        23.89 aten_clamp_default_12 
Clamp                Clamp                NPU           310528            31.58      48816              0.96      48816       5940          0          0           0      93312           0.03         0.75 aten_clamp_default_12 
Concat               MemoryCopy           NPU           279936            28.47      46960              0.92      46960       7172          0          0           0      93312           0.03         0.78 aten_cat_default_3   
MaxPool              MaxPool              NPU           229888            23.38       7184              0.14       4377       7184          0          0           0     389376           0.15        21.17 aten_max_pool2d_default_2 
Conv2D               Conv2D               NPU            62768             6.38       9173              0.18       9173       4210          0          0        3614    2076672           0.78        88.43 aten_clamp_default_13 
Clamp                Clamp                NPU            62768             6.38       4680              0.09       4680        526          0          0           0       8112           0.00         0.68 aten_clamp_default_13 
Conv2D               Conv2D               NPU            83872             8.53      79766              1.57      79766      13392          0          0       41976   14017536           5.24        68.65 aten_clamp_default_14 
Clamp                Clamp                NPU            83872             8.53      17888              0.35      17888       2574          0          0           0      32448           0.01         0.71 aten_clamp_default_14 
Concat               MemoryCopy           NPU           105456            10.72      16303              0.32      16303       2694          0          0           0      32448           0.01         0.78 aten_cat_default_4   
Conv2D               Conv2D               NPU           117808            11.98       8496              0.17       8496       4622          0          0        3966    1557504           0.58        71.61 aten_clamp_default_15 
Clamp                Clamp                NPU           117808            11.98      16736              0.33      16736       2550          0          0           0      32448           0.01         0.76 aten_clamp_default_15 
Concat               MemoryCopy           NPU            97344             9.90      16303              0.32      16303       2694          0          0           0      32448           0.01         0.78 aten_cat_default_4   
Conv2D               Conv2D               NPU            89632             9.12      13709              0.27      13709       5969          0          0        1735    3115008           1.16        88.76 aten_clamp_default_16 
Clamp                Clamp                NPU            89632             9.12       4680              0.09       4680        526          0          0           0       8112           0.00         0.68 aten_clamp_default_16 
Conv2D               Conv2D               NPU           106160            10.80      86439              1.70      86439      13036          0          0       35082   14017536           5.24        63.35 aten_clamp_default_17 
Clamp                Clamp                NPU           106160            10.80      17888              0.35      17888       2574          0          0           0      32448           0.01         0.71 aten_clamp_default_17 
Concat               MemoryCopy           NPU           105456            10.72      16303              0.32      16303       2694          0          0           0      32448           0.01         0.78 aten_cat_default_5   
Conv2D               Conv2D               NPU           117840            11.98       8496              0.17       8496       4626          0          0        3966    1557504           0.58        71.61 aten_clamp_default_18 
Clamp                Clamp                NPU           117840            11.98      16736              0.33      16736       2550          0          0           0      32448           0.01         0.76 aten_clamp_default_18 
Concat               MemoryCopy           NPU            97344             9.90      16303              0.32      16303       2694          0          0           0      32448           0.01         0.78 aten_cat_default_5   
Conv2D               Conv2D               NPU            98048             9.97      18288              0.36      18288       8042          0          0        1322    4153344           1.55        88.71 aten_clamp_default_19 
Clamp                Clamp                NPU            98048             9.97       6240              0.12       6240        754          0          0           0      10816           0.00         0.68 aten_clamp_default_19 
Conv2D               Conv2D               NPU           170080            17.30     149971              2.95     149971      23044          0          0       66496   24920064           9.31        64.91 aten_clamp_default_20 
Clamp                Clamp                NPU           170080            17.30      23712              0.47      23712       3432          0          0           0      43264           0.02         0.71 aten_clamp_default_20 
Concat               MemoryCopy           NPU           140608            14.30      21808              0.43      21808       3816          0          0           0      43264           0.02         0.77 aten_cat_default_6   
Conv2D               Conv2D               NPU           159168            16.19      12064              0.24      12064       9008          0          0        2644    2768896           1.03        89.66 aten_clamp_default_21 
Clamp                Clamp                NPU           159168            16.19      23712              0.47      23712       3016          0          0           0      43264           0.02         0.71 aten_clamp_default_21 
Concat               MemoryCopy           NPU           129792            13.20      21808              0.43      21808       3816          0          0           0      43264           0.02         0.77 aten_cat_default_6   
Conv2D               Conv2D               NPU           127200            12.94      24336              0.48      24336      10646          0          0        1322    5537792           2.07        88.89 aten_clamp_default_22 
Clamp                Clamp                NPU           127200            12.94       6240              0.12       6240        754          0          0           0      10816           0.00         0.68 aten_clamp_default_22 
Conv2D               Conv2D               NPU           172144            17.51     155268              3.05     155268      22965          0          0       62570   24920064           9.31        62.69 aten_clamp_default_23 
Clamp                Clamp                NPU           172144            17.51      23712              0.47      23712       3432          0          0           0      43264           0.02         0.71 aten_clamp_default_23 
Concat               MemoryCopy           NPU           140608            14.30      21808              0.43      21808       3816          0          0           0      43264           0.02         0.77 aten_cat_default_7   
Conv2D               Conv2D               NPU           159488            16.22      12064              0.24      12064       9028          0          0        2644    2768896           1.03        89.66 aten_clamp_default_24 
Clamp                Clamp                NPU           159488            16.22      23712              0.47      23712       3016          0          0           0      43264           0.02         0.71 aten_clamp_default_24 
Concat               MemoryCopy           NPU           129792            13.20      21808              0.43      21808       3816          0          0           0      43264           0.02         0.77 aten_cat_default_7   
Conv2D               Conv2D               NPU            91760             9.33       3560              0.07       3442       3560          0          0         165     346112           0.13        37.98 aten_clamp_default_25 
Clamp                Clamp                NPU            91760             9.33        873              0.02        873        188          0          0           0        676           0.00         0.30 aten_clamp_default_25 
AvgPool              AvgPool              NPU             2720             0.28        858              0.02        858        160          0          0           0        676           0.00         0.31 aten_view_copy_default 

Network summary for out
Accelerator configuration               Ethos_U85_256
System configuration              Ethos_U85_SRAM_MRAM
Memory mode                               Shared_Sram
Accelerator clock                                 400 MHz
Design peak SRAM bandwidth                      11.92 GB/s
Design peak Off-chip Flash bandwidth             0.72 GB/s

Total SRAM used                                960.27 KiB
Total Off-chip Flash used                      747.66 KiB

CPU operators = 0 (0.0%)
NPU operators = 48 (100.0%)

Average SRAM bandwidth                           1.67 GB/s
Input   SRAM bandwidth                          11.68 MB/batch
Weight  SRAM bandwidth                           4.18 MB/batch
Output  SRAM bandwidth                           5.06 MB/batch
Total   SRAM bandwidth                          21.26 MB/batch
Total   SRAM bandwidth            per input     21.26 MB/inference (batch size 1)

Average Off-chip Flash bandwidth                 0.06 GB/s
Input   Off-chip Flash bandwidth                 0.00 MB/batch
Weight  Off-chip Flash bandwidth                 0.73 MB/batch
Output  Off-chip Flash bandwidth                 0.00 MB/batch
Total   Off-chip Flash bandwidth                 0.73 MB/batch
Total   Off-chip Flash bandwidth  per input      0.73 MB/inference (batch size 1)

Original Weights Size                          704.69 KiB
NPU Encoded Weights Size                       718.75 KiB

Neural network macs                         267693188 MACs/batch

Info: The numbers below are internal compiler estimates.
For performance numbers the compiled network should be run on an FVP Model or FPGA.

Network Tops/s                                   0.04 Tops/s

NPU cycles                                    5075829 cycles/batch
SRAM Access cycles                             674460 cycles/batch
DRAM Access cycles                                  0 cycles/batch
On-chip Flash Access cycles                         0 cycles/batch
Off-chip Flash Access cycles                   389938 cycles/batch
Total cycles                                  5083373 cycles/batch

Batch Inference time                12.71 ms,   78.69 inferences/s (batch size 1)

class GraphModule(torch.nn.Module):
    def forward(self, x):
        x: "f32[1, 3, 224, 224]"; 
    
        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
        # No stacktrace found for following nodes
        alloc: "i8[1, 3, 224, 224]" = executorch_exir_memory_alloc(((1, 3, 224, 224), torch.int8))
        quantized_decomposed_quantize_per_tensor_default: "i8[1, 3, 224, 224]" = torch.ops.quantized_decomposed.quantize_per_tensor.out(x, 0.03026646561920643, -1, -128, 127, torch.int8, out = alloc);  x = alloc = None
        lowered_module_0 = self.lowered_module_0
        executorch_call_delegate = torch.ops.higher_order.executorch_call_delegate(lowered_module_0, quantized_decomposed_quantize_per_tensor_default);  lowered_module_0 = quantized_decomposed_quantize_per_tensor_default = None
        getitem: "i8[1, 4]" = executorch_call_delegate[0];  executorch_call_delegate = None
        alloc_1: "f32[1, 4]" = executorch_exir_memory_alloc(((1, 4), torch.float32))
        quantized_decomposed_dequantize_per_tensor_default: "f32[1, 4]" = torch.ops.quantized_decomposed.dequantize_per_tensor.out(getitem, 1.39414381980896, -26, -128, 127, torch.int8, out = alloc_1);  getitem = alloc_1 = None
        return pytree.tree_unflatten((quantized_decomposed_dequantize_per_tensor_default,), self._out_spec)
        
